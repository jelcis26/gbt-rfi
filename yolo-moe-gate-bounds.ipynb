{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81b1b1d6",
      "metadata": {
        "id": "81b1b1d6",
        "outputId": "c49a5694-57c3-431a-cb64-32661851a528"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: bitshuffle in /mnt_home/jliang/.local/lib/python3.7/site-packages (0.5.1)\n",
            "Requirement already satisfied: h5py>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from bitshuffle) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from bitshuffle) (1.20.3)\n",
            "Requirement already satisfied: setuptools>=0.7 in /opt/conda/lib/python3.7/site-packages (from bitshuffle) (62.3.2)\n",
            "Requirement already satisfied: Cython>=0.19 in /opt/conda/lib/python3.7/site-packages (from bitshuffle) (0.29.30)\n",
            "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py>=2.4.0->bitshuffle) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (7.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install bitshuffle\n",
        "%pip install Pillow\n",
        "%pip install scikit-image\n",
        "%pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2fe6857",
      "metadata": {
        "id": "a2fe6857"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import blimpy as bl\n",
        "#from ultralytics import YOLO\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import psutil\n",
        "import scipy.ndimage\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import cv2\n",
        "from scipy.ndimage import label as connected_components\n",
        "from skimage.filters import sobel\n",
        "from skimage.segmentation import slic\n",
        "from skimage.util import img_as_float\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from random import shuffle\n",
        "from tqdm import tqdm\n",
        "from skimage.util import img_as_float\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from skimage.segmentation import slic\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dba8ef3",
      "metadata": {
        "id": "8dba8ef3",
        "outputId": "afb83b76-0ae6-4e68-8c1d-42593d2d9e2a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Target</th>\n",
              "      <th>Session</th>\n",
              "      <th>Band</th>\n",
              "      <th>Cadence ID</th>\n",
              "      <th>Frequency</th>\n",
              "      <th>.h5 path</th>\n",
              "      <th>.dat path</th>\n",
              "      <th>Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36553</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36554</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36555</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36556</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36557</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>36558 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Target          Session Band  Cadence ID  Frequency  \\\n",
              "0       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "1       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "2       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "3       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "4       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "...        ...              ...  ...         ...        ...   \n",
              "36553  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "36554  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "36555  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "36556  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "36557  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "\n",
              "                                                .h5 path  \\\n",
              "0      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "1      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "2      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "3      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "4      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "...                                                  ...   \n",
              "36553  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "36554  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "36555  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "36556  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "36557  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "\n",
              "                                               .dat path                 Time  \n",
              "0      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "1      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "2      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "3      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "4      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "...                                                  ...                  ...  \n",
              "36553  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "36554  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "36555  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "36556  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "36557  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "\n",
              "[36558 rows x 8 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('/home/cgchoza/galaxies/complete_cadences_catalog.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "730aa85b",
      "metadata": {
        "id": "730aa85b",
        "outputId": "67aee906-65d3-4e1e-bdb1-77408418462b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Target</th>\n",
              "      <th>Session</th>\n",
              "      <th>Band</th>\n",
              "      <th>Cadence ID</th>\n",
              "      <th>Frequency</th>\n",
              "      <th>.h5 path</th>\n",
              "      <th>.dat path</th>\n",
              "      <th>Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30309</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30310</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30311</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30312</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30313</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30314 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Target          Session Band  Cadence ID  Frequency  \\\n",
              "0       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "1       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "2       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "3       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "4       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "...        ...              ...  ...         ...        ...   \n",
              "30309  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "30310  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "30311  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "30312  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "30313  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "\n",
              "                                                .h5 path  \\\n",
              "0      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "1      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "2      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "3      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "4      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "...                                                  ...   \n",
              "30309  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "30310  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "30311  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "30312  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "30313  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "\n",
              "                                               .dat path                 Time  \n",
              "0      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "1      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "2      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "3      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "4      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "...                                                  ...                  ...  \n",
              "30309  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "30310  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "30311  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "30312  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "30313  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "\n",
              "[30314 rows x 8 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['.h5 path'] = df['.h5 path'].str.replace('0000.h5', '0002.h5', regex=False)\n",
        "df = df.drop_duplicates(subset='.h5 path', keep='first').reset_index(drop=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38b7592b",
      "metadata": {
        "id": "38b7592b",
        "outputId": "9914de53-3dfa-4a3a-d891-1752d649ade3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Target</th>\n",
              "      <th>Session</th>\n",
              "      <th>Band</th>\n",
              "      <th>Cadence ID</th>\n",
              "      <th>Frequency</th>\n",
              "      <th>.h5 path</th>\n",
              "      <th>.dat path</th>\n",
              "      <th>Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AND_XIV</td>\n",
              "      <td>AGBT18B_999_07</td>\n",
              "      <td>S</td>\n",
              "      <td>30225</td>\n",
              "      <td>3151</td>\n",
              "      <td>/datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18B_999_07/blc40_blp00...</td>\n",
              "      <td>2018-08-18 08:41:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AND_XIV</td>\n",
              "      <td>AGBT18B_999_07</td>\n",
              "      <td>S</td>\n",
              "      <td>30225</td>\n",
              "      <td>3151</td>\n",
              "      <td>/datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18B_999_07/blc40_blp00...</td>\n",
              "      <td>2018-08-18 08:41:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AND_XIV</td>\n",
              "      <td>AGBT18B_999_07</td>\n",
              "      <td>S</td>\n",
              "      <td>30225</td>\n",
              "      <td>3151</td>\n",
              "      <td>/datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18B_999_07/blc40_blp00...</td>\n",
              "      <td>2018-08-18 08:41:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AND_XIV</td>\n",
              "      <td>AGBT18B_999_07</td>\n",
              "      <td>S</td>\n",
              "      <td>30225</td>\n",
              "      <td>3151</td>\n",
              "      <td>/datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18B_999_07/blc40_blp00...</td>\n",
              "      <td>2018-08-18 08:41:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AND_XIV</td>\n",
              "      <td>AGBT18B_999_07</td>\n",
              "      <td>S</td>\n",
              "      <td>30225</td>\n",
              "      <td>3151</td>\n",
              "      <td>/datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18B_999_07/blc40_blp00...</td>\n",
              "      <td>2018-08-18 08:41:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29341</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29342</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29343</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29344</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29345</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>29346 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Target         Session Band  Cadence ID  Frequency  \\\n",
              "0      AND_XIV  AGBT18B_999_07    S       30225       3151   \n",
              "1      AND_XIV  AGBT18B_999_07    S       30225       3151   \n",
              "2      AND_XIV  AGBT18B_999_07    S       30225       3151   \n",
              "3      AND_XIV  AGBT18B_999_07    S       30225       3151   \n",
              "4      AND_XIV  AGBT18B_999_07    S       30225       3151   \n",
              "...        ...             ...  ...         ...        ...   \n",
              "29341  NGC3226  AGBT22B_999_25    L      411390       1126   \n",
              "29342  NGC3226  AGBT22B_999_25    L      411390       1126   \n",
              "29343  NGC3226  AGBT22B_999_25    L      411390       1126   \n",
              "29344  NGC3226  AGBT22B_999_25    L      411390       1126   \n",
              "29345  NGC3226  AGBT22B_999_25    L      411390       1126   \n",
              "\n",
              "                                                .h5 path  \\\n",
              "0      /datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...   \n",
              "1      /datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...   \n",
              "2      /datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...   \n",
              "3      /datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...   \n",
              "4      /datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...   \n",
              "...                                                  ...   \n",
              "29341  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "29342  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "29343  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "29344  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "29345  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "\n",
              "                                               .dat path                 Time  \n",
              "0      /home/obs/turboseti/AGBT18B_999_07/blc40_blp00...  2018-08-18 08:41:37  \n",
              "1      /home/obs/turboseti/AGBT18B_999_07/blc40_blp00...  2018-08-18 08:41:37  \n",
              "2      /home/obs/turboseti/AGBT18B_999_07/blc40_blp00...  2018-08-18 08:41:37  \n",
              "3      /home/obs/turboseti/AGBT18B_999_07/blc40_blp00...  2018-08-18 08:41:37  \n",
              "4      /home/obs/turboseti/AGBT18B_999_07/blc40_blp00...  2018-08-18 08:41:37  \n",
              "...                                                  ...                  ...  \n",
              "29341  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "29342  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "29343  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "29344  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "29345  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "\n",
              "[29346 rows x 8 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df[~df['.h5 path'].str.contains('spliced')].reset_index(drop=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f1451bd",
      "metadata": {
        "id": "2f1451bd"
      },
      "outputs": [],
      "source": [
        "df = df.drop(index = 17546)\n",
        "#df = df[df['Band'] == 'L']\n",
        "# df = df.sample(n=1000, random_state=42).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6802a3bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import blimpy as bl\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "\n",
        "def scan_file_for_band(\n",
        "    h5_path: str,\n",
        "    band: str,\n",
        "    fmin: float,\n",
        "    fmax: float,\n",
        "    default_nfpc: int = 1024\n",
        "):\n",
        "    \"\"\"\n",
        "    Open one .h5, read header, return list of dicts for all channels\n",
        "    whose freq range lies in [fmin, fmax].\n",
        "    \"\"\"\n",
        "    records = []\n",
        "    fb = bl.Waterfall(h5_path, load_data=False)\n",
        "    hdr = fb.header\n",
        "\n",
        "    # skip if not the right band\n",
        "    # you could also pass row['Band'] in here if you like\n",
        "    # if row['Band'] != band: return []\n",
        "\n",
        "    fch1   = hdr['fch1']\n",
        "    foff   = hdr['foff']\n",
        "    nchans = hdr.get('nchans')\n",
        "    nfpc   = hdr.get('nfpc', default_nfpc)\n",
        "    n_coarse = nchans // nfpc\n",
        "\n",
        "    for ch in range(n_coarse):\n",
        "        f0 = ch * nfpc\n",
        "        f1 = (ch+1) * nfpc\n",
        "        f_start = fch1 + f0 * foff\n",
        "        f_stop  = fch1 + (f1-1) * foff\n",
        "        if f_start >= fmin or f_stop <= fmax:\n",
        "            records.append({\n",
        "                '.h5 path': h5_path,\n",
        "                'channel':    ch,\n",
        "                'Band':       band,\n",
        "                'f_start':    f_start,\n",
        "                'f_stop':     f_stop\n",
        "            })\n",
        "\n",
        "    return records\n",
        "\n",
        "def parallel_filter_df(\n",
        "    df: pd.DataFrame,\n",
        "    band: str = \"L\",\n",
        "    fmin: float = 1500,\n",
        "    fmax: float = 1650,\n",
        "    max_workers: int = 8\n",
        ") -> pd.DataFrame:\n",
        "    # restrict to Band==L first to cut the task list down\n",
        "    df_band = df[df['Band'] == band]\n",
        "    paths   = df_band[\".h5 path\"].unique().tolist()\n",
        "\n",
        "    all_records = []\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as exe:\n",
        "        futures = {\n",
        "            exe.submit(scan_file_for_band, p, band, fmin, fmax): p\n",
        "            for p in paths\n",
        "        }\n",
        "        for fut in tqdm(as_completed(futures), total=len(futures),\n",
        "                        desc=f\"Scanning {band}-band files\"):\n",
        "            try:\n",
        "                recs = fut.result()\n",
        "                all_records.extend(recs)\n",
        "            except Exception as e:\n",
        "                h5 = futures[fut]\n",
        "                print(f\"Error on {h5}: {e!r}\")\n",
        "\n",
        "    return pd.DataFrame.from_records(all_records)\n",
        "\n",
        "# Usage:\n",
        "df = parallel_filter_df(df, band=\"L\", fmin=1500, fmax=1650, max_workers=6)\n",
        "print(f\"Kept {len(df)} channel entries in L-band 1500-1650 MHz\")\n",
        "df = df.sample(n=1000, random_state=42).reset_index(drop=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84164fd0",
      "metadata": {
        "id": "84164fd0",
        "outputId": "1805d3f3-922d-48ec-8192-de3fd9753d90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- File Info ---\n",
            "DIMENSION_LABELS : [b'time' b'feed_id' b'frequency']\n",
            "        az_start :                              0.0\n",
            "       data_type :                                1\n",
            "            fch1 :               10501.46484375 MHz\n",
            "            foff :         -0.00286102294921875 MHz\n",
            "           ibeam :                               -1\n",
            "      machine_id :                               20\n",
            "          nbeams :                                1\n",
            "           nbits :                               32\n",
            "          nchans :                            65536\n",
            "            nifs :                                1\n",
            "     rawdatafile : guppi_59411_54386_HIP30264_0094.0000.raw\n",
            "     source_name :                         HIP30264\n",
            "         src_dej :                     -8:26:53.228\n",
            "         src_raj :                      6:21:58.451\n",
            "    telescope_id :                                6\n",
            "           tsamp :                1.073741823999999\n",
            "   tstart (ISOT) :          2021-07-16T15:06:26.000\n",
            "    tstart (MJD) :                59411.62946759259\n",
            "        za_start :                              0.0\n",
            "\n",
            "Num ints in file :                              279\n",
            "      File shape :                  (279, 1, 65536)\n",
            "--- Selection Info ---\n",
            "Data selection shape :                  (279, 1, 65536)\n",
            "Minimum freq (MHz) :                10313.96770477295\n",
            "Maximum freq (MHz) :                   10501.46484375\n"
          ]
        }
      ],
      "source": [
        "fb = bl.Waterfall(df['.h5 path'].iloc[-1], load_data=True)\n",
        "fb.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51b073aa",
      "metadata": {
        "id": "51b073aa"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "import time\n",
        "\n",
        "def ensure_cpu_mem(bytes_needed, safety=0.8):\n",
        "    avail = psutil.virtual_memory().available\n",
        "    if bytes_needed > avail * safety:\n",
        "        time.sleep(120)\n",
        "\n",
        "def ensure_gpu_mem(bytes_needed, safety=0.8):\n",
        "    free, total = cp.cuda.runtime.memGetInfo()\n",
        "    if bytes_needed > free * safety:\n",
        "        time.sleep(120)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d199c5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def fuse(box_lists, iou_thresh=0.3, min_width=3, max_width=30):\n",
        "    \"\"\"\n",
        "    box_lists: list of lists of (l, r, meta) tuples from each expert\n",
        "    iou_thresh: IoU threshold above which two boxes get merged\n",
        "    returns: fused list of (l, r, merged_meta)\n",
        "    \"\"\"\n",
        "    # flatten\n",
        "    boxes = [b for sub in box_lists for b in sub]\n",
        "    # sort by descending width or snr to merge biggest ones first\n",
        "    boxes.sort(key=lambda b: (b[1]-b[0]), reverse=True)\n",
        "\n",
        "    fused = []\n",
        "    for (l, r, meta) in boxes:\n",
        "        w = r-l\n",
        "        if w < min_width or w > max_width:\n",
        "            continue\n",
        "        # try to merge with an existing fused box\n",
        "        for idx,(fl,fr,fmeta) in enumerate(fused):\n",
        "            # IoU = intersection / union\n",
        "            inter = max(0, min(r,fr)-max(l,fl))\n",
        "            union = (r-l)+(fr-fl)-inter\n",
        "            if union>0 and (inter/union)>=iou_thresh:\n",
        "                # merge intervals\n",
        "                nl, nr = min(l,fl), max(r,fr)\n",
        "                # you can also merge metadata however you like:\n",
        "                fused[idx] = (nl, nr, {**fmeta, **meta})\n",
        "                break\n",
        "        else:\n",
        "            # no overlap → keep as new box\n",
        "            fused.append((l, r, meta))\n",
        "\n",
        "    return fused\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e069be5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def iou(box1, box2):\n",
        "    # box = (xc, yc, w, h) in normalized coords\n",
        "    x10 = box1[0] - box1[2]/2; x11 = box1[0] + box1[2]/2\n",
        "    y10 = box1[1] - box1[3]/2; y11 = box1[1] + box1[3]/2\n",
        "    x20 = box2[0] - box2[2]/2; x21 = box2[0] + box2[2]/2\n",
        "    y20 = box2[1] - box2[3]/2; y21 = box2[1] + box2[3]/2\n",
        "\n",
        "    xi0 = max(x10, x20);  yi0 = max(y10, y20)\n",
        "    xi1 = min(x11, x21);  yi1 = min(y11, y21)\n",
        "    inter_w = max(0, xi1 - xi0)\n",
        "    inter_h = max(0, yi1 - yi0)\n",
        "    inter = inter_w * inter_h\n",
        "\n",
        "    area1 = box1[2] * box1[3]\n",
        "    area2 = box2[2] * box2[3]\n",
        "    union = area1 + area2 - inter\n",
        "    return inter/union if union>0 else 0\n",
        "\n",
        "def non_max_suppression(boxes, scores=None, iou_thresh=0.3):\n",
        "    \"\"\"\n",
        "    boxes: list of (xc, yc, w, h)\n",
        "    scores: optional list of same length; if None we sort by box area (bigger first)\n",
        "    iou_thresh: IoU above which we suppress duplicates\n",
        "    \"\"\"\n",
        "    if len(boxes) == 0:\n",
        "        return []\n",
        "    if scores is None:\n",
        "        scores = [w*h for (_,_,w,h) in boxes]\n",
        "\n",
        "    # sort indices by descending score\n",
        "    idxs = np.argsort(scores)[::-1]\n",
        "    keep = []\n",
        "    for i in idxs:\n",
        "        b = boxes[i]\n",
        "        if all(iou(b, boxes[j]) < iou_thresh for j in keep):\n",
        "            keep.append(i)\n",
        "    return [boxes[i] for i in keep]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb678c08",
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_nested_boxes(boxes, scores):\n",
        "    \"\"\"\n",
        "    Drop any box that fully contains another, choosing the higher‐score box\n",
        "    when deciding which to keep. Returns (boxes_kept, scores_kept).\n",
        "    \"\"\"\n",
        "    def to_corners(b):\n",
        "        xc, yc, w, h = b\n",
        "        x0, x1 = xc - w/2, xc + w/2\n",
        "        y0, y1 = yc - h/2, yc + h/2\n",
        "        return x0, y0, x1, y1\n",
        "\n",
        "    corners = [to_corners(b) for b in boxes]\n",
        "    keep = [True]*len(boxes)\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "        if not keep[i]:\n",
        "            continue\n",
        "        x0_i,y0_i,x1_i,y1_i = corners[i]\n",
        "        for j in range(len(boxes)):\n",
        "            if i==j or not keep[j]:\n",
        "                continue\n",
        "            x0_j,y0_j,x1_j,y1_j = corners[j]\n",
        "            # if box i fully contains box j\n",
        "            if x0_i <= x0_j and x1_i >= x1_j and y0_i <= y0_j and y1_i >= y1_j:\n",
        "                # keep only the higher‐scoring one\n",
        "                if scores[i] >= scores[j]:\n",
        "                    keep[j] = False\n",
        "                else:\n",
        "                    keep[i] = False\n",
        "\n",
        "    boxes_kept  = [b for b,k in zip(boxes, keep) if k]\n",
        "    scores_kept = [s for s,k in zip(scores,keep) if k]\n",
        "    return boxes_kept, scores_kept\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54d1011a",
      "metadata": {
        "id": "54d1011a"
      },
      "outputs": [],
      "source": [
        "def generate_yolo_boxes(edge_map, threshold=0.1):\n",
        "    binary = (edge_map > threshold).astype(np.uint8)\n",
        "    labeled, n = connected_components(binary)\n",
        "    boxes = []\n",
        "    for i in range(1, n + 1):\n",
        "        coords = np.argwhere(labeled == i)\n",
        "        if coords.shape[0] < 10:\n",
        "            continue\n",
        "        y0, x0 = coords.min(axis=0)\n",
        "        y1, x1 = coords.max(axis=0)\n",
        "        cx = (x0 + x1) / 2 / edge_map.shape[1]\n",
        "        cy = (y0 + y1) / 2 / edge_map.shape[0]\n",
        "        w = (x1 - x0) / edge_map.shape[1]\n",
        "        h = (y1 - y0) / edge_map.shape[0]\n",
        "        boxes.append((cx, cy, w, h))\n",
        "    return boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37e12fd6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_tasks(df):\n",
        "    \"\"\"\n",
        "    Build a flat list of (h5_path, channel_idx) from your DataFrame,\n",
        "    skipping any channels that fall into known notch filter frequency ranges.\n",
        "    \"\"\"\n",
        "    GBT_NOTCH_FILTERS = {\n",
        "        \"L\": [(1200, 1340)],\n",
        "        \"S\": [(2300, 2360)],\n",
        "    }\n",
        "\n",
        "    tasks = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        h5 = row[\".h5 path\"]\n",
        "        band = row[\"Band\"]  # e.g., 'L', 'S', etc.\n",
        "        fb = bl.Waterfall(h5, load_data=False)\n",
        "        nfreq = fb.header.get(\"nchans\")\n",
        "        nfpc = fb.header.get(\"nfpc\", 1024)\n",
        "        fch1 = fb.header[\"fch1\"]\n",
        "        foff = fb.header[\"foff\"]\n",
        "        n_coarse = nfreq // nfpc\n",
        "\n",
        "        for ch in range(n_coarse):\n",
        "            f0 = fch1 + ch * nfpc * foff\n",
        "            f1 = fch1 + (ch + 1) * nfpc * foff\n",
        "            f_min, f_max = sorted([f0, f1])\n",
        "\n",
        "            # Check against notch filter exclusion ranges\n",
        "            skip = False\n",
        "            if band in GBT_NOTCH_FILTERS:\n",
        "                for lo, hi in GBT_NOTCH_FILTERS[band]:\n",
        "                    if lo <= f_min <= hi or lo <= f_max <= hi:\n",
        "                        skip = True\n",
        "                        break\n",
        "            if not skip:\n",
        "                tasks.append((h5, ch))\n",
        "\n",
        "    return tasks\n",
        "\n",
        "\n",
        "def split_tasks(tasks, train_frac=0.8, seed=42):\n",
        "    \"\"\"\n",
        "    Shuffle & split the flat task list into train vs. val sets.\n",
        "    Returns two sets of (h5_path, channel_idx).\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    shuffled = tasks.copy()\n",
        "    random.shuffle(shuffled)\n",
        "    cut = int(train_frac * len(shuffled))\n",
        "    train = set(shuffled[:cut])\n",
        "    val   = set(shuffled[cut:])\n",
        "    return train, val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "882a3870",
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import kurtosis\n",
        "\n",
        "def detect_boxes_with_patch_kurtosis(\n",
        "    block,               # raw 2D power array (ntime × nfreq)\n",
        "    fused,               # your fused MoE map, same shape\n",
        "    threshold,           # global threshold you already compute\n",
        "    kurt_thresh=3.5,     # min kurtosis inside a patch to even try detection\n",
        "    tile_size=64,        # patch size in pixels\n",
        "    step=None           # stride between patches (defaults to 50% overlap)\n",
        "):\n",
        "    H, W = block.shape\n",
        "    if step is None:\n",
        "        step = tile_size // 2\n",
        "\n",
        "    all_boxes = []\n",
        "    for y0 in range(0, H, step):\n",
        "        for x0 in range(0, W, step):\n",
        "            y1 = min(H, y0 + tile_size)\n",
        "            x1 = min(W, x0 + tile_size)\n",
        "\n",
        "            # 1) compute patch‐local kurtosis\n",
        "            patch_blk = block[y0:y1, x0:x1]\n",
        "            flat = patch_blk.ravel()\n",
        "            if flat.size < 2:\n",
        "                continue\n",
        "            k_loc = kurtosis(flat, fisher=False)\n",
        "            if k_loc < kurt_thresh:\n",
        "                continue\n",
        "\n",
        "            # 2) detect boxes in this patch\n",
        "            patch_fused = fused[y0:y1, x0:x1]\n",
        "            boxes_patch = generate_yolo_boxes(patch_fused, threshold=threshold)\n",
        "\n",
        "            # 3) convert each box to global normalized coords\n",
        "            ph, pw = y1 - y0, x1 - x0\n",
        "            for cx_p, cy_p, w_p, h_p in boxes_patch:\n",
        "                # patch‐pixel center\n",
        "                xc_pix = cx_p * pw  \n",
        "                yc_pix = cy_p * ph\n",
        "                # convert to absolute pixel coords, then normalize\n",
        "                xc = (x0 + xc_pix) / W\n",
        "                yc = (y0 + yc_pix) / H\n",
        "                w  = (w_p * pw) / W\n",
        "                h  = (h_p * ph) / H\n",
        "                all_boxes.append((xc, yc, w, h))\n",
        "\n",
        "    return all_boxes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d65f16e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def shrink_large_boxes(\n",
        "    boxes: list[tuple[float, float, float, float]],\n",
        "    fused: np.ndarray,\n",
        "    w_max: float = 0.9,\n",
        "    h_max: float = 0.9,\n",
        "    global_thresh: float = None,\n",
        "    local_frac: float = 0.1,\n",
        "    min_coverage: float = 0.05\n",
        ") -> list[tuple[float, float, float, float]]:\n",
        "    \"\"\"\n",
        "    For any box whose normalized width >= w_max OR height >= h_max,\n",
        "    re-extract the patch in `fused`, threshold it at `mask_thr` and\n",
        "    only shrink if the mask covers at least `min_coverage` fraction\n",
        "    of the patch.  Otherwise, keep the original box.\n",
        "\n",
        "    Parameters:\n",
        "    - boxes: YOLO boxes (xc, yc, w, h) normalized coords\n",
        "    - fused: 2D fused map\n",
        "    - w_max, h_max: thresholds for deciding \"large\" boxes\n",
        "    - global_thresh: if provided, used as local mask threshold;\n",
        "                     otherwise mask_thr = local_frac * fused.max()\n",
        "    - local_frac: fraction of fused.max() used if global_thresh is None\n",
        "    - min_coverage: minimum fraction of patch pixels above mask_thr\n",
        "                     to allow shrinking\n",
        "    \"\"\"\n",
        "    H, W = fused.shape\n",
        "    # Determine local mask threshold\n",
        "    if global_thresh is None:\n",
        "        mask_thr = local_frac * fused.max()\n",
        "    else:\n",
        "        mask_thr = global_thresh\n",
        "\n",
        "    shrunk = []\n",
        "    for xc, yc, w, h in boxes:\n",
        "        # only shrink boxes that exceed size\n",
        "        if w < w_max and h < h_max:\n",
        "            shrunk.append((xc, yc, w, h))\n",
        "            continue\n",
        "\n",
        "        # map normalized center/size → pixel coords\n",
        "        x0 = max(0, int((xc - w/2) * W))\n",
        "        x1 = min(W, int((xc + w/2) * W))\n",
        "        y0 = max(0, int((yc - h/2) * H))\n",
        "        y1 = min(H, int((yc + h/2) * H))\n",
        "\n",
        "        patch = fused[y0:y1, x0:x1]\n",
        "        if patch.size == 0:\n",
        "            shrunk.append((xc, yc, w, h))\n",
        "            continue\n",
        "\n",
        "        # binarize and check coverage\n",
        "        mask = patch > mask_thr\n",
        "        cov = mask.sum() / mask.size\n",
        "        if cov < min_coverage:\n",
        "            # not enough core pixels: skip shrinking\n",
        "            shrunk.append((xc, yc, w, h))\n",
        "            continue\n",
        "\n",
        "        ys, xs = np.nonzero(mask)\n",
        "        y_min, y_max = ys.min(), ys.max()\n",
        "        x_min, x_max = xs.min(), xs.max()\n",
        "\n",
        "        # convert back to normalized coords\n",
        "        new_w = (x_max - x_min + 1) / W\n",
        "        new_h = (y_max - y_min + 1) / H\n",
        "        new_xc = (x0 + (x_min + x_max + 1)/2) / W\n",
        "        new_yc = (y0 + (y_min + y_max + 1)/2) / H\n",
        "\n",
        "        shrunk.append((new_xc, new_yc, new_w, new_h))\n",
        "\n",
        "    return shrunk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5fa3669",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from astropy.stats import sigma_clip\n",
        "from scipy.signal import find_peaks\n",
        "import traceback\n",
        "\n",
        "def clipped_2D_bounds_numpy(\n",
        "    data: np.ndarray,\n",
        "    min_empty_bins: int = 2,\n",
        "    min_clipped:    int = 1,\n",
        "    peak_prominence:int = 4\n",
        "):\n",
        "    \"\"\"\n",
        "    Run sigma-clip on a 2D intensity array (time × freq), and\n",
        "    find the left/right frequency-bin bounds around the strongest\n",
        "    central peak of clipped pixels.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    data : 2D np.ndarray\n",
        "        Intensity data, shape (n_time, n_freq).\n",
        "    min_empty_bins : int\n",
        "        Kernel size for requiring adjacent non-zero runs.\n",
        "    min_clipped : int\n",
        "        Minimum clipped pixels along time for a freq-bin to count.\n",
        "    peak_prominence : int\n",
        "        Minimum prominence (in clipped-pixel counts) for peak finding.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    l : int\n",
        "        Left frequency-bin index (inclusive).\n",
        "    r : int\n",
        "        Right frequency-bin index (exclusive).\n",
        "    metadata : dict\n",
        "        { 'num_peaks': <int>, 'peaks': <peaks tuple from scipy> }\n",
        "    \"\"\"\n",
        "    # 1) Normalize to [0,1]\n",
        "    arr = data.astype(float)\n",
        "    arr -= arr.min()\n",
        "    mx = arr.max()\n",
        "    if mx > 0:\n",
        "        arr /= mx\n",
        "\n",
        "    # 2) Sigma-clip along time axis\n",
        "    clipped = sigma_clip(arr, axis=0)\n",
        "    mask_spec = np.sum(clipped.mask, axis=0)  # counts of clipped pixels per freq\n",
        "\n",
        "    # 3) Peak finding in that “clipped count” spectrum\n",
        "    peaks = find_peaks(mask_spec, prominence=peak_prominence)\n",
        "    if len(peaks[0]) == 0:\n",
        "        raise ValueError(\"No peaks found in clipped-count spectrum\")\n",
        "\n",
        "    # choose the peak of highest prominence, nearest the center\n",
        "    center = mask_spec.size // 2\n",
        "    prominences = peaks[1]['prominences']\n",
        "    max_idxs    = np.where(prominences == prominences.max())[0]\n",
        "    peak_i      = peaks[0][ max_idxs[np.argmin(np.abs(peaks[0][max_idxs] - center))] ]\n",
        "\n",
        "    # 4) Threshold mask and convolve to enforce runs of non-zero\n",
        "    good = (mask_spec >= min_clipped).astype(int)\n",
        "    conv = np.convolve(good, np.ones(min_empty_bins, dtype=int), mode='valid')\n",
        "    c_mask = (conv != 0).astype(int)\n",
        "    diffs  = np.diff(c_mask)\n",
        "\n",
        "    # 5) Rising / falling edges around peak_i\n",
        "    l_idx = np.where(diffs > 0)[0]\n",
        "    r_idx = np.where(diffs < 0)[0]\n",
        "\n",
        "    # pick last rising edge to the left, first falling to the right\n",
        "    l = l_idx[l_idx + min_empty_bins <= peak_i][-1] + min_empty_bins\n",
        "    r = r_idx[r_idx >= peak_i][0] + 1\n",
        "\n",
        "    metadata = {\n",
        "        'num_peaks': len(peaks[0]),\n",
        "        'peaks':     peaks\n",
        "    }\n",
        "    return l, r, metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3424043c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from astropy.stats import sigma_clip\n",
        "\n",
        "def threshold_baseline_bounds_numpy(spec: np.ndarray, p: float = 0.01):\n",
        "    \"\"\"\n",
        "    Pure-NumPy version of threshold_baseline_bounds.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    spec : 1D np.ndarray\n",
        "        Input intensity spectrum.\n",
        "    p : float\n",
        "        Fraction of the peak (0 < p < 1) at which to set bounds.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    l : int\n",
        "        Left bin index (inclusive) of the thresholded region.\n",
        "    r : int\n",
        "        Right bin index (exclusive) of the thresholded region.\n",
        "    metadata : dict\n",
        "        {\n",
        "          'noise_mean': mean of sigma-clipped noise,\n",
        "          'spec_max':   maximum of baseline-subtracted spectrum\n",
        "        }\n",
        "    \"\"\"\n",
        "    # 1) Estimate noise via sigma-clip\n",
        "    noise_spec = sigma_clip(spec, masked=True)\n",
        "    x = np.arange(spec.size)\n",
        "\n",
        "    # 2) Fit a 1st-degree baseline to the unmasked (i.e. noise) points\n",
        "    good = ~noise_spec.mask\n",
        "    coeffs = np.polyfit(x[good], noise_spec.data[good], deg=1)\n",
        "    poly   = np.poly1d(coeffs)\n",
        "\n",
        "    # 3) Subtract baseline and normalize by its maximum\n",
        "    spec_sub = spec - poly(x)\n",
        "    spec_max = spec_sub.max()\n",
        "    if spec_max <= 0:\n",
        "        raise ValueError(\"Spectrum has non-positive dynamic range after baseline subtraction\")\n",
        "    norm_spec = spec_sub / spec_max\n",
        "\n",
        "    # 4) Find all bins below fraction p of the peak\n",
        "    cutoffs = np.where(norm_spec < p)[0]\n",
        "    if cutoffs.size < 2:\n",
        "        raise ValueError(\"Not enough cutoff points found; check p or input spectrum\")\n",
        "\n",
        "    # 5) Locate the central peak in the normalized spectrum\n",
        "    peak_i = int(np.argmax(norm_spec))\n",
        "\n",
        "    # 6) Digitize to find which gap interval contains the peak\n",
        "    idx = np.digitize(peak_i, cutoffs) - 1\n",
        "    if idx < 0 or idx + 1 >= cutoffs.size:\n",
        "        raise IndexError(\"Peak lies outside cutoff intervals\")\n",
        "\n",
        "    # 7) Bounds are the edges just outside the cutoff run containing the peak\n",
        "    l = int(cutoffs[idx] + 1)\n",
        "    r = int(cutoffs[idx + 1])\n",
        "\n",
        "    metadata = {\n",
        "        'noise_mean': float(np.mean(noise_spec.data)), \n",
        "        'spec_max':   float(spec_max)\n",
        "    }\n",
        "    return l, r, metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FwocEh2jdjRX",
      "metadata": {
        "id": "FwocEh2jdjRX"
      },
      "outputs": [],
      "source": [
        "# The MoE: Structured Forest (SF), Sobel (UED), Canny, HOG (Histogram of Oriented Gradients)\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageDraw\n",
        "import blimpy as bl\n",
        "from skimage.segmentation import slic\n",
        "from skimage.feature import hog\n",
        "from skimage.transform import probabilistic_hough_line\n",
        "from scipy.stats import kurtosis\n",
        "from setigen.frame import Frame\n",
        "\n",
        "\n",
        "# --- Define RuleGate  ------------------------------------------------------\n",
        "class RuleGate(nn.Module):\n",
        "    def __init__(self,\n",
        "                 w_align=3.0,   # weight on alignment diff\n",
        "                 w_ent=0.8,     # weight on entropy diff\n",
        "                 w_dens=0.5,    # (optional) weight on density diff\n",
        "                 w_lin=3.0,     # weight on linearity diff\n",
        "                 bias=-0.2,     # bias for the mix‐gate\n",
        "                 line_thresh=0.2 # baseline linearity to trigger pure Hough\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.w_align    = w_align\n",
        "        self.w_ent      = w_ent\n",
        "        self.w_dens     = w_dens\n",
        "        self.w_lin      = w_lin\n",
        "        self.bias       = bias\n",
        "        self.line_thresh = line_thresh\n",
        "\n",
        "    def forward(self,\n",
        "                h_sf: torch.Tensor,  # (B,4)\n",
        "                h_ued: torch.Tensor,  # (B,4)\n",
        "                h_canny: torch.Tensor,  # (B,4)\n",
        "                h_hough: torch.Tensor   # (B,4)\n",
        "               ):\n",
        "        # ---- stage 1: Hough \"confidence\" gate ----\n",
        "        line_diff = h_hough[:, 3] - self.line_thresh\n",
        "        p_hough   = torch.sigmoid(self.w_lin * line_diff)\n",
        "\n",
        "        # ---- stage 2: two-expert mix: E1 = avg(SF, Canny), E2 = UED ----\n",
        "        # alignment difference\n",
        "        e1_align   = 0.5 * (h_sf[:,1] + h_canny[:,1])\n",
        "        e2_align   =       h_ued[:,1]\n",
        "        align_diff = e2_align - e1_align\n",
        "\n",
        "        # entropy difference (penalize when SF+Canny more textured)\n",
        "        e1_ent  = 0.5 * (h_sf[:,2] + h_canny[:,2])\n",
        "        e2_ent  =       h_ued[:,2]\n",
        "        ent_diff = e1_ent - e2_ent\n",
        "\n",
        "        # density difference\n",
        "        e1_dens   = 0.5 * (h_sf[:,0] + h_canny[:,0])\n",
        "        e2_dens   =       h_ued[:,0]\n",
        "        dens_diff = e2_dens - e1_dens\n",
        "\n",
        "        # mix score: favor UED when alignment and density high, penalize entropy\n",
        "        score_mix = (\n",
        "            self.w_align   * align_diff\n",
        "          - self.w_ent     * ent_diff\n",
        "          + self.w_dens    * dens_diff\n",
        "          + self.bias\n",
        "        )\n",
        "        p_mix = torch.sigmoid(score_mix)\n",
        "\n",
        "        return p_hough, p_mix\n",
        "\n",
        "\n",
        "\n",
        "# -- 2) Heuristic helper -------------------------------------------------\n",
        "def compute_heuristics(edge_map, gray):\n",
        "    mask = edge_map > 0.2\n",
        "    density = float(mask.mean())\n",
        "    if density > 0:\n",
        "        gx = cv2.Sobel(gray, cv2.CV_32F, 1, 0, 3)\n",
        "        gy = cv2.Sobel(gray, cv2.CV_32F, 0, 1, 3)\n",
        "        alignment = float(np.hypot(gx, gy)[mask].mean())\n",
        "    else:\n",
        "        alignment = 0.0\n",
        "    entropy = float(-np.sum(edge_map * np.log2(edge_map + 1e-8)))\n",
        "    bw = mask.astype(np.uint8)\n",
        "    n, labels = cv2.connectedComponents(bw)\n",
        "    # build ratios as you already do…\n",
        "    ratios = []\n",
        "    for L in range(1, n):\n",
        "        ys, xs = np.where(labels == L)\n",
        "        if ys.size:\n",
        "            h, w = np.ptp(ys)+1, np.ptp(xs)+1\n",
        "            ratios.append(w/h if h else 0.0)\n",
        "\n",
        "    # simple list-truth test:\n",
        "    if ratios:\n",
        "        linearity = float(np.mean(ratios))\n",
        "    else:\n",
        "        linearity = 0.0\n",
        "\n",
        "    if density < 0.01:\n",
        "        return [0.0, 0.0, 0.0, 0.0]\n",
        "    return [density, alignment, entropy, linearity]\n",
        "\n",
        "\n",
        "# -------- 3) process_file --------------------------------------------\n",
        "def process_file(job):\n",
        "    (h5_path, channels, gidxs,\n",
        "     base_dir, class_id, train_set, val_set,\n",
        "     pad_width) = job\n",
        "\n",
        "    fb   = bl.Waterfall(h5_path, load_data=True)\n",
        "    data = 10*np.log10(fb.data.squeeze())\n",
        "\n",
        "    EDGE_MODEL = \"/home/jliang/gbt-rfi/model.yml.gz\"\n",
        "    sf_det     = cv2.ximgproc.createStructuredEdgeDetection(EDGE_MODEL)\n",
        "\n",
        "    for ch_idx, gidx in zip(channels, gidxs):\n",
        "        subset = \"train\" if (h5_path,ch_idx) in train_set else \"val\"\n",
        "        img_dir = Path(base_dir)/subset/\"images\"\n",
        "        lbl_dir = Path(base_dir)/subset/\"labels\"\n",
        "        vis_dir = Path(base_dir)/\"visualization\"/subset\n",
        "        for d in (img_dir,lbl_dir,vis_dir): d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        cw = fb.header.get(\"nfpc\",1024)\n",
        "        f0, f1 = ch_idx*cw, (ch_idx+1)*cw\n",
        "        block = data[:,f0:f1]\n",
        "        low, high = int(0.15*cw), int(0.85*cw)\n",
        "        block = block[:,low:high]\n",
        "        h_img, w_img = block.shape\n",
        "\n",
        "        # Remove vertical line artifact\n",
        "        rows, cols = block.shape\n",
        "        vert_means = block.mean(axis=0)\n",
        "        center = np.argmax(vert_means)\n",
        "        left_col = center - 1\n",
        "        right_col = center + 1\n",
        "        if left_col >= 0 and right_col < cols:\n",
        "            block[:, center] = (block[:, left_col] + block[:, right_col]) / 2\n",
        "        elif left_col >= 0:\n",
        "            block[:, center] = block[:, left_col]\n",
        "        elif right_col < cols:\n",
        "            block[:, center] = block[:, right_col]\n",
        "\n",
        "\n",
        "        # Structured Forest\n",
        "        norm = (block - block.min())/(np.ptp(block)+1e-6)\n",
        "        img3 = np.stack([norm]*3, axis=-1).astype(np.float32)\n",
        "        sf_map = sf_det.detectEdges(img3).squeeze()\n",
        "        gray8  = (255*norm).astype(np.uint8)\n",
        "\n",
        "        # Sobel\n",
        "        gx = cv2.Sobel(gray8, cv2.CV_32F, 1,0,3)\n",
        "        gy = cv2.Sobel(gray8, cv2.CV_32F, 0,1,3)\n",
        "        ued_map = np.hypot(gx, gy)\n",
        "        ued_map = cv2.normalize(ued_map, None, 0,1, cv2.NORM_MINMAX)\n",
        "\n",
        "        # Canny\n",
        "        low_thresh, high_thresh = 30, 100\n",
        "        gray_denoised = cv2.medianBlur(gray8, 3)\n",
        "        edges = cv2.Canny(gray_denoised, low_thresh, high_thresh)\n",
        "        canny_map = edges.astype(np.float32) / 255.0\n",
        "\n",
        "        # Hough\n",
        "        hough_map = np.zeros_like(canny_map, dtype=np.float32)\n",
        "        lines = probabilistic_hough_line(\n",
        "            (edges > 0).astype(np.uint8),\n",
        "            threshold=5,\n",
        "            line_length=10,\n",
        "            line_gap=2\n",
        "        )\n",
        "        for (y0, x0), (y1, x1) in lines:\n",
        "            cv2.line(hough_map, (x0, y0), (x1, y1), 1.0, 1)\n",
        "        hough_map = cv2.GaussianBlur(hough_map, (3,3), 0)\n",
        "\n",
        "        bk = float(kurtosis(gray8.flatten(), fisher=False))\n",
        "        # Define the RuleGate based on kurtosis\n",
        "        if bk < 15.0:\n",
        "            gate = RuleGate(\n",
        "            w_align    = 3.5,   # more trust in edges\n",
        "            w_ent      = 0.6,   # slightly less veto power\n",
        "            w_dens     = 0.5,\n",
        "            w_lin      = 5.0,\n",
        "            bias       = -0.1,  # less bias toward SF\n",
        "            line_thresh= 0.08\n",
        "            )\n",
        "        elif bk < 50:\n",
        "            gate = RuleGate(\n",
        "                w_align    = 5.0,    # more trust in edges\n",
        "                w_ent      = 0.4,    # less veto from SF entropy\n",
        "                w_dens     = 0.8,    # let density help a bit\n",
        "                w_lin      = 6.0,    # Hough fires more easily\n",
        "                bias       = 0.1,   # slight default tilt toward edges\n",
        "                line_thresh= 0.06    # only 6 % linearity needed\n",
        "            )\n",
        "\n",
        "        elif bk < 150.0:\n",
        "            # relaxed mid-kurtosis gate: more permissive for edges & Hough\n",
        "            gate = RuleGate(\n",
        "                w_align    = 5.0,    # stronger trust in edge alignment\n",
        "                w_ent      = 0.4,    # less veto from SF‐entropy\n",
        "                w_dens     = 0.6,    # moderate density boost\n",
        "                w_lin      = 6.0,    # let Hough fire on softer linearity\n",
        "                bias       =  0.1,   # slight default tilt toward edges\n",
        "                line_thresh= 0.06    # lower bar for pure Hough\n",
        "            )\n",
        "        elif bk < 300.0:\n",
        "            # high‐kurtosis → favor edges more\n",
        "            gate = RuleGate(\n",
        "                w_align    = 5.0,\n",
        "                w_ent      = 0.3,\n",
        "                w_dens     = 0.5,\n",
        "                w_lin      = 6.0,\n",
        "                bias       =  0.2,\n",
        "                line_thresh= 0.06\n",
        "            )\n",
        "        else:\n",
        "            # extremely‐spiky → almost‐pure Hough & edges\n",
        "            gate = RuleGate(\n",
        "                w_align    = 6.0,\n",
        "                w_ent      = 0.1,\n",
        "                w_dens     = 1.5,\n",
        "                w_lin      = 6.0,\n",
        "                bias       =  0.4,\n",
        "                line_thresh= 0.04\n",
        "            )\n",
        "        \n",
        "\n",
        "        fused = np.zeros_like(sf_map, dtype=np.float32)\n",
        "        segments = slic(img3, n_segments=100, compactness=10)\n",
        "        for v in np.unique(segments):\n",
        "            mask = (segments==v)\n",
        "            if mask.sum()<50:\n",
        "                continue\n",
        "            h_sf = compute_heuristics(sf_map[mask], gray8[mask])\n",
        "            h_ued = compute_heuristics(ued_map[mask], gray8[mask])\n",
        "            h_canny = compute_heuristics(canny_map[mask], gray8[mask])\n",
        "            h_hough = compute_heuristics(hough_map[mask], gray8[mask])\n",
        "            h_sf_t = torch.tensor([h_sf], dtype=torch.float32)\n",
        "            h_ued_t = torch.tensor([h_ued], dtype=torch.float32)\n",
        "            h_canny_t = torch.tensor([h_canny], dtype=torch.float32)\n",
        "            h_hough_t = torch.tensor([h_hough], dtype=torch.float32)\n",
        "            p_line, p_mix = gate(h_sf_t, h_ued_t, h_canny_t, h_hough_t)\n",
        "            p_line, p_mix = p_line.item(), p_mix.item()\n",
        "            fused_region = (p_line * hough_map[mask] + (1 - p_line) * (p_mix * (ued_map[mask] + canny_map[mask]) / 2\n",
        "                + (1 - p_mix) * sf_map[mask]))\n",
        "            fused[mask] = fused_region\n",
        "\n",
        "        # 1) measure & clamp kurtosis\n",
        "        bk = float(kurtosis(gray8.flatten(), fisher=False))\n",
        "        # bk_clamped = np.clip(bk, 2.0, 800.0)\n",
        "\n",
        "        # p = float(np.interp(\n",
        "        #     bk_clamped,\n",
        "        #     [   3.0,   10.0,   30.0,  50.0,  100.0,  400.0,  800.0 ],\n",
        "        #     [  97.0,   94.0,   88.0,  75.0,   65.0,   60.0,   55.0 ]\n",
        "        # ))\n",
        "\n",
        "        # # 3) compute the p-th percentile of the fused‐map\n",
        "        # flat = fused.ravel()\n",
        "        # threshold = float(np.percentile(flat, p))\n",
        "\n",
        "        # 4) run your usual box generator\n",
        "        boxes = generate_yolo_boxes(fused, threshold=0.04)\n",
        "\n",
        "        # 2) run patch‐based pre‐filter + detection\n",
        "        filtered = []\n",
        "        H, W = block.shape\n",
        "        for xc, yc, w_n, h_n in boxes:\n",
        "            # map back to pixel coords\n",
        "            x0 = max(0, int((xc - w_n/2) * W))\n",
        "            x1 = min(W-1, int((xc + w_n/2) * W))\n",
        "            y0 = max(0, int((yc - h_n/2) * H))\n",
        "            y1 = min(H-1, int((yc + h_n/2) * H))\n",
        "\n",
        "            patch = block[y0:y1, x0:x1].ravel()\n",
        "            k_loc = kurtosis(patch, fisher=False) if patch.size >= 2 else 0.0\n",
        "\n",
        "            if k_loc >= 4.5:         # only keep boxes over peaky patches\n",
        "                filtered.append((xc, yc, w_n, h_n))\n",
        "\n",
        "        # 3) continue with your existing scoring/NMS on boxes\n",
        "        if len(filtered) == 0:\n",
        "            boxes = []\n",
        "        else:\n",
        "            areas = [w*h for (_,_,w,h) in boxes]\n",
        "            H, W = fused.shape\n",
        "            kurt_vals = []\n",
        "            for (cx, cy, w_n, h_n) in boxes:\n",
        "                # convert normalized coords back to pixel indices\n",
        "                x0 = max(0, int((cx - w_n/2) * W))\n",
        "                x1 = min(W, int((cx + w_n/2) * W))\n",
        "                y0 = max(0, int((cy - h_n/2) * H))\n",
        "                y1 = min(H, int((cy + h_n/2) * H))\n",
        "\n",
        "                patch = block[y0:y1, x0:x1].flatten()\n",
        "                if patch.size < 2:\n",
        "                    kurt_vals.append(0.0)\n",
        "                else:\n",
        "                    # use excess‐kurtosis (Gaussian = 0)\n",
        "                    kurt_vals.append(kurtosis(patch, fisher=True))\n",
        "                    # 4) Convert to numpy arrays\n",
        "            a_arr = np.array(areas, dtype=float)\n",
        "            k_arr = np.array(kurt_vals, dtype=float)\n",
        "\n",
        "            # 5) Normalize safely\n",
        "            a_norm = (a_arr - a_arr.min()) / (np.ptp(a_arr) + 1e-6)\n",
        "            k_norm = (k_arr - k_arr.min()) / (np.ptp(k_arr) + 1e-6)\n",
        "\n",
        "            # 6) Composite score & NMS\n",
        "            scores = (0.9 * k_norm + 0.1 * a_norm).tolist()\n",
        "            assert len(scores) == len(boxes), f\"scores({len(scores)})!=boxes({len(boxes)})\"\n",
        "            boxes = non_max_suppression(boxes, scores, iou_thresh=0.3)\n",
        "            boxes = shrink_large_boxes(\n",
        "                boxes,\n",
        "                fused,\n",
        "                w_max=0.8,      # only shrink boxes ≥80% wide\n",
        "                h_max=0.8,      # or ≥80% tall\n",
        "                global_thresh=0.2   # threshold inside each patch\n",
        "            )\n",
        "\n",
        "            boxes, scores = remove_nested_boxes(boxes, scores)\n",
        "\n",
        "        moe_boxes = boxes\n",
        "\n",
        "        # 5) Convert all sources to raw pixel-space boxes\n",
        "        H, W = fused.shape\n",
        "\n",
        "        # a) MoE raw\n",
        "        moe_raw = []\n",
        "        for idx, (xc, yc, w_n, h_n) in enumerate(moe_boxes):\n",
        "            x0 = max(0, int((xc - w_n/2) * W))\n",
        "            x1 = min(W, int((xc + w_n/2) * W))\n",
        "            # carry original MoE confidence (if available) or use 1.0\n",
        "            moe_raw.append((x0, x1, { 'peak_snr': 1.0 }))\n",
        "        #from bounds import threshold_baseline_bounds\n",
        "\n",
        "        min_width   = 1\n",
        "        max_width   = W\n",
        "        snr_cutoff  = 7.0    # SNR threshold\n",
        "\n",
        "        raw_refined = []\n",
        "        for cx, cy, w_n, h_n in moe_boxes:\n",
        "            # 1) back-project from normalized back to pixel coords\n",
        "            l0 = max(0, int((cx - w_n/2) * W))\n",
        "            r0 = min(W, int((cx + w_n/2) * W))\n",
        "            if r0 - l0 < 2:\n",
        "                # too narrow to do any threshold search\n",
        "                continue\n",
        "\n",
        "            spec = block.mean(axis=0)[l0:r0]\n",
        "            if spec.size < 2:\n",
        "                # absolutely no data\n",
        "                continue\n",
        "\n",
        "            # 2) compute local threshold offsets\n",
        "            try:\n",
        "                l_ref, r_ref, meta = threshold_baseline_bounds_numpy(spec, p=0.02)\n",
        "            except (ValueError, IndexError):\n",
        "                # threshold routine couldn’t find a crossing\n",
        "                continue\n",
        "\n",
        "            # 3) clamp offsets to valid spec-indices\n",
        "            l_ref = int(np.clip(l_ref, 0, spec.size-2))\n",
        "            r_ref = int(np.clip(r_ref, 1, spec.size-1))\n",
        "            if r_ref <= l_ref:\n",
        "                continue\n",
        "\n",
        "            # 4) map back to global pixel coords\n",
        "            new_l = l0 + l_ref\n",
        "            new_r = l0 + r_ref\n",
        "            width = new_r - new_l\n",
        "\n",
        "            # 5) compute SNR and apply your filters\n",
        "            snr_estimate = (spec.max() - meta['noise_mean']) / (meta['spec_max'] - meta['noise_mean'])\n",
        "            if not (min_width < width < max_width and snr_estimate > snr_cutoff):\n",
        "                continue\n",
        "\n",
        "            # 6) append as raw pixel‐coords so fuse() can consume it\n",
        "            raw_refined.append((new_l, new_r, meta))\n",
        "\n",
        "\n",
        "        # c) Fallback raw (vertical + horizontal sweeps)\n",
        "        #from bounds import clipped_2D_bounds\n",
        "        stride, fwin = 50, 128\n",
        "        meb, mcb, ppm = 3, 1, 4\n",
        "        raw_fallback = []\n",
        "        # vertical\n",
        "        for start in range(0, W, stride):\n",
        "            sub_block = block[:, start:start + fwin]\n",
        "            if sub_block.shape[1] < 2:\n",
        "                continue\n",
        "            sub = sub_block.copy()\n",
        "            try:\n",
        "                l_rel, r_rel, meta = clipped_2D_bounds_numpy(sub, min_empty_bins=meb,\n",
        "                                                    min_clipped=mcb,\n",
        "                                                    peak_prominence=ppm)\n",
        "            except (ValueError, IndexError):\n",
        "                continue\n",
        "            gl, gr = start + l_rel, start + r_rel\n",
        "            w_pix = gr - gl\n",
        "            if not (min_width < w_pix < max_width): continue\n",
        "            raw_fallback.append((gl, gr, meta))\n",
        "        # horizontal\n",
        "        for t0 in range(0, H, stride):\n",
        "            sub_block = block[t0:t0 + fwin, :]\n",
        "            if sub_block.shape[0] < 2:\n",
        "                continue\n",
        "            sub = sub_block.copy()\n",
        "            try:\n",
        "                l_rel, r_rel, meta = clipped_2D_bounds_numpy(sub, min_empty_bins=meb,\n",
        "                                                    min_clipped=mcb,\n",
        "                                                    peak_prominence=ppm)\n",
        "            except (ValueError, IndexError):\n",
        "                continue\n",
        "            gt, gb = t0 + l_rel, t0 + r_rel\n",
        "            h_pix = gb - gt\n",
        "            if not (min_width < h_pix < max_width): continue\n",
        "            # tag horizontal boxes with top/bottom\n",
        "            meta_h = meta.copy(); meta_h.update({'top': gt, 'bottom': gb})\n",
        "            raw_fallback.append((0, W, meta_h))\n",
        "\n",
        "            # 6) Fuse all raw boxes (pixel-space)\n",
        "        all_raw = [moe_raw, raw_refined, raw_fallback]\n",
        "        fused_raw = fuse(all_raw, iou_thresh=0.5)\n",
        "\n",
        "        # 7) Normalize fused boxes back to YOLO format\n",
        "        yolo_boxes = []  # (class_id, xc, yc, w, h)\n",
        "        for l, r, meta in fused_raw:\n",
        "            if 'top' in meta:\n",
        "                x0, x1 = 0, W\n",
        "                y0, y1 = meta['top'], meta['bottom']\n",
        "            else:\n",
        "                x0, x1 = l, r\n",
        "                y0, y1 = 0, H\n",
        "            w_pix, h_pix = x1 - x0, y1 - y0\n",
        "            cx = x0 + w_pix/2; cy = y0 + h_pix/2\n",
        "            yolo_boxes.append((class_id,\n",
        "                                cx / W,\n",
        "                                cy / H,\n",
        "                                w_pix / W,\n",
        "                                h_pix / H))\n",
        "\n",
        "\n",
        "        f_start = fb.header['fch1'] + f0 * fb.header['foff']\n",
        "        f_stop  = fb.header['fch1'] + (f1 - 1) * fb.header['foff']\n",
        "        kurt_val = kurtosis(gray8.flatten(), fisher=False)\n",
        "        fn = f\"img_{kurt_val:0{pad_width}.2f}_f_{f_start:.4f}_{f_stop:.4f}.png\"\n",
        "        img_path = img_dir / fn\n",
        "        txt_path = lbl_dir / fn.replace(\".png\", \".txt\")\n",
        "\n",
        "        arr8 = (255 * (block - block.min()) / (np.ptp(block) + 1e-6)).astype(np.uint8)\n",
        "        img = Image.fromarray(np.stack([arr8]*3, axis=-1))\n",
        "        img.save(img_path)\n",
        "\n",
        "        full_image_threshold = 0.95\n",
        "        min_norm_area        = 0.005\n",
        "        # boxes is List[(x_c, y_c, w_n, h_n)]\n",
        "        boxes = [\n",
        "            (_, x_c, y_c, w_n, h_n)\n",
        "            for _, x_c, y_c, w_n, h_n in yolo_boxes\n",
        "            if min_norm_area <= (w_n * h_n) <= full_image_threshold\n",
        "        ]\n",
        "\n",
        "        with open(txt_path, \"w\") as f:\n",
        "            for _, x_c, y_c, w_n, h_n in boxes:\n",
        "                f.write(f\"{class_id} {x_c:.6f} {y_c:.6f} {w_n:.6f} {h_n:.6f}\\n\")\n",
        "\n",
        "\n",
        "        if len(boxes) == 0:\n",
        "            (lbl_dir/\"empty_labels.csv\").open(\"a\").write(f\"{fn},{h5_path},{ch_idx}\\n\")\n",
        "\n",
        "        vis_img = img.convert(\"RGB\")\n",
        "        draw    = ImageDraw.Draw(vis_img)\n",
        "        for _, x_c, y_c, w_n, h_n in boxes:\n",
        "            xc, yc = x_c*w_img, y_c*h_img\n",
        "            bw, bh = w_n*w_img, h_n*h_img\n",
        "            x0, y0 = int(xc - bw/2), int(yc - bh/2)\n",
        "            x1, y1 = int(xc + bw/2), int(yc + bh/2)\n",
        "            draw.rectangle([x0, y0, x1, y1], outline=\"red\", width=2)\n",
        "        vis_img.save(vis_dir/fn)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tasks = build_tasks(df)\n",
        "    train_set, val_set = split_tasks(tasks)\n",
        "\n",
        "    # 2) build job tuples of exactly what process_file unpacks:\n",
        "    job_args = []\n",
        "    BASE_DIR = \"/datax/scratch/jliang/dataset_moe_bounds-gate\"\n",
        "    NUM_WORKERS = 8\n",
        "    class_id = 0  # Assuming a single class for simplicity\n",
        "    pad_width = 4  # Zero-padding width for gidx in filenames\n",
        "    for idx, (h5_path, ch_idx) in enumerate(tasks):\n",
        "        job = (\n",
        "            h5_path,\n",
        "            [ch_idx],        # channels\n",
        "            [idx],           # gidxs\n",
        "            BASE_DIR,\n",
        "            class_id,\n",
        "            train_set,\n",
        "            val_set,\n",
        "            pad_width\n",
        "        )\n",
        "        job_args.append(job)\n",
        "\n",
        "    # 3) submit each job as a single argument\n",
        "    with ProcessPoolExecutor(max_workers=NUM_WORKERS) as exe:\n",
        "        futures = {\n",
        "            exe.submit(process_file, job): (job[0], job[1])\n",
        "            for job in job_args\n",
        "        }\n",
        "\n",
        "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
        "            h5, ch_list = futures[future]\n",
        "            try:\n",
        "                future.result()\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {h5} channel {ch_list}: {e}\")\n",
        "                traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48e59983",
      "metadata": {
        "id": "48e59983"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import cv2\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from skimage.segmentation import slic\n",
        "# from scipy import ndimage\n",
        "\n",
        "# # ─── 1) HEURISTIC COMPUTATION ─────────────────────────────────────────\n",
        "\n",
        "# def compute_heuristics(edge_map, gray_image):\n",
        "#     # edge_map: 2D float32 [0,1], gray_image: 2D uint8\n",
        "#     # 1) gradient magnitude\n",
        "#     gx = cv2.Sobel(gray_image, cv2.CV_32F, 1, 0, ksize=3)\n",
        "#     gy = cv2.Sobel(gray_image, cv2.CV_32F, 0, 1, ksize=3)\n",
        "#     grad = np.hypot(gx, gy)\n",
        "\n",
        "#     # 2) statistics\n",
        "#     density   = np.mean(edge_map > 0.2)\n",
        "#     align     = np.mean(grad[edge_map > 0.2]) if density>0 else 0.0\n",
        "#     entropy   = -np.sum(edge_map * np.log2(edge_map + 1e-8))\n",
        "#     # 3) linearity via eccentricity of components\n",
        "#     bw   = edge_map > 0.2\n",
        "#     lbl, n = ndimage.label(bw)\n",
        "#     props = ndimage.find_objects(lbl)\n",
        "#     eccs = []\n",
        "#     for i, sl in enumerate(props, start=1):\n",
        "#         region = (lbl[sl] == i).astype(np.uint8)\n",
        "#         # approximate by second moments → skip details here\n",
        "#         eccs.append(region.sum()>0 and region.sum() / (region.shape[0]*region.shape[1]))\n",
        "#     linearity = float(np.mean(eccs)) if eccs else 0.0\n",
        "\n",
        "#     if density < 0.01:\n",
        "#         return [0.0, 0.0, 0.0, 0.0]\n",
        "#     return [density, align, entropy, linearity]\n",
        "\n",
        "\n",
        "# # ─── 2) RULE-BASED GATE MODULE ──────────────────────────────────────────\n",
        "\n",
        "# class RuleGate(nn.Module):\n",
        "#     def __init__(self, w_align=1.0, w_ent=1.0, bias=0.0):\n",
        "#         super().__init__()\n",
        "#         # wrap as parameters if you want to tune via backprop:\n",
        "#         self.w_align = nn.Parameter(torch.tensor(w_align))\n",
        "#         self.w_ent   = nn.Parameter(torch.tensor(w_ent))\n",
        "#         self.bias    = nn.Parameter(torch.tensor(bias))\n",
        "\n",
        "#     def forward(self, h_sf, h_ued):\n",
        "#         # h_*: [B,4] tensors on CUDA\n",
        "#         align = h_ued[:,1] - h_sf[:,1]      # favors UED if >0\n",
        "#         ent   = h_sf[:,2] - h_ued[:,2]      # favors UED if >0\n",
        "#         score  = self.w_align * align + self.w_ent * ent + self.bias\n",
        "#         return torch.sigmoid(score)         # [B] weights for UED\n",
        "\n",
        "\n",
        "# class MoEBlock(nn.Module):\n",
        "#     def __init__(self, gate: RuleGate):\n",
        "#         super().__init__()\n",
        "#         self.gate = gate\n",
        "\n",
        "#     def forward(self, sf_map, ued_map, h_sf, h_ued):\n",
        "#         \"\"\"\n",
        "#         sf_map, ued_map: [B,H,W] floats on CUDA\n",
        "#         h_sf, h_ued:        [B,4] heuristic tensors on CUDA\n",
        "#         \"\"\"\n",
        "#         p = self.gate(h_sf, h_ued).view(-1,1,1)  # [B,1,1]\n",
        "#         return p * ued_map + (1 - p) * sf_map\n",
        "\n",
        "\n",
        "# # ─── 3) FULL PROCESSING LOOP ───────────────────────────────────────────\n",
        "\n",
        "# def process_image_patch(patch, sf_detector, gate_block, device='cuda'):\n",
        "#     \"\"\"\n",
        "#     patch: HxWx3 BGR uint8\n",
        "#     sf_detector: cv2.ximgproc StructuredEdgeDetection\n",
        "#     gate_block:   instance of MoEBlock on device\n",
        "#     \"\"\"\n",
        "#     # 1) compute both edge maps\n",
        "#     gray    = cv2.cvtColor(patch, cv2.COLOR_BGR2GRAY)\n",
        "#     sf_edges= sf_detector.detectEdges(np.float32(patch)/255.0).squeeze()\n",
        "#     gx = cv2.Sobel(gray, cv2.CV_32F, 1,0,3)\n",
        "#     gy = cv2.Sobel(gray, cv2.CV_32F, 0,1,3)\n",
        "#     ued_edges = np.hypot(gx, gy)\n",
        "#     ued_edges = (ued_edges/ued_edges.max()).astype(np.float32)\n",
        "\n",
        "#     # 2) superpixel segmentation\n",
        "#     segments = slic(patch, n_segments=100, compactness=10)\n",
        "\n",
        "#     # 3) gather heuristics and masks\n",
        "#     feats_sf, feats_ued, masks = [], [], []\n",
        "#     for seg in np.unique(segments):\n",
        "#         mask = (segments==seg)\n",
        "#         if mask.sum()<50: continue\n",
        "#         hsf = compute_heuristics(sf_edges*mask, gray)\n",
        "#         hud = compute_heuristics(ued_edges*mask, gray)\n",
        "#         if not any(hsf) and not any(hud): continue\n",
        "#         feats_sf.append(hsf)\n",
        "#         feats_ued.append(hud)\n",
        "#         masks.append(mask)\n",
        "\n",
        "#     if not masks:\n",
        "#         return np.zeros_like(sf_edges, np.float32)\n",
        "\n",
        "#     # 4) move to GPU\n",
        "#     h_sf = torch.tensor(feats_sf, device=device)\n",
        "#     h_ued= torch.tensor(feats_ued, device=device)\n",
        "#     sf_m = torch.tensor(np.stack([sf_edges[m] for m in masks]), device=device)\n",
        "#     ue_m = torch.tensor(np.stack([ued_edges[m] for m in masks]), device=device)\n",
        "\n",
        "#     # 5) fuse with gate\n",
        "#     fused_masks = gate_block(sf_m, ue_m, h_sf, h_ued)  # [N_pixels]\n",
        "#     fused = np.zeros_like(sf_edges, np.float32)\n",
        "#     for p, m in zip(fused_masks.cpu().numpy(), masks):\n",
        "#         fused[m] = p\n",
        "\n",
        "#     # 6) generate YOLO boxes\n",
        "#     return fused\n",
        "\n",
        "\n",
        "# # ─── 4) USAGE ─────────────────────────────────────────────────────────\n",
        "\n",
        "# # Initialize:\n",
        "# EDGE_MODEL_PATH = \"model.yml\"  # your SF model\n",
        "# sf_detector = cv2.ximgproc.createStructuredEdgeDetection(EDGE_MODEL_PATH)\n",
        "# gate        = RuleGate(w_align=2.0, w_ent=1.0, bias=-0.5).cuda()\n",
        "# moe_block   = MoEBlock(gate).cuda()\n",
        "\n",
        "# # For each patch:\n",
        "# # fused_map = process_image_patch(patch, sf_detector, moe_block)\n",
        "# # then threshold fused_map, label, convert to YOLO (cx,cy,w,h) as before.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bde136f",
      "metadata": {
        "id": "8bde136f"
      },
      "outputs": [],
      "source": [
        "# def compute_heuristics(image, edge_map):\n",
        "#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "#     gradient = sobel(gray)\n",
        "#     edge_density = np.mean(edge_map > 0.2)\n",
        "#     edge_gradient_alignment = np.mean(gradient[edge_map > 0.2])\n",
        "#     edge_entropy = -np.sum(edge_map * np.log2(edge_map + 1e-8))\n",
        "#     lin = linearity_score(edge_map)\n",
        "#     if edge_density < 0.01:\n",
        "#         return [0.0, 0.0, 0.0, 0.0]\n",
        "#     return [edge_density, edge_gradient_alignment, edge_entropy, lin]\n",
        "\n",
        "# def pseudo_labels(image, sf_edges, ued_edges):\n",
        "#     h_sf  = compute_heuristics(image, sf_edges)\n",
        "#     h_ued = compute_heuristics(image, ued_edges)\n",
        "#     # if UED better alignment & lower entropy, choose UED\n",
        "#     if h_ued[1] > h_sf[1] and h_ued[2] < h_sf[2]:\n",
        "#         return 0\n",
        "#     else:\n",
        "#         return 1\n",
        "\n",
        "# # PyTorch gating network (input_dim=8 for 4 heuristics each)\n",
        "# class GatingNet(nn.Module):\n",
        "#     def __init__(self, input_dim=8, hidden=16):\n",
        "#         super().__init__()\n",
        "#         self.net = nn.Sequential(\n",
        "#             nn.Linear(input_dim, hidden),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(hidden, 2)\n",
        "#         )\n",
        "#     def forward(self, x):\n",
        "#         return self.net(x)\n",
        "\n",
        "# # Train gating model in PyTorch\n",
        "# def train_gating_model_torch(images, sf_edge_maps, ued_edge_maps,\n",
        "#                              batch_size=64, epochs=10, lr=1e-3,\n",
        "#                              device='cuda'):\n",
        "#     # 1) Collect features & labels\n",
        "#     feats, labels = [], []\n",
        "#     for img, sf, ued in zip(images, sf_edge_maps, ued_edge_maps):\n",
        "#         segments = slic(img_as_float(img), n_segments=100, compactness=10)\n",
        "#         for seg_val in np.unique(segments):\n",
        "#             mask = (segments == seg_val)\n",
        "#             if mask.sum() < 50:\n",
        "#                 continue\n",
        "#             sf_patch, ued_patch = sf * mask, ued * mask\n",
        "#             if sf_patch.mean() + ued_patch.mean() < 0.01:\n",
        "#                 continue\n",
        "#             h_sf  = compute_heuristics(img, sf_patch)\n",
        "#             h_ued = compute_heuristics(img, ued_patch)\n",
        "#             feats.append(np.array(h_sf + h_ued, dtype=np.float32))\n",
        "#             labels.append(pseudo_labels(img, sf_patch, ued_patch))\n",
        "#     if len(set(labels)) < 2:\n",
        "#         return None, None, None\n",
        "\n",
        "#     # 2) Build tensors & standardize\n",
        "#     X = torch.tensor(feats)  # (N, 8)\n",
        "#     y = torch.tensor(labels, dtype=torch.long)  # (N,)\n",
        "#     mean, std = X.mean(dim=0, keepdim=True), X.std(dim=0, keepdim=True) + 1e-6\n",
        "#     X = (X - mean) / std\n",
        "\n",
        "#     # 3) DataLoader\n",
        "#     dataset = TensorDataset(X, y)\n",
        "#     loader  = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "#     # 4) Model, loss, optimizer\n",
        "#     model = GatingNet(input_dim=X.shape[1]).to(device)\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#     # 5) Training loop\n",
        "#     model.train()\n",
        "#     for epoch in range(epochs):\n",
        "#         total_loss = 0.0\n",
        "#         for xb, yb in loader:\n",
        "#             xb, yb = xb.to(device), yb.to(device)\n",
        "#             logits = model(xb)\n",
        "#             loss   = criterion(logits, yb)\n",
        "#             optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "#             total_loss += loss.item()\n",
        "#         print(f\"Epoch {epoch+1}/{epochs} - loss: {total_loss/len(loader):.4f}\")\n",
        "\n",
        "#     return model, mean.to(device), std.to(device)\n",
        "\n",
        "# # Apply gating model to fuse edges\n",
        "# def apply_gating_model_torch(image, sf_edges, ued_edges,\n",
        "#                              model, mean, std, device='cuda'):\n",
        "#     segments = slic(img_as_float(image), n_segments=100, compactness=10)\n",
        "#     feats, masks = [], []\n",
        "#     for seg_val in np.unique(segments):\n",
        "#         mask = (segments == seg_val)\n",
        "#         if mask.sum() < 50:\n",
        "#             continue\n",
        "#         sf_patch, ued_patch = sf_edges * mask, ued_edges * mask\n",
        "#         if sf_patch.mean() + ued_patch.mean() < 0.01:\n",
        "#             continue\n",
        "#         h_sf, h_ued = compute_heuristics(image, sf_patch), compute_heuristics(image, ued_patch)\n",
        "#         f = np.array(h_sf + h_ued, dtype=np.float32)\n",
        "#         if not np.all(np.isfinite(f)) or np.allclose(f, 0, atol=1e-3):\n",
        "#             continue\n",
        "#         feats.append(f)\n",
        "#         masks.append(mask)\n",
        "\n",
        "#     if not feats:\n",
        "#         return np.zeros_like(sf_edges, dtype=float)\n",
        "\n",
        "#     X = torch.tensor(feats).to(device)\n",
        "#     X = (X - mean) / std\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         probs = torch.softmax(model(X), dim=1)[:,1].cpu().numpy()\n",
        "\n",
        "#     gated = np.zeros_like(sf_edges, dtype=float)\n",
        "#     for p, mask in zip(probs, masks):\n",
        "#         gated[mask] = p * sf_edges[mask] + (1 - p) * ued_edges[mask]\n",
        "#     return gated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c822d4e5",
      "metadata": {
        "id": "c822d4e5"
      },
      "outputs": [],
      "source": [
        "# def edge_detection(gray, factor=6, dilation=(3, 3)):\n",
        "#         # STEP 1: apply power threshold to suppress background\n",
        "#         median = np.median(gray)\n",
        "#         mad = np.median(np.abs(gray - median))\n",
        "#         power_mask = gray > (median + factor * mad)\n",
        "\n",
        "#         # STEP 2: edge detection on filtered signal only\n",
        "#         grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
        "#         grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
        "#         magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
        "#         magnitude *= power_mask  # mask out noise\n",
        "#         magnitude = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "\n",
        "#         # STEP 3: binary threshold + dilation\n",
        "#         binary = magnitude > 50  # you can tune this\n",
        "#         binary = scipy.ndimage.binary_dilation(binary, structure=np.ones(dilation))\n",
        "#         labeled, n_objs = scipy.ndimage.label(binary)\n",
        "#         slices = scipy.ndimage.find_objects(labeled)\n",
        "#         return slices\n",
        "\n",
        "# EDGE_MODEL_PATH = '/home/jliang/gbt-rfi/model.yml.gz'\n",
        "# EDGE_DETECTOR = cv2.ximgproc.createStructuredEdgeDetection(EDGE_MODEL_PATH)\n",
        "\n",
        "# def detect_edges(spectrogram):\n",
        "#     img = (spectrogram - spectrogram.min()) / (spectrogram.ptp() + 1e-6)\n",
        "#     img_3ch = cv2.merge([img.astype(np.float32)] * 3)\n",
        "#     return EDGE_DETECTOR.detectEdges(img_3ch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06fea6ff",
      "metadata": {
        "id": "06fea6ff"
      },
      "outputs": [],
      "source": [
        "# def extract_one_sample(h5_path, ch, edge_model_path='/home/jliang/gbt-rfi/model.yml.gz'):\n",
        "#     try:\n",
        "#         EDGE_DETECTOR = cv2.ximgproc.createStructuredEdgeDetection(edge_model_path)\n",
        "#         fb = bl.Waterfall(h5_path, load_data=True)\n",
        "#         data = 10 * np.log10(fb.data.squeeze())\n",
        "\n",
        "#         nfpc = fb.header.get(\"nfpc\", 1024)\n",
        "#         f0, f1 = ch * nfpc, (ch + 1) * nfpc\n",
        "#         block = data[:, f0:f1]\n",
        "\n",
        "#         n_cols = block.shape[1]\n",
        "#         low, high = int(0.1 * n_cols), int(0.9 * n_cols)\n",
        "#         block_middle80 = block[:, low:high]\n",
        "\n",
        "#         vert_means = block_middle80.mean(axis=0)\n",
        "#         center = np.argmax(vert_means)\n",
        "#         left_col, right_col = center - 1, center + 1\n",
        "#         if left_col >= 0 and right_col < block_middle80.shape[1]:\n",
        "#             block_middle80[:, center] = (block_middle80[:, left_col] + block_middle80[:, right_col]) / 2\n",
        "#         elif left_col >= 0:\n",
        "#             block_middle80[:, center] = block_middle80[:, left_col]\n",
        "#         elif right_col < block_middle80.shape[1]:\n",
        "#             block_middle80[:, center] = block_middle80[:, right_col]\n",
        "\n",
        "#         img_norm = (block_middle80 - block_middle80.min()) / (block_middle80.ptp() + 1e-6)\n",
        "#         img_rgb = cv2.merge([img_norm.astype(np.float32)] * 3)\n",
        "\n",
        "#         sf_edges = EDGE_DETECTOR.detectEdges(img_rgb)\n",
        "\n",
        "#         gray = (255 * img_norm).astype(np.uint8)\n",
        "#         grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
        "#         grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
        "#         magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
        "#         magnitude = cv2.normalize(magnitude, None, 0, 1, cv2.NORM_MINMAX)\n",
        "\n",
        "#         sf_mean = np.mean(sf_edges)\n",
        "#         ued_mean = np.mean(magnitude)\n",
        "\n",
        "#         if sf_mean + ued_mean < 0.005:  # threshold to skip empty or background-only blocks\n",
        "#             return None  # skip empty or background-only blocks\n",
        "\n",
        "\n",
        "#         return (img_rgb, sf_edges, magnitude)\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error processing {h5_path} ch {ch}: {e}\")\n",
        "#         return None\n",
        "\n",
        "# def extract_samples_batched(df, total_samples=40000, batch_size=32, num_workers=4):\n",
        "#     all_tasks = []\n",
        "#     for _, row in df.iterrows():\n",
        "#         h5 = row[\".h5 path\"]\n",
        "#         fb = bl.Waterfall(h5, load_data=False)\n",
        "#         nfreq = fb.header.get(\"nchans\")\n",
        "#         nfpc = fb.header.get(\"nfpc\", 1024)\n",
        "#         for ch in range(nfreq // nfpc):\n",
        "#             all_tasks.append((h5, ch))\n",
        "\n",
        "#     shuffle(all_tasks)\n",
        "#     selected_tasks = all_tasks[:total_samples]\n",
        "\n",
        "#     train_images, train_sf_edges, train_ued_edges = [], [], []\n",
        "#     for i in tqdm(range(0, total_samples, batch_size)):\n",
        "#         batch = selected_tasks[i:i+batch_size]\n",
        "#         with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
        "#             futures = [executor.submit(extract_one_sample, h5, ch) for h5, ch in batch]\n",
        "#             for future in as_completed(futures):\n",
        "#                 result = future.result()\n",
        "#                 if result:\n",
        "#                     img_rgb, sf_edge, ued_edge = result\n",
        "#                     train_images.append(img_rgb)\n",
        "#                     train_sf_edges.append(sf_edge)\n",
        "#                     train_ued_edges.append(ued_edge)\n",
        "\n",
        "#     return train_images, train_sf_edges, train_ued_edges\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50421249",
      "metadata": {
        "id": "50421249"
      },
      "outputs": [],
      "source": [
        "# # 4) SET UP LOGGING\n",
        "# logging.basicConfig(\n",
        "#     level=logging.INFO,\n",
        "#     format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "#     datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "# )\n",
        "\n",
        "# from gating_model import train_gating_model_torch\n",
        "\n",
        "# def build_tasks(df):\n",
        "#     \"\"\"\n",
        "#     Build a flat list of (h5_path, channel_idx) from your DataFrame,\n",
        "#     skipping any channels that fall into known notch filter frequency ranges.\n",
        "#     \"\"\"\n",
        "#     GBT_NOTCH_FILTERS = {\n",
        "#         \"L\": [(1200, 1340)],\n",
        "#         \"S\": [(2300, 2360)],\n",
        "#     }\n",
        "\n",
        "#     tasks = []\n",
        "\n",
        "#     for _, row in df.iterrows():\n",
        "#         h5 = row[\".h5 path\"]\n",
        "#         band = row[\"Band\"]  # e.g., 'L', 'S', etc.\n",
        "#         fb = bl.Waterfall(h5, load_data=False)\n",
        "#         nfreq = fb.header.get(\"nchans\")\n",
        "#         nfpc = fb.header.get(\"nfpc\", 1024)\n",
        "#         fch1 = fb.header[\"fch1\"]\n",
        "#         foff = fb.header[\"foff\"]\n",
        "#         n_coarse = nfreq // nfpc\n",
        "\n",
        "#         for ch in range(n_coarse):\n",
        "#             f0 = fch1 + ch * nfpc * foff\n",
        "#             f1 = fch1 + (ch + 1) * nfpc * foff\n",
        "#             f_min, f_max = sorted([f0, f1])\n",
        "\n",
        "#             # Check against notch filter exclusion ranges\n",
        "#             skip = False\n",
        "#             if band in GBT_NOTCH_FILTERS:\n",
        "#                 for lo, hi in GBT_NOTCH_FILTERS[band]:\n",
        "#                     if lo <= f_min <= hi or lo <= f_max <= hi:\n",
        "#                         skip = True\n",
        "#                         break\n",
        "#             if not skip:\n",
        "#                 tasks.append((h5, ch))\n",
        "\n",
        "#     return tasks\n",
        "\n",
        "\n",
        "# def split_tasks(tasks, train_frac=0.8, seed=42):\n",
        "#     \"\"\"\n",
        "#     Shuffle & split the flat task list into train vs. val sets.\n",
        "#     Returns two sets of (h5_path, channel_idx).\n",
        "#     \"\"\"\n",
        "#     random.seed(seed)\n",
        "#     shuffled = tasks.copy()\n",
        "#     random.shuffle(shuffled)\n",
        "#     cut = int(train_frac * len(shuffled))\n",
        "#     train = set(shuffled[:cut])\n",
        "#     val   = set(shuffled[cut:])\n",
        "#     return train, val\n",
        "\n",
        "# def process_file(job):\n",
        "#     \"\"\"\n",
        "#     job is a tuple:\n",
        "#       (h5_path, channels, global_indices,\n",
        "#        base_dir, factor, dilation,\n",
        "#        class_id, train_set, val_set,\n",
        "#        pad_width, model, mean, std)\n",
        "#     \"\"\"\n",
        "#     (h5_path, channels, global_indices,\n",
        "#      base_dir, factor, dilation,\n",
        "#      class_id, train_set, val_set,\n",
        "#      pad_width, model, mean, std) = job\n",
        "\n",
        "#     # 1) Prepare device & trained gate\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     model = model.to(device)\n",
        "#     model.eval()\n",
        "\n",
        "#     # 2) Load Structured Forest detector once\n",
        "#     EDGE_MODEL_PATH = \"/home/jliang/gbt-rfi/model.yml.gz\"\n",
        "#     sf_detector = cv2.ximgproc.createStructuredEdgeDetection(EDGE_MODEL_PATH)\n",
        "\n",
        "#     # 3) Load the .h5 file\n",
        "#     fb   = bl.Waterfall(h5_path, load_data=True)\n",
        "#     data = 10 * np.log10(fb.data.squeeze())   # (ntime, nfreq)\n",
        "\n",
        "#     # 4) Process each coarse channel\n",
        "#     for ch_idx, gidx in zip(channels, global_indices):\n",
        "#         # decide train vs val\n",
        "#         subset = \"train\" if (h5_path, ch_idx) in train_set else \"val\"\n",
        "\n",
        "#         # make sure directories exist\n",
        "#         img_dir = Path(base_dir)/subset/\"images\"\n",
        "#         lbl_dir = Path(base_dir)/subset/\"labels\"\n",
        "#         vis_dir = Path(base_dir)/\"visualization\"/subset\n",
        "#         for d in (img_dir, lbl_dir, vis_dir):\n",
        "#             d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#         # extract the 80% middle of the block\n",
        "#         cw = fb.header.get(\"nfpc\", 1024)\n",
        "#         f0, f1 = ch_idx*cw, (ch_idx+1)*cw\n",
        "#         block = data[:, f0:f1]\n",
        "#         low, high = int(0.1*cw), int(0.9*cw)\n",
        "#         block = block[:, low:high]\n",
        "\n",
        "#         # remove single‐column artifact\n",
        "#         col_means = block.mean(axis=0)\n",
        "#         c = int(np.argmax(col_means))\n",
        "#         if 0 < c < block.shape[1]-1:\n",
        "#             block[:,c] = 0.5*(block[:,c-1] + block[:,c+1])\n",
        "\n",
        "#         # normalize & build RGB input for SF\n",
        "#         norm = (block - block.min())/(block.ptp()+1e-6)\n",
        "#         img_rgb = cv2.merge([norm.astype(np.float32)]*3)\n",
        "\n",
        "#         # compute the two expert edge‐maps\n",
        "#         sf_edge_map  = sf_detector.detectEdges(img_rgb).squeeze()\n",
        "#         gray_uint8  = (255*norm).astype(np.uint8)\n",
        "#         gx = cv2.Sobel(gray_uint8, cv2.CV_32F, 1, 0, ksize=3)\n",
        "#         gy = cv2.Sobel(gray_uint8, cv2.CV_32F, 0, 1, ksize=3)\n",
        "#         ued_edge_map = np.hypot(gx, gy)\n",
        "#         ued_edge_map = cv2.normalize(ued_edge_map, None, 0.0, 1.0, cv2.NORM_MINMAX)\n",
        "\n",
        "#         # 5) Fuse using the PyTorch gate\n",
        "#         fused_edges = apply_gating_model_torch(\n",
        "#             img_rgb, sf_edge_map, ued_edge_map,\n",
        "#             model, mean, std,\n",
        "#             device=device\n",
        "#         )\n",
        "\n",
        "#         # 6) Generate & save YOLO boxes\n",
        "#         boxes = generate_yolo_boxes(fused_edges, threshold=0.1)\n",
        "\n",
        "#         # Dimensions for filtering\n",
        "#         h_img, w_img = block.shape\n",
        "#         min_size = 3                   # px\n",
        "#         full_image_threshold = 0.95    # normalized area\n",
        "\n",
        "#         # Filter\n",
        "#         final_boxes = []\n",
        "#         for x_c, y_c, w_n, h_n, area in boxes:\n",
        "#             if area > full_image_threshold:\n",
        "#                 continue\n",
        "#             if (w_n * w_img) < min_size or (h_n * h_img) < min_size:\n",
        "#                 continue\n",
        "#             final_boxes.append((x_c, y_c, w_n, h_n))\n",
        "\n",
        "#         # Calculate frequency range of this coarse channel\n",
        "#         f_start = fb.header['fch1'] + f0 * fb.header['foff']\n",
        "#         f_stop  = fb.header['fch1'] + (f1 - 1) * fb.header['foff']\n",
        "\n",
        "#         # Build filename\n",
        "#         fn = f\"img_{gidx:0{pad_width}d}_f_{f_start:.4f}_{f_stop:.4f}.png\"\n",
        "#         img_path = img_dir / fn\n",
        "#         txt_path = lbl_dir / fn.replace(\".png\", \".txt\")\n",
        "\n",
        "#         # Save image\n",
        "#         arr8 = (255 * (block - block.min()) / (block.ptp() + 1e-6)).astype(np.uint8)\n",
        "#         img = Image.fromarray(np.stack([arr8]*3, axis=-1))\n",
        "#         img.save(img_path)\n",
        "\n",
        "#         # Write YOLO labels\n",
        "#         with open(txt_path, \"w\") as f:\n",
        "#             for x_c, y_c, w_n, h_n in final_boxes:\n",
        "#                 f.write(f\"{class_id} {x_c:.6f} {y_c:.6f} {w_n:.6f} {h_n:.6f}\\n\")\n",
        "\n",
        "#         # If empty, log it\n",
        "#         if not final_boxes:\n",
        "#             (lbl_dir/\"empty_labels.csv\").open(\"a\").write(f\"{fn},{h5_path},{ch_idx}\\n\")\n",
        "\n",
        "#         # Draw & save visualization\n",
        "#         vis_img = img.convert(\"RGB\")\n",
        "#         draw    = ImageDraw.Draw(vis_img)\n",
        "#         for x_c, y_c, w_n, h_n in final_boxes:\n",
        "#             xc, yc = x_c*w_img, y_c*h_img\n",
        "#             bw, bh = w_n*w_img, h_n*h_img\n",
        "#             x0, y0 = int(xc - bw/2), int(yc - bh/2)\n",
        "#             x1, y1 = int(xc + bw/2), int(yc + bh/2)\n",
        "#             draw.rectangle([x0, y0, x1, y1], outline=\"red\", width=2)\n",
        "#         vis_img.save(vis_dir/fn)\n",
        "\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # === your settings ===\n",
        "#     BASE_DIR             = \"/datax/scratch/jliang/dataset_moe\"\n",
        "#     FACTOR              = 6\n",
        "#     DILATION = (30, 30)\n",
        "#     CLASS_ID             = 0\n",
        "#     TRAIN_FRAC           = 0.8\n",
        "#     SEED                 = 42\n",
        "#     NUM_WORKERS          = 2\n",
        "\n",
        "#     # --- assume you already have a DataFrame `df` with at least 'h5_path' and optionally 'nchans' ---\n",
        "#     # df = pd.read_csv(...)  # or however you built it\n",
        "\n",
        "#     # 1) Build & split tasks\n",
        "#     tasks = build_tasks(df)\n",
        "#     train_set, val_set = split_tasks(tasks, TRAIN_FRAC, SEED)\n",
        "\n",
        "#     # 2) compute zero-pad width from total images\n",
        "#     pad_width = len(str(len(tasks) - 1))\n",
        "\n",
        "#     # 3) regroup tasks by file to load each .h5 only once\n",
        "#     jobs = {}\n",
        "#     for gidx, (h5, ch) in enumerate(tasks):\n",
        "#         jobs.setdefault(h5, {\"chs\": [], \"gidxs\": []})\n",
        "#         jobs[h5][\"chs\"].append(ch)\n",
        "#         jobs[h5][\"gidxs\"].append(gidx)\n",
        "\n",
        "#     # 4) prepare job‐tuples\n",
        "#     job_list = []\n",
        "#     for h5, info in jobs.items():\n",
        "#         job_list.append((\n",
        "#             h5,\n",
        "#             info[\"chs\"],\n",
        "#             info[\"gidxs\"],\n",
        "#             BASE_DIR,\n",
        "#             FACTOR,\n",
        "#             DILATION,\n",
        "#             CLASS_ID,\n",
        "#             train_set,\n",
        "#             val_set,\n",
        "#             pad_width\n",
        "#         ))\n",
        "\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#     # train_images, train_sf_edges, train_ued_edges = extract_samples_batched(\n",
        "#     #     df, total_samples=40000, num_workers=2\n",
        "#     # )\n",
        "\n",
        "#     # model, mean, std = train_gating_model_torch(\n",
        "#     #     train_images,\n",
        "#     #     train_sf_edges,\n",
        "#     #     train_ued_edges,\n",
        "#     #     batch_size=64,\n",
        "#     #     epochs=10,\n",
        "#     #     lr=1e-3,\n",
        "#     #     device=device\n",
        "#     # )\n",
        "\n",
        "#     # if model is None:\n",
        "#     #     logging.warning(\"Skipping inference because gate net couldn't train.\")\n",
        "#     #     exit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae743955",
      "metadata": {
        "id": "ae743955"
      },
      "outputs": [],
      "source": [
        "# import h5py, numpy as np, torch, logging\n",
        "# from torch.utils.data import IterableDataset, DataLoader\n",
        "# import cv2\n",
        "# from skimage.segmentation import slic\n",
        "# from yolo_moe_pytorch_channel import (\n",
        "#     apply_gating_model_torch,\n",
        "#     generate_yolo_boxes,\n",
        "#     compute_heuristics,\n",
        "#     pseudo_labels\n",
        "# )\n",
        "# from PIL import Image, ImageDraw\n",
        "\n",
        "# # 1) Streaming dataset\n",
        "# class H5ChannelDataset(IterableDataset):\n",
        "#     def __init__(self, df, nfpc=1024):\n",
        "#         self.paths = df[\".h5 path\"].tolist()\n",
        "#         self.nfpc  = nfpc\n",
        "#     def __iter__(self):\n",
        "#         for p in self.paths:\n",
        "#             with h5py.File(p, \"r\") as f:\n",
        "#                 d = f[\"data\"]\n",
        "#                 n_coarse = d.shape[1] // self.nfpc\n",
        "#                 for ch in range(n_coarse):\n",
        "#                     yield d[:, ch*self.nfpc:(ch+1)*self.nfpc].astype(np.float32)\n",
        "\n",
        "# # 2) Pre- and post-processing helpers\n",
        "# EDGE_MODEL_PATH = \"/home/jliang/gbt-rfi/model.yml.gz\"\n",
        "# sf_detector    = cv2.ximgproc.createStructuredEdgeDetection(EDGE_MODEL_PATH)\n",
        "\n",
        "# def preprocess(block):\n",
        "#     norm = (block - block.min())/(block.ptp()+1e-6)\n",
        "#     img_rgb = np.stack([norm]*3, axis=-1).astype(np.float32)\n",
        "#     sf_map   = sf_detector.detectEdges(img_rgb).squeeze()\n",
        "#     gray8    = (255*norm).astype(np.uint8)\n",
        "#     gx = cv2.Sobel(gray8, cv2.CV_32F, 1,0,3)\n",
        "#     gy = cv2.Sobel(gray8, cv2.CV_32F, 0,1,3)\n",
        "#     ued_map = np.hypot(gx, gy)\n",
        "#     ued_map = cv2.normalize(ued_map, None, 0.0, 1.0, cv2.NORM_MINMAX)\n",
        "#     return img_rgb, sf_map, ued_map\n",
        "\n",
        "# def segment_patches(img, sf, ued):\n",
        "#     segs = slic(img, n_segments=100, compactness=10)\n",
        "#     for v in np.unique(segs):\n",
        "#         m = segs==v\n",
        "#         if m.sum()<50: continue\n",
        "#         sf_p, ued_p = sf*m, ued*m\n",
        "#         if sf_p.mean()+ued_p.mean()<0.01: continue\n",
        "#         yield sf_p, ued_p\n",
        "\n",
        "# # 3) Collate: build feature / label tensors\n",
        "# def collate_fn(batch):\n",
        "#     feats, labs = [], []\n",
        "#     for block in batch:\n",
        "#         img, sf, ued = preprocess(block)\n",
        "#         for sf_p, ued_p in segment_patches(img, sf, ued):\n",
        "#             h_sf = compute_heuristics(sf_p, img)\n",
        "#             h_ued= compute_heuristics(ued_p, img)\n",
        "#             feats.append(h_sf + h_ued)\n",
        "#             labs.append(pseudo_labels(img, sf_p, ued_p))\n",
        "#     if not feats: return None\n",
        "#     return torch.tensor(feats), torch.tensor(labs)\n",
        "\n",
        "# # 4) Training\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# dataset = H5ChannelDataset(df)\n",
        "# loader  = DataLoader(dataset,\n",
        "#                      batch_size=4,\n",
        "#                      num_workers=4,\n",
        "#                      persistent_workers=True,\n",
        "#                      collate_fn=collate_fn)\n",
        "\n",
        "# from your_module import GatingNet  # the BatchNorm version\n",
        "# net   = GatingNet().to(device)\n",
        "# opt   = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
        "# crit  = nn.CrossEntropyLoss()\n",
        "\n",
        "# for epoch in range(10):\n",
        "#     for batch in loader:\n",
        "#         if batch is None: continue\n",
        "#         X, y = batch\n",
        "#         X, y = X.to(device), y.to(device)\n",
        "#         logits = net(X)\n",
        "#         loss   = crit(logits, y)\n",
        "#         opt.zero_grad()\n",
        "#         loss.backward()\n",
        "#         opt.step()\n",
        "#     print(f\"Epoch {epoch+1}/10 done\")\n",
        "\n",
        "# # 5) Build job_list as before, but append (net) instead of (mean,std)\n",
        "# # and in process_file call apply_gating_model_torch(img, sf, ued, net, device=...)\n",
        "\n",
        "# # 6) Inference uses your existing process_file, no further change needed.\n",
        "\n",
        "\n",
        "# # ─── 3) Build job list for inference ────────────────────────────────────\n",
        "\n",
        "# # your existing build_tasks(), split_tasks(), jobs grouping...\n",
        "# tasks    = build_tasks(df)\n",
        "# train_set, val_set = split_tasks(tasks, TRAIN_FRAC, SEED)\n",
        "# pad_width = len(str(len(tasks)))\n",
        "\n",
        "# jobs = {}\n",
        "# for gidx,(h5,ch) in enumerate(tasks):\n",
        "#     jobs.setdefault(h5, {\"chs\":[], \"gidxs\":[]})\n",
        "#     jobs[h5][\"chs\"].append(ch)\n",
        "#     jobs[h5][\"gidxs\"].append(gidx)\n",
        "\n",
        "# job_list = []\n",
        "# for h5,info in jobs.items():\n",
        "#     job_list.append((h5,\n",
        "#                      info[\"chs\"],\n",
        "#                      info[\"gidxs\"],\n",
        "#                      BASE_DIR,\n",
        "#                      FACTOR,\n",
        "#                      DILATION,\n",
        "#                      CLASS_ID,\n",
        "#                      train_set,\n",
        "#                      val_set,\n",
        "#                      pad_width,\n",
        "#                      gate_net, mean, std))\n",
        "\n",
        "# # ─── 4) Run inference in parallel ───────────────────────────────────────\n",
        "# from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "# with ProcessPoolExecutor(max_workers=NUM_WORKERS) as exe:\n",
        "#     for _ in exe.map(process_file, job_list):\n",
        "#         pass\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
