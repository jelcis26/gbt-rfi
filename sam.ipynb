{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9774b741",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Install SAM (and dependencies)\n",
    "pip install torch torchvision\n",
    "git clone https://github.com/facebookresearch/segment-anything.git\n",
    "cd segment-anything\n",
    "pip install -e .             # installs `segment_anything`\n",
    "\n",
    "# 2. (Optional) Also install albumentations for data augmentation\n",
    "pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22449a19",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# train_sam.py\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "# 3. Dataset definition\n",
    "class MaskDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 images_dir: str,\n",
    "                 masks_dir: str,\n",
    "                 transform=None):\n",
    "        self.images = sorted(glob(os.path.join(images_dir, \"*.jpg\")))\n",
    "        self.masks = [os.path.join(masks_dir, os.path.basename(p).replace('.jpg','.png'))\n",
    "                      for p in self.images]\n",
    "        self.transform = transform or T.Compose([\n",
    "            T.Resize((512,512)),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.images[idx]).convert(\"RGB\")\n",
    "        msk = Image.open(self.masks[idx]).convert(\"L\")  # binary mask\n",
    "        img = self.transform(img)\n",
    "        msk = (self.transform(msk) > 0.5).float()\n",
    "        return img, msk\n",
    "\n",
    "# 4. Set up dataloaders\n",
    "train_ds = MaskDataset(\"data/images/train\", \"data/masks/train\")\n",
    "val_ds   = MaskDataset(\"data/images/val\",   \"data/masks/val\")\n",
    "train_dl = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=4)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=8, shuffle=False,num_workers=4)\n",
    "\n",
    "# 5. Load a pretrained SAM + attach simple mask head\n",
    "#    We'll fine-tune the 'prompt_encoder' + mask decoder layers.\n",
    "sam = sam_model_registry[\"vit_b\"](checkpoint=\"sam_vit_b_01ec64.pth\")\n",
    "# freeze image encoder\n",
    "for p in sam.image_encoder.parameters(): p.requires_grad = False\n",
    "\n",
    "# optionally: freeze the prompt encoder and only train the mask decoder:\n",
    "# for p in sam.prompt_encoder.parameters(): p.requires_grad = False\n",
    "\n",
    "sam.to(device := torch.device(\"cuda\"))\n",
    "optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad, sam.parameters()), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 6. Training loop\n",
    "for epoch in range(10):\n",
    "    sam.train()\n",
    "    total_loss = 0\n",
    "    for imgs, masks in train_dl:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        # SAM forward: returns low-level mask logits\n",
    "        outputs = sam.mask_decoder(\n",
    "            image_embeddings = sam.image_encoder(imgs),\n",
    "            # for simplicity we don't pass any prompt; decoder can predict full mask\n",
    "            # you can experiment supplying sparse prompts if you have them\n",
    "            sparse_prompt_embeddings=None,\n",
    "            dense_prompt_embeddings=None,\n",
    "        )  # dict with 'pred_mask'\n",
    "        logits = outputs['pred_mask']\n",
    "        loss = criterion(logits, masks.unsqueeze(1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch} train loss: {total_loss/len(train_dl):.4f}\")\n",
    "\n",
    "    # validation\n",
    "    sam.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for imgs, masks in val_dl:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            out = sam.mask_decoder(\n",
    "                image_embeddings = sam.image_encoder(imgs),\n",
    "                sparse_prompt_embeddings=None,\n",
    "                dense_prompt_embeddings=None,\n",
    "            )\n",
    "            val_loss += criterion(out['pred_mask'], masks.unsqueeze(1)).item()\n",
    "        print(f\"   val loss: {val_loss/len(val_dl):.4f}\")\n",
    "\n",
    "# 7. Save your fine-tuned SAM\n",
    "torch.save(sam.state_dict(), \"sam_finetuned.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
