{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81b1b1d6",
      "metadata": {
        "id": "81b1b1d6",
        "outputId": "c49a5694-57c3-431a-cb64-32661851a528"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: bitshuffle in /mnt_home/jliang/.local/lib/python3.7/site-packages (0.5.1)\n",
            "Requirement already satisfied: h5py>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from bitshuffle) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from bitshuffle) (1.20.3)\n",
            "Requirement already satisfied: setuptools>=0.7 in /opt/conda/lib/python3.7/site-packages (from bitshuffle) (62.3.2)\n",
            "Requirement already satisfied: Cython>=0.19 in /opt/conda/lib/python3.7/site-packages (from bitshuffle) (0.29.30)\n",
            "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py>=2.4.0->bitshuffle) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (7.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install bitshuffle\n",
        "%pip install Pillow\n",
        "%pip install scikit-image\n",
        "%pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2fe6857",
      "metadata": {
        "id": "a2fe6857"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import blimpy as bl\n",
        "#from ultralytics import YOLO\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import psutil\n",
        "import scipy.ndimage\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import cv2\n",
        "from scipy.ndimage import label as connected_components\n",
        "from skimage.filters import sobel\n",
        "from skimage.segmentation import slic\n",
        "from skimage.util import img_as_float\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from random import shuffle\n",
        "from tqdm import tqdm\n",
        "from skimage.util import img_as_float\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from skimage.segmentation import slic\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dba8ef3",
      "metadata": {
        "id": "8dba8ef3",
        "outputId": "afb83b76-0ae6-4e68-8c1d-42593d2d9e2a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Target</th>\n",
              "      <th>Session</th>\n",
              "      <th>Band</th>\n",
              "      <th>Cadence ID</th>\n",
              "      <th>Frequency</th>\n",
              "      <th>.h5 path</th>\n",
              "      <th>.dat path</th>\n",
              "      <th>Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36553</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36554</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36555</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36556</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36557</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>36558 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Target          Session Band  Cadence ID  Frequency  \\\n",
              "0       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "1       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "2       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "3       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "4       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "...        ...              ...  ...         ...        ...   \n",
              "36553  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "36554  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "36555  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "36556  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "36557  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "\n",
              "                                                .h5 path  \\\n",
              "0      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "1      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "2      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "3      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "4      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "...                                                  ...   \n",
              "36553  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "36554  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "36555  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "36556  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "36557  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "\n",
              "                                               .dat path                 Time  \n",
              "0      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "1      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "2      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "3      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "4      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "...                                                  ...                  ...  \n",
              "36553  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "36554  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "36555  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "36556  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "36557  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "\n",
              "[36558 rows x 8 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('/home/cgchoza/galaxies/complete_cadences_catalog.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "730aa85b",
      "metadata": {
        "id": "730aa85b",
        "outputId": "67aee906-65d3-4e1e-bdb1-77408418462b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Target</th>\n",
              "      <th>Session</th>\n",
              "      <th>Band</th>\n",
              "      <th>Cadence ID</th>\n",
              "      <th>Frequency</th>\n",
              "      <th>.h5 path</th>\n",
              "      <th>.dat path</th>\n",
              "      <th>Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30309</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30310</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30311</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30312</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30313</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30314 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Target          Session Band  Cadence ID  Frequency  \\\n",
              "0       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "1       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "2       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "3       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "4       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "...        ...              ...  ...         ...        ...   \n",
              "30309  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "30310  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "30311  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "30312  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "30313  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "\n",
              "                                                .h5 path  \\\n",
              "0      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "1      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "2      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "3      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "4      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "...                                                  ...   \n",
              "30309  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "30310  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "30311  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "30312  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "30313  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "\n",
              "                                               .dat path                 Time  \n",
              "0      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "1      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "2      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "3      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "4      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "...                                                  ...                  ...  \n",
              "30309  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "30310  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "30311  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "30312  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "30313  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "\n",
              "[30314 rows x 8 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['.h5 path'] = df['.h5 path'].str.replace('0000.h5', '0002.h5', regex=False)\n",
        "df = df.drop_duplicates(subset='.h5 path', keep='first').reset_index(drop=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38b7592b",
      "metadata": {
        "id": "38b7592b",
        "outputId": "9914de53-3dfa-4a3a-d891-1752d649ade3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Target</th>\n",
              "      <th>Session</th>\n",
              "      <th>Band</th>\n",
              "      <th>Cadence ID</th>\n",
              "      <th>Frequency</th>\n",
              "      <th>.h5 path</th>\n",
              "      <th>.dat path</th>\n",
              "      <th>Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AND_XIV</td>\n",
              "      <td>AGBT18B_999_07</td>\n",
              "      <td>S</td>\n",
              "      <td>30225</td>\n",
              "      <td>3151</td>\n",
              "      <td>/datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18B_999_07/blc40_blp00...</td>\n",
              "      <td>2018-08-18 08:41:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AND_XIV</td>\n",
              "      <td>AGBT18B_999_07</td>\n",
              "      <td>S</td>\n",
              "      <td>30225</td>\n",
              "      <td>3151</td>\n",
              "      <td>/datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18B_999_07/blc40_blp00...</td>\n",
              "      <td>2018-08-18 08:41:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AND_XIV</td>\n",
              "      <td>AGBT18B_999_07</td>\n",
              "      <td>S</td>\n",
              "      <td>30225</td>\n",
              "      <td>3151</td>\n",
              "      <td>/datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18B_999_07/blc40_blp00...</td>\n",
              "      <td>2018-08-18 08:41:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AND_XIV</td>\n",
              "      <td>AGBT18B_999_07</td>\n",
              "      <td>S</td>\n",
              "      <td>30225</td>\n",
              "      <td>3151</td>\n",
              "      <td>/datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18B_999_07/blc40_blp00...</td>\n",
              "      <td>2018-08-18 08:41:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AND_XIV</td>\n",
              "      <td>AGBT18B_999_07</td>\n",
              "      <td>S</td>\n",
              "      <td>30225</td>\n",
              "      <td>3151</td>\n",
              "      <td>/datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18B_999_07/blc40_blp00...</td>\n",
              "      <td>2018-08-18 08:41:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29341</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29342</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29343</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29344</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29345</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>29346 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Target         Session Band  Cadence ID  Frequency  \\\n",
              "0      AND_XIV  AGBT18B_999_07    S       30225       3151   \n",
              "1      AND_XIV  AGBT18B_999_07    S       30225       3151   \n",
              "2      AND_XIV  AGBT18B_999_07    S       30225       3151   \n",
              "3      AND_XIV  AGBT18B_999_07    S       30225       3151   \n",
              "4      AND_XIV  AGBT18B_999_07    S       30225       3151   \n",
              "...        ...             ...  ...         ...        ...   \n",
              "29341  NGC3226  AGBT22B_999_25    L      411390       1126   \n",
              "29342  NGC3226  AGBT22B_999_25    L      411390       1126   \n",
              "29343  NGC3226  AGBT22B_999_25    L      411390       1126   \n",
              "29344  NGC3226  AGBT22B_999_25    L      411390       1126   \n",
              "29345  NGC3226  AGBT22B_999_25    L      411390       1126   \n",
              "\n",
              "                                                .h5 path  \\\n",
              "0      /datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...   \n",
              "1      /datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...   \n",
              "2      /datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...   \n",
              "3      /datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...   \n",
              "4      /datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...   \n",
              "...                                                  ...   \n",
              "29341  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "29342  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "29343  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "29344  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "29345  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "\n",
              "                                               .dat path                 Time  \n",
              "0      /home/obs/turboseti/AGBT18B_999_07/blc40_blp00...  2018-08-18 08:41:37  \n",
              "1      /home/obs/turboseti/AGBT18B_999_07/blc40_blp00...  2018-08-18 08:41:37  \n",
              "2      /home/obs/turboseti/AGBT18B_999_07/blc40_blp00...  2018-08-18 08:41:37  \n",
              "3      /home/obs/turboseti/AGBT18B_999_07/blc40_blp00...  2018-08-18 08:41:37  \n",
              "4      /home/obs/turboseti/AGBT18B_999_07/blc40_blp00...  2018-08-18 08:41:37  \n",
              "...                                                  ...                  ...  \n",
              "29341  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "29342  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "29343  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "29344  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "29345  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "\n",
              "[29346 rows x 8 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df[~df['.h5 path'].str.contains('spliced')].reset_index(drop=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f1451bd",
      "metadata": {
        "id": "2f1451bd"
      },
      "outputs": [],
      "source": [
        "df = df.drop(index = 17546)\n",
        "df = df.sample(n=1000, random_state=42).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84164fd0",
      "metadata": {
        "id": "84164fd0",
        "outputId": "1805d3f3-922d-48ec-8192-de3fd9753d90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- File Info ---\n",
            "DIMENSION_LABELS : [b'time' b'feed_id' b'frequency']\n",
            "        az_start :                              0.0\n",
            "       data_type :                                1\n",
            "            fch1 :               10501.46484375 MHz\n",
            "            foff :         -0.00286102294921875 MHz\n",
            "           ibeam :                               -1\n",
            "      machine_id :                               20\n",
            "          nbeams :                                1\n",
            "           nbits :                               32\n",
            "          nchans :                            65536\n",
            "            nifs :                                1\n",
            "     rawdatafile : guppi_59411_54386_HIP30264_0094.0000.raw\n",
            "     source_name :                         HIP30264\n",
            "         src_dej :                     -8:26:53.228\n",
            "         src_raj :                      6:21:58.451\n",
            "    telescope_id :                                6\n",
            "           tsamp :                1.073741823999999\n",
            "   tstart (ISOT) :          2021-07-16T15:06:26.000\n",
            "    tstart (MJD) :                59411.62946759259\n",
            "        za_start :                              0.0\n",
            "\n",
            "Num ints in file :                              279\n",
            "      File shape :                  (279, 1, 65536)\n",
            "--- Selection Info ---\n",
            "Data selection shape :                  (279, 1, 65536)\n",
            "Minimum freq (MHz) :                10313.96770477295\n",
            "Maximum freq (MHz) :                   10501.46484375\n"
          ]
        }
      ],
      "source": [
        "fb = bl.Waterfall(df['.h5 path'].iloc[-1], load_data=True)\n",
        "fb.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51b073aa",
      "metadata": {
        "id": "51b073aa"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "import time\n",
        "\n",
        "def ensure_cpu_mem(bytes_needed, safety=0.8):\n",
        "    avail = psutil.virtual_memory().available\n",
        "    if bytes_needed > avail * safety:\n",
        "        time.sleep(120)\n",
        "\n",
        "def ensure_gpu_mem(bytes_needed, safety=0.8):\n",
        "    free, total = cp.cuda.runtime.memGetInfo()\n",
        "    if bytes_needed > free * safety:\n",
        "        time.sleep(120)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "395191b3",
      "metadata": {
        "id": "395191b3"
      },
      "outputs": [],
      "source": [
        "def merge_boxes(boxes, iou_thresh=0.1, proximity_thresh=0.05):\n",
        "    \"\"\"\n",
        "    Merge boxes if their centers are close or they have significant IoU overlap.\n",
        "    boxes: list of (cx, cy, w, h, area)\n",
        "    returns: merged list of boxes\n",
        "    \"\"\"\n",
        "\n",
        "    def iou(b1, b2):\n",
        "        x1_min = b1[0] - b1[2] / 2\n",
        "        x1_max = b1[0] + b1[2] / 2\n",
        "        y1_min = b1[1] - b1[3] / 2\n",
        "        y1_max = b1[1] + b1[3] / 2\n",
        "\n",
        "        x2_min = b2[0] - b2[2] / 2\n",
        "        x2_max = b2[0] + b2[2] / 2\n",
        "        y2_min = b2[1] - b2[3] / 2\n",
        "        y2_max = b2[1] + b2[3] / 2\n",
        "\n",
        "        inter_xmin = max(x1_min, x2_min)\n",
        "        inter_ymin = max(y1_min, y2_min)\n",
        "        inter_xmax = min(x1_max, x2_max)\n",
        "        inter_ymax = min(y1_max, y2_max)\n",
        "\n",
        "        inter_area = max(0, inter_xmax - inter_xmin) * max(0, inter_ymax - inter_ymin)\n",
        "        area1 = b1[2] * b1[3]\n",
        "        area2 = b2[2] * b2[3]\n",
        "        union = area1 + area2 - inter_area\n",
        "\n",
        "        return inter_area / union if union > 0 else 0\n",
        "\n",
        "    def distance(b1, b2):\n",
        "        return np.hypot(b1[0] - b2[0], b1[1] - b2[1])\n",
        "\n",
        "    merged = []\n",
        "    used = [False] * len(boxes)\n",
        "\n",
        "    for i, b1 in enumerate(boxes):\n",
        "        if used[i]:\n",
        "            continue\n",
        "        group = [b1]\n",
        "        used[i] = True\n",
        "        for j, b2 in enumerate(boxes):\n",
        "            if used[j]:\n",
        "                continue\n",
        "            if iou(b1, b2) > iou_thresh or distance(b1, b2) < proximity_thresh:\n",
        "                group.append(b2)\n",
        "                used[j] = True\n",
        "        if len(group) == 1:\n",
        "            merged.append(group[0])\n",
        "        else:\n",
        "            # merge into one big box (average center, min enclosing width/height)\n",
        "            xs = [b[0] for b in group]\n",
        "            ys = [b[1] for b in group]\n",
        "            ws = [b[2] for b in group]\n",
        "            hs = [b[3] for b in group]\n",
        "            xmins = [x - w/2 for x, w in zip(xs, ws)]\n",
        "            xmaxs = [x + w/2 for x, w in zip(xs, ws)]\n",
        "            ymins = [y - h/2 for y, h in zip(ys, hs)]\n",
        "            ymaxs = [y + h/2 for y, h in zip(ys, hs)]\n",
        "            new_x = (min(xmins) + max(xmaxs)) / 2\n",
        "            new_y = (min(ymins) + max(ymaxs)) / 2\n",
        "            new_w = max(xmaxs) - min(xmins)\n",
        "            new_h = max(ymaxs) - min(ymins)\n",
        "            merged.append((new_x, new_y, new_w, new_h, new_w * new_h))\n",
        "    return merged\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54d1011a",
      "metadata": {
        "id": "54d1011a"
      },
      "outputs": [],
      "source": [
        "def generate_yolo_boxes(edge_map, threshold=0.1):\n",
        "    binary = (edge_map > threshold).astype(np.uint8)\n",
        "    labeled, n = connected_components(binary)\n",
        "    boxes = []\n",
        "    for i in range(1, n + 1):\n",
        "        coords = np.argwhere(labeled == i)\n",
        "        if coords.shape[0] < 10:\n",
        "            continue\n",
        "        y0, x0 = coords.min(axis=0)\n",
        "        y1, x1 = coords.max(axis=0)\n",
        "        cx = (x0 + x1) / 2 / edge_map.shape[1]\n",
        "        cy = (y0 + y1) / 2 / edge_map.shape[0]\n",
        "        w = (x1 - x0) / edge_map.shape[1]\n",
        "        h = (y1 - y0) / edge_map.shape[0]\n",
        "        boxes.append((cx, cy, w, h))\n",
        "    return boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37e12fd6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_tasks(df):\n",
        "    \"\"\"\n",
        "    Build a flat list of (h5_path, channel_idx) from your DataFrame,\n",
        "    skipping any channels that fall into known notch filter frequency ranges.\n",
        "    \"\"\"\n",
        "    GBT_NOTCH_FILTERS = {\n",
        "        \"L\": [(1200, 1340)],\n",
        "        \"S\": [(2300, 2360)],\n",
        "    }\n",
        "\n",
        "    tasks = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        h5 = row[\".h5 path\"]\n",
        "        band = row[\"Band\"]  # e.g., 'L', 'S', etc.\n",
        "        fb = bl.Waterfall(h5, load_data=False)\n",
        "        nfreq = fb.header.get(\"nchans\")\n",
        "        nfpc = fb.header.get(\"nfpc\", 1024)\n",
        "        fch1 = fb.header[\"fch1\"]\n",
        "        foff = fb.header[\"foff\"]\n",
        "        n_coarse = nfreq // nfpc\n",
        "\n",
        "        for ch in range(n_coarse):\n",
        "            f0 = fch1 + ch * nfpc * foff\n",
        "            f1 = fch1 + (ch + 1) * nfpc * foff\n",
        "            f_min, f_max = sorted([f0, f1])\n",
        "\n",
        "            # Check against notch filter exclusion ranges\n",
        "            skip = False\n",
        "            if band in GBT_NOTCH_FILTERS:\n",
        "                for lo, hi in GBT_NOTCH_FILTERS[band]:\n",
        "                    if lo <= f_min <= hi or lo <= f_max <= hi:\n",
        "                        skip = True\n",
        "                        break\n",
        "            if not skip:\n",
        "                tasks.append((h5, ch))\n",
        "\n",
        "    return tasks\n",
        "\n",
        "\n",
        "def split_tasks(tasks, train_frac=0.8, seed=42):\n",
        "    \"\"\"\n",
        "    Shuffle & split the flat task list into train vs. val sets.\n",
        "    Returns two sets of (h5_path, channel_idx).\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    shuffled = tasks.copy()\n",
        "    random.shuffle(shuffled)\n",
        "    cut = int(train_frac * len(shuffled))\n",
        "    train = set(shuffled[:cut])\n",
        "    val   = set(shuffled[cut:])\n",
        "    return train, val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FwocEh2jdjRX",
      "metadata": {
        "id": "FwocEh2jdjRX"
      },
      "outputs": [],
      "source": [
        "# The MoE: Structured Forest (SF), Sobel (UED), Canny, HOG (Histogram of Oriented Gradients)\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageDraw\n",
        "import blimpy as bl\n",
        "from skimage.segmentation import slic\n",
        "from skimage.feature import hog\n",
        "\n",
        "\n",
        "# ─── 1) Define RuleGate  ─────────────────────────────────────────\n",
        "class RuleGate(nn.Module):\n",
        "    def __init__(self, w_align=2.0, w_ent=1.0, bias=-0.5):\n",
        "        super().__init__()\n",
        "        self.w_align = w_align\n",
        "        self.w_ent   = w_ent\n",
        "        self.bias    = bias\n",
        "\n",
        "    def forward(self, h_sf, h_ued, h_canny):\n",
        "        # Simple average of Sobel/Canny contribution vs SF\n",
        "        align_diff = (h_sf[:,1] + h_canny[:,1]) / 2 - h_ued[:,1]\n",
        "        ent_diff   =  h_ued[:,2] - (h_sf[:,2] + h_canny[:,2]) / 2\n",
        "        score  = self.w_align * align_diff + self.w_ent * ent_diff + self.bias\n",
        "        return torch.sigmoid(score)\n",
        "\n",
        "\n",
        "# ─── 2) Heuristic helper ─────────────────────────────────────────────────\n",
        "def compute_heuristics(edge_map, gray):\n",
        "    mask = edge_map > 0.2\n",
        "    density = float(mask.mean())\n",
        "    if density > 0:\n",
        "        gx = cv2.Sobel(gray, cv2.CV_32F, 1, 0, 3)\n",
        "        gy = cv2.Sobel(gray, cv2.CV_32F, 0, 1, 3)\n",
        "        alignment = float(np.hypot(gx, gy)[mask].mean())\n",
        "    else:\n",
        "        alignment = 0.0\n",
        "    entropy = float(-np.sum(edge_map * np.log2(edge_map + 1e-8)))\n",
        "    bw = mask.astype(np.uint8)\n",
        "    n, labels = cv2.connectedComponents(bw)\n",
        "    ratios = []\n",
        "    for L in range(1, n):\n",
        "        ys, xs = np.where(labels == L)\n",
        "        if ys.size:\n",
        "            h, w = np.ptp(ys)+1, np.ptp(xs)+1\n",
        "            ratios.append(float(w)/h if h else 0.0)\n",
        "    linearity = float(np.mean(ratios)) if ratios else 0.0\n",
        "    if density < 0.01:\n",
        "        return [0.0, 0.0, 0.0, 0.0]\n",
        "    return [density, alignment, entropy, linearity]\n",
        "\n",
        "\n",
        "# ─── 3) Updated process_file with HOG expert ─────────────────────────────\n",
        "def process_file(job):\n",
        "    (h5_path, channels, gidxs,\n",
        "     base_dir, class_id, train_set, val_set,\n",
        "     pad_width) = job\n",
        "\n",
        "    fb   = bl.Waterfall(h5_path, load_data=True)\n",
        "    data = 10*np.log10(fb.data.squeeze())\n",
        "\n",
        "    EDGE_MODEL = \"/home/jliang/gbt-rfi/model.yml.gz\"\n",
        "    sf_det     = cv2.ximgproc.createStructuredEdgeDetection(EDGE_MODEL)\n",
        "    gate       = RuleGate()\n",
        "\n",
        "    for ch_idx, gidx in zip(channels, gidxs):\n",
        "        subset = \"train\" if (h5_path,ch_idx) in train_set else \"val\"\n",
        "        img_dir = Path(base_dir)/subset/\"images\"\n",
        "        lbl_dir = Path(base_dir)/subset/\"labels\"\n",
        "        vis_dir = Path(base_dir)/\"visualization\"/subset\n",
        "        for d in (img_dir,lbl_dir,vis_dir): d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        cw = fb.header.get(\"nfpc\",1024)\n",
        "        f0, f1 = ch_idx*cw, (ch_idx+1)*cw\n",
        "        block = data[:,f0:f1]\n",
        "        low, high = int(0.1*cw), int(0.9*cw)\n",
        "        block = block[:,low:high]\n",
        "        h_img, w_img = block.shape\n",
        "\n",
        "        # Remove vertical line artifact\n",
        "        rows, cols = block.shape\n",
        "        vert_means = block.mean(axis=0)\n",
        "        center = np.argmax(vert_means)\n",
        "        left_col = center - 1\n",
        "        right_col = center + 1\n",
        "        if left_col >= 0 and right_col < cols:\n",
        "            block[:, center] = (block[:, left_col] + block[:, right_col]) / 2\n",
        "        elif left_col >= 0:\n",
        "            block[:, center] = block[:, left_col]\n",
        "        elif right_col < cols:\n",
        "            block[:, center] = block[:, right_col]\n",
        "\n",
        "        \n",
        "        norm   = (block - block.min())/(np.ptp(block)+1e-6)\n",
        "        img3   = np.stack([norm]*3, axis=-1).astype(np.float32)\n",
        "        sf_map = sf_det.detectEdges(img3).squeeze()\n",
        "        gray8  = (255*norm).astype(np.uint8)\n",
        "\n",
        "        # Sobel\n",
        "        gx      = cv2.Sobel(gray8, cv2.CV_32F, 1,0,3)\n",
        "        gy      = cv2.Sobel(gray8, cv2.CV_32F, 0,1,3)\n",
        "        ued_map = np.hypot(gx, gy)\n",
        "        ued_map = cv2.normalize(ued_map, None, 0,1, cv2.NORM_MINMAX)\n",
        "\n",
        "        # Canny\n",
        "        # you can tune these two thresholds at will\n",
        "        low_thresh, high_thresh = 50, 150\n",
        "        gray_denoised = cv2.medianBlur(gray8, 5)\n",
        "        edges = cv2.Canny(gray_denoised, low_thresh, high_thresh)\n",
        "\n",
        "        # build your float map for heuristics\n",
        "        canny_map = edges.astype(np.float32) / 255.0\n",
        "\n",
        "        # now find contours on the *binary* edges\n",
        "        #contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        \n",
        "\n",
        "        fused = np.zeros_like(sf_map, dtype=np.float32)\n",
        "        segments = slic(img3, n_segments=100, compactness=10)\n",
        "        for v in np.unique(segments):\n",
        "            mask = (segments==v)\n",
        "            if mask.sum()<50:\n",
        "                continue\n",
        "            h_sf    = compute_heuristics(sf_map[mask], gray8[mask])\n",
        "            h_ued   = compute_heuristics(ued_map[mask], gray8[mask])\n",
        "            h_canny = compute_heuristics(canny_map[mask], gray8[mask])\n",
        "            #h_hog   = compute_heuristics(hog_map[mask], gray8[mask])\n",
        "            h_sf_t    = torch.tensor([h_sf], dtype=torch.float32)\n",
        "            h_ued_t   = torch.tensor([h_ued], dtype=torch.float32)\n",
        "            h_canny_t = torch.tensor([h_canny], dtype=torch.float32)\n",
        "            #h_hog_t   = torch.tensor([h_hog], dtype=torch.float32)\n",
        "            p = gate(h_sf_t, h_ued_t, h_canny_t).item()\n",
        "            fused[mask] = (p/2)*(sf_map[mask] + canny_map[mask]) + (1-p)*ued_map[mask]\n",
        "\n",
        "        boxes = generate_yolo_boxes(fused, threshold=0.05)\n",
        "\n",
        "        f_start = fb.header['fch1'] + f0 * fb.header['foff']\n",
        "        f_stop  = fb.header['fch1'] + (f1 - 1) * fb.header['foff']\n",
        "        fn = f\"img_{gidx:0{pad_width}d}_f_{f_start:.4f}_{f_stop:.4f}.png\"\n",
        "        img_path = img_dir / fn\n",
        "        txt_path = lbl_dir / fn.replace(\".png\", \".txt\")\n",
        "\n",
        "        arr8 = (255 * (block - block.min()) / (np.ptp(block) + 1e-6)).astype(np.uint8)\n",
        "        img = Image.fromarray(np.stack([arr8]*3, axis=-1))\n",
        "        img.save(img_path)\n",
        "\n",
        "        full_image_threshold = 0.95\n",
        "        min_norm_area        = 0.01\n",
        "        # boxes is List[(x_c, y_c, w_n, h_n)]\n",
        "        boxes = [\n",
        "            (x_c, y_c, w_n, h_n)\n",
        "            for x_c, y_c, w_n, h_n in boxes\n",
        "            if min_norm_area <= (w_n * h_n) <= full_image_threshold\n",
        "        ]\n",
        "\n",
        "        with open(txt_path, \"w\") as f:\n",
        "            for x_c, y_c, w_n, h_n in boxes:\n",
        "                f.write(f\"{class_id} {x_c:.6f} {y_c:.6f} {w_n:.6f} {h_n:.6f}\\n\")\n",
        "\n",
        "\n",
        "        if not boxes:\n",
        "            (lbl_dir/\"empty_labels.csv\").open(\"a\").write(f\"{fn},{h5_path},{ch_idx}\\n\")\n",
        "\n",
        "        vis_img = img.convert(\"RGB\")\n",
        "        draw    = ImageDraw.Draw(vis_img)\n",
        "        for x_c, y_c, w_n, h_n in boxes:\n",
        "            xc, yc = x_c*w_img, y_c*h_img\n",
        "            bw, bh = w_n*w_img, h_n*h_img\n",
        "            x0, y0 = int(xc - bw/2), int(yc - bh/2)\n",
        "            x1, y1 = int(xc + bw/2), int(yc + bh/2)\n",
        "            draw.rectangle([x0, y0, x1, y1], outline=\"red\", width=2)\n",
        "        vis_img.save(vis_dir/fn)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tasks = build_tasks(df)\n",
        "    train_set, val_set = split_tasks(tasks)\n",
        "\n",
        "    # 2) build job tuples of exactly what process_file unpacks:\n",
        "    job_args = []\n",
        "    BASE_DIR = \"/datax/scratch/jliang/dataset_moe_gate\"\n",
        "    NUM_WORKERS = 4\n",
        "    class_id = 0  # Assuming a single class for simplicity\n",
        "    pad_width = 4  # Zero-padding width for gidx in filenames\n",
        "    for idx, (h5_path, ch_idx) in enumerate(tasks):\n",
        "        job = (\n",
        "            h5_path,\n",
        "            [ch_idx],        # channels\n",
        "            [idx],           # gidxs\n",
        "            BASE_DIR,\n",
        "            class_id,\n",
        "            train_set,\n",
        "            val_set,\n",
        "            pad_width\n",
        "        )\n",
        "        job_args.append(job)\n",
        "\n",
        "    # 3) submit each job as a single argument\n",
        "    with ProcessPoolExecutor(max_workers=NUM_WORKERS) as exe:\n",
        "        futures = {\n",
        "            exe.submit(process_file, job): (job[0], job[1])\n",
        "            for job in job_args\n",
        "        }\n",
        "\n",
        "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
        "            h5, ch_list = futures[future]\n",
        "            try:\n",
        "                future.result()\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {h5} channel {ch_list}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48e59983",
      "metadata": {
        "id": "48e59983"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import cv2\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from skimage.segmentation import slic\n",
        "# from scipy import ndimage\n",
        "\n",
        "# # ─── 1) HEURISTIC COMPUTATION ─────────────────────────────────────────\n",
        "\n",
        "# def compute_heuristics(edge_map, gray_image):\n",
        "#     # edge_map: 2D float32 [0,1], gray_image: 2D uint8\n",
        "#     # 1) gradient magnitude\n",
        "#     gx = cv2.Sobel(gray_image, cv2.CV_32F, 1, 0, ksize=3)\n",
        "#     gy = cv2.Sobel(gray_image, cv2.CV_32F, 0, 1, ksize=3)\n",
        "#     grad = np.hypot(gx, gy)\n",
        "\n",
        "#     # 2) statistics\n",
        "#     density   = np.mean(edge_map > 0.2)\n",
        "#     align     = np.mean(grad[edge_map > 0.2]) if density>0 else 0.0\n",
        "#     entropy   = -np.sum(edge_map * np.log2(edge_map + 1e-8))\n",
        "#     # 3) linearity via eccentricity of components\n",
        "#     bw   = edge_map > 0.2\n",
        "#     lbl, n = ndimage.label(bw)\n",
        "#     props = ndimage.find_objects(lbl)\n",
        "#     eccs = []\n",
        "#     for i, sl in enumerate(props, start=1):\n",
        "#         region = (lbl[sl] == i).astype(np.uint8)\n",
        "#         # approximate by second moments → skip details here\n",
        "#         eccs.append(region.sum()>0 and region.sum() / (region.shape[0]*region.shape[1]))\n",
        "#     linearity = float(np.mean(eccs)) if eccs else 0.0\n",
        "\n",
        "#     if density < 0.01:\n",
        "#         return [0.0, 0.0, 0.0, 0.0]\n",
        "#     return [density, align, entropy, linearity]\n",
        "\n",
        "\n",
        "# # ─── 2) RULE-BASED GATE MODULE ──────────────────────────────────────────\n",
        "\n",
        "# class RuleGate(nn.Module):\n",
        "#     def __init__(self, w_align=1.0, w_ent=1.0, bias=0.0):\n",
        "#         super().__init__()\n",
        "#         # wrap as parameters if you want to tune via backprop:\n",
        "#         self.w_align = nn.Parameter(torch.tensor(w_align))\n",
        "#         self.w_ent   = nn.Parameter(torch.tensor(w_ent))\n",
        "#         self.bias    = nn.Parameter(torch.tensor(bias))\n",
        "\n",
        "#     def forward(self, h_sf, h_ued):\n",
        "#         # h_*: [B,4] tensors on CUDA\n",
        "#         align = h_ued[:,1] - h_sf[:,1]      # favors UED if >0\n",
        "#         ent   = h_sf[:,2] - h_ued[:,2]      # favors UED if >0\n",
        "#         score  = self.w_align * align + self.w_ent * ent + self.bias\n",
        "#         return torch.sigmoid(score)         # [B] weights for UED\n",
        "\n",
        "\n",
        "# class MoEBlock(nn.Module):\n",
        "#     def __init__(self, gate: RuleGate):\n",
        "#         super().__init__()\n",
        "#         self.gate = gate\n",
        "\n",
        "#     def forward(self, sf_map, ued_map, h_sf, h_ued):\n",
        "#         \"\"\"\n",
        "#         sf_map, ued_map: [B,H,W] floats on CUDA\n",
        "#         h_sf, h_ued:        [B,4] heuristic tensors on CUDA\n",
        "#         \"\"\"\n",
        "#         p = self.gate(h_sf, h_ued).view(-1,1,1)  # [B,1,1]\n",
        "#         return p * ued_map + (1 - p) * sf_map\n",
        "\n",
        "\n",
        "# # ─── 3) FULL PROCESSING LOOP ───────────────────────────────────────────\n",
        "\n",
        "# def process_image_patch(patch, sf_detector, gate_block, device='cuda'):\n",
        "#     \"\"\"\n",
        "#     patch: HxWx3 BGR uint8\n",
        "#     sf_detector: cv2.ximgproc StructuredEdgeDetection\n",
        "#     gate_block:   instance of MoEBlock on device\n",
        "#     \"\"\"\n",
        "#     # 1) compute both edge maps\n",
        "#     gray    = cv2.cvtColor(patch, cv2.COLOR_BGR2GRAY)\n",
        "#     sf_edges= sf_detector.detectEdges(np.float32(patch)/255.0).squeeze()\n",
        "#     gx = cv2.Sobel(gray, cv2.CV_32F, 1,0,3)\n",
        "#     gy = cv2.Sobel(gray, cv2.CV_32F, 0,1,3)\n",
        "#     ued_edges = np.hypot(gx, gy)\n",
        "#     ued_edges = (ued_edges/ued_edges.max()).astype(np.float32)\n",
        "\n",
        "#     # 2) superpixel segmentation\n",
        "#     segments = slic(patch, n_segments=100, compactness=10)\n",
        "\n",
        "#     # 3) gather heuristics and masks\n",
        "#     feats_sf, feats_ued, masks = [], [], []\n",
        "#     for seg in np.unique(segments):\n",
        "#         mask = (segments==seg)\n",
        "#         if mask.sum()<50: continue\n",
        "#         hsf = compute_heuristics(sf_edges*mask, gray)\n",
        "#         hud = compute_heuristics(ued_edges*mask, gray)\n",
        "#         if not any(hsf) and not any(hud): continue\n",
        "#         feats_sf.append(hsf)\n",
        "#         feats_ued.append(hud)\n",
        "#         masks.append(mask)\n",
        "\n",
        "#     if not masks:\n",
        "#         return np.zeros_like(sf_edges, np.float32)\n",
        "\n",
        "#     # 4) move to GPU\n",
        "#     h_sf = torch.tensor(feats_sf, device=device)\n",
        "#     h_ued= torch.tensor(feats_ued, device=device)\n",
        "#     sf_m = torch.tensor(np.stack([sf_edges[m] for m in masks]), device=device)\n",
        "#     ue_m = torch.tensor(np.stack([ued_edges[m] for m in masks]), device=device)\n",
        "\n",
        "#     # 5) fuse with gate\n",
        "#     fused_masks = gate_block(sf_m, ue_m, h_sf, h_ued)  # [N_pixels]\n",
        "#     fused = np.zeros_like(sf_edges, np.float32)\n",
        "#     for p, m in zip(fused_masks.cpu().numpy(), masks):\n",
        "#         fused[m] = p\n",
        "\n",
        "#     # 6) generate YOLO boxes\n",
        "#     return fused\n",
        "\n",
        "\n",
        "# # ─── 4) USAGE ─────────────────────────────────────────────────────────\n",
        "\n",
        "# # Initialize:\n",
        "# EDGE_MODEL_PATH = \"model.yml\"  # your SF model\n",
        "# sf_detector = cv2.ximgproc.createStructuredEdgeDetection(EDGE_MODEL_PATH)\n",
        "# gate        = RuleGate(w_align=2.0, w_ent=1.0, bias=-0.5).cuda()\n",
        "# moe_block   = MoEBlock(gate).cuda()\n",
        "\n",
        "# # For each patch:\n",
        "# # fused_map = process_image_patch(patch, sf_detector, moe_block)\n",
        "# # then threshold fused_map, label, convert to YOLO (cx,cy,w,h) as before.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bde136f",
      "metadata": {
        "id": "8bde136f"
      },
      "outputs": [],
      "source": [
        "# def compute_heuristics(image, edge_map):\n",
        "#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "#     gradient = sobel(gray)\n",
        "#     edge_density = np.mean(edge_map > 0.2)\n",
        "#     edge_gradient_alignment = np.mean(gradient[edge_map > 0.2])\n",
        "#     edge_entropy = -np.sum(edge_map * np.log2(edge_map + 1e-8))\n",
        "#     lin = linearity_score(edge_map)\n",
        "#     if edge_density < 0.01:\n",
        "#         return [0.0, 0.0, 0.0, 0.0]\n",
        "#     return [edge_density, edge_gradient_alignment, edge_entropy, lin]\n",
        "\n",
        "# def pseudo_labels(image, sf_edges, ued_edges):\n",
        "#     h_sf  = compute_heuristics(image, sf_edges)\n",
        "#     h_ued = compute_heuristics(image, ued_edges)\n",
        "#     # if UED better alignment & lower entropy, choose UED\n",
        "#     if h_ued[1] > h_sf[1] and h_ued[2] < h_sf[2]:\n",
        "#         return 0\n",
        "#     else:\n",
        "#         return 1\n",
        "\n",
        "# # PyTorch gating network (input_dim=8 for 4 heuristics each)\n",
        "# class GatingNet(nn.Module):\n",
        "#     def __init__(self, input_dim=8, hidden=16):\n",
        "#         super().__init__()\n",
        "#         self.net = nn.Sequential(\n",
        "#             nn.Linear(input_dim, hidden),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(hidden, 2)\n",
        "#         )\n",
        "#     def forward(self, x):\n",
        "#         return self.net(x)\n",
        "\n",
        "# # Train gating model in PyTorch\n",
        "# def train_gating_model_torch(images, sf_edge_maps, ued_edge_maps,\n",
        "#                              batch_size=64, epochs=10, lr=1e-3,\n",
        "#                              device='cuda'):\n",
        "#     # 1) Collect features & labels\n",
        "#     feats, labels = [], []\n",
        "#     for img, sf, ued in zip(images, sf_edge_maps, ued_edge_maps):\n",
        "#         segments = slic(img_as_float(img), n_segments=100, compactness=10)\n",
        "#         for seg_val in np.unique(segments):\n",
        "#             mask = (segments == seg_val)\n",
        "#             if mask.sum() < 50:\n",
        "#                 continue\n",
        "#             sf_patch, ued_patch = sf * mask, ued * mask\n",
        "#             if sf_patch.mean() + ued_patch.mean() < 0.01:\n",
        "#                 continue\n",
        "#             h_sf  = compute_heuristics(img, sf_patch)\n",
        "#             h_ued = compute_heuristics(img, ued_patch)\n",
        "#             feats.append(np.array(h_sf + h_ued, dtype=np.float32))\n",
        "#             labels.append(pseudo_labels(img, sf_patch, ued_patch))\n",
        "#     if len(set(labels)) < 2:\n",
        "#         return None, None, None\n",
        "\n",
        "#     # 2) Build tensors & standardize\n",
        "#     X = torch.tensor(feats)  # (N, 8)\n",
        "#     y = torch.tensor(labels, dtype=torch.long)  # (N,)\n",
        "#     mean, std = X.mean(dim=0, keepdim=True), X.std(dim=0, keepdim=True) + 1e-6\n",
        "#     X = (X - mean) / std\n",
        "\n",
        "#     # 3) DataLoader\n",
        "#     dataset = TensorDataset(X, y)\n",
        "#     loader  = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "#     # 4) Model, loss, optimizer\n",
        "#     model = GatingNet(input_dim=X.shape[1]).to(device)\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#     # 5) Training loop\n",
        "#     model.train()\n",
        "#     for epoch in range(epochs):\n",
        "#         total_loss = 0.0\n",
        "#         for xb, yb in loader:\n",
        "#             xb, yb = xb.to(device), yb.to(device)\n",
        "#             logits = model(xb)\n",
        "#             loss   = criterion(logits, yb)\n",
        "#             optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "#             total_loss += loss.item()\n",
        "#         print(f\"Epoch {epoch+1}/{epochs} - loss: {total_loss/len(loader):.4f}\")\n",
        "\n",
        "#     return model, mean.to(device), std.to(device)\n",
        "\n",
        "# # Apply gating model to fuse edges\n",
        "# def apply_gating_model_torch(image, sf_edges, ued_edges,\n",
        "#                              model, mean, std, device='cuda'):\n",
        "#     segments = slic(img_as_float(image), n_segments=100, compactness=10)\n",
        "#     feats, masks = [], []\n",
        "#     for seg_val in np.unique(segments):\n",
        "#         mask = (segments == seg_val)\n",
        "#         if mask.sum() < 50:\n",
        "#             continue\n",
        "#         sf_patch, ued_patch = sf_edges * mask, ued_edges * mask\n",
        "#         if sf_patch.mean() + ued_patch.mean() < 0.01:\n",
        "#             continue\n",
        "#         h_sf, h_ued = compute_heuristics(image, sf_patch), compute_heuristics(image, ued_patch)\n",
        "#         f = np.array(h_sf + h_ued, dtype=np.float32)\n",
        "#         if not np.all(np.isfinite(f)) or np.allclose(f, 0, atol=1e-3):\n",
        "#             continue\n",
        "#         feats.append(f)\n",
        "#         masks.append(mask)\n",
        "\n",
        "#     if not feats:\n",
        "#         return np.zeros_like(sf_edges, dtype=float)\n",
        "\n",
        "#     X = torch.tensor(feats).to(device)\n",
        "#     X = (X - mean) / std\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         probs = torch.softmax(model(X), dim=1)[:,1].cpu().numpy()\n",
        "\n",
        "#     gated = np.zeros_like(sf_edges, dtype=float)\n",
        "#     for p, mask in zip(probs, masks):\n",
        "#         gated[mask] = p * sf_edges[mask] + (1 - p) * ued_edges[mask]\n",
        "#     return gated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c822d4e5",
      "metadata": {
        "id": "c822d4e5"
      },
      "outputs": [],
      "source": [
        "# def edge_detection(gray, factor=6, dilation=(3, 3)):\n",
        "#         # STEP 1: apply power threshold to suppress background\n",
        "#         median = np.median(gray)\n",
        "#         mad = np.median(np.abs(gray - median))\n",
        "#         power_mask = gray > (median + factor * mad)\n",
        "\n",
        "#         # STEP 2: edge detection on filtered signal only\n",
        "#         grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
        "#         grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
        "#         magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
        "#         magnitude *= power_mask  # mask out noise\n",
        "#         magnitude = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "\n",
        "#         # STEP 3: binary threshold + dilation\n",
        "#         binary = magnitude > 50  # you can tune this\n",
        "#         binary = scipy.ndimage.binary_dilation(binary, structure=np.ones(dilation))\n",
        "#         labeled, n_objs = scipy.ndimage.label(binary)\n",
        "#         slices = scipy.ndimage.find_objects(labeled)\n",
        "#         return slices\n",
        "\n",
        "# EDGE_MODEL_PATH = '/home/jliang/gbt-rfi/model.yml.gz'\n",
        "# EDGE_DETECTOR = cv2.ximgproc.createStructuredEdgeDetection(EDGE_MODEL_PATH)\n",
        "\n",
        "# def detect_edges(spectrogram):\n",
        "#     img = (spectrogram - spectrogram.min()) / (spectrogram.ptp() + 1e-6)\n",
        "#     img_3ch = cv2.merge([img.astype(np.float32)] * 3)\n",
        "#     return EDGE_DETECTOR.detectEdges(img_3ch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06fea6ff",
      "metadata": {
        "id": "06fea6ff"
      },
      "outputs": [],
      "source": [
        "# def extract_one_sample(h5_path, ch, edge_model_path='/home/jliang/gbt-rfi/model.yml.gz'):\n",
        "#     try:\n",
        "#         EDGE_DETECTOR = cv2.ximgproc.createStructuredEdgeDetection(edge_model_path)\n",
        "#         fb = bl.Waterfall(h5_path, load_data=True)\n",
        "#         data = 10 * np.log10(fb.data.squeeze())\n",
        "\n",
        "#         nfpc = fb.header.get(\"nfpc\", 1024)\n",
        "#         f0, f1 = ch * nfpc, (ch + 1) * nfpc\n",
        "#         block = data[:, f0:f1]\n",
        "\n",
        "#         n_cols = block.shape[1]\n",
        "#         low, high = int(0.1 * n_cols), int(0.9 * n_cols)\n",
        "#         block_middle80 = block[:, low:high]\n",
        "\n",
        "#         vert_means = block_middle80.mean(axis=0)\n",
        "#         center = np.argmax(vert_means)\n",
        "#         left_col, right_col = center - 1, center + 1\n",
        "#         if left_col >= 0 and right_col < block_middle80.shape[1]:\n",
        "#             block_middle80[:, center] = (block_middle80[:, left_col] + block_middle80[:, right_col]) / 2\n",
        "#         elif left_col >= 0:\n",
        "#             block_middle80[:, center] = block_middle80[:, left_col]\n",
        "#         elif right_col < block_middle80.shape[1]:\n",
        "#             block_middle80[:, center] = block_middle80[:, right_col]\n",
        "\n",
        "#         img_norm = (block_middle80 - block_middle80.min()) / (block_middle80.ptp() + 1e-6)\n",
        "#         img_rgb = cv2.merge([img_norm.astype(np.float32)] * 3)\n",
        "\n",
        "#         sf_edges = EDGE_DETECTOR.detectEdges(img_rgb)\n",
        "\n",
        "#         gray = (255 * img_norm).astype(np.uint8)\n",
        "#         grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
        "#         grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
        "#         magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
        "#         magnitude = cv2.normalize(magnitude, None, 0, 1, cv2.NORM_MINMAX)\n",
        "\n",
        "#         sf_mean = np.mean(sf_edges)\n",
        "#         ued_mean = np.mean(magnitude)\n",
        "\n",
        "#         if sf_mean + ued_mean < 0.005:  # threshold to skip empty or background-only blocks\n",
        "#             return None  # skip empty or background-only blocks\n",
        "\n",
        "\n",
        "#         return (img_rgb, sf_edges, magnitude)\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error processing {h5_path} ch {ch}: {e}\")\n",
        "#         return None\n",
        "\n",
        "# def extract_samples_batched(df, total_samples=40000, batch_size=32, num_workers=4):\n",
        "#     all_tasks = []\n",
        "#     for _, row in df.iterrows():\n",
        "#         h5 = row[\".h5 path\"]\n",
        "#         fb = bl.Waterfall(h5, load_data=False)\n",
        "#         nfreq = fb.header.get(\"nchans\")\n",
        "#         nfpc = fb.header.get(\"nfpc\", 1024)\n",
        "#         for ch in range(nfreq // nfpc):\n",
        "#             all_tasks.append((h5, ch))\n",
        "\n",
        "#     shuffle(all_tasks)\n",
        "#     selected_tasks = all_tasks[:total_samples]\n",
        "\n",
        "#     train_images, train_sf_edges, train_ued_edges = [], [], []\n",
        "#     for i in tqdm(range(0, total_samples, batch_size)):\n",
        "#         batch = selected_tasks[i:i+batch_size]\n",
        "#         with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
        "#             futures = [executor.submit(extract_one_sample, h5, ch) for h5, ch in batch]\n",
        "#             for future in as_completed(futures):\n",
        "#                 result = future.result()\n",
        "#                 if result:\n",
        "#                     img_rgb, sf_edge, ued_edge = result\n",
        "#                     train_images.append(img_rgb)\n",
        "#                     train_sf_edges.append(sf_edge)\n",
        "#                     train_ued_edges.append(ued_edge)\n",
        "\n",
        "#     return train_images, train_sf_edges, train_ued_edges\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50421249",
      "metadata": {
        "id": "50421249"
      },
      "outputs": [],
      "source": [
        "# # 4) SET UP LOGGING\n",
        "# logging.basicConfig(\n",
        "#     level=logging.INFO,\n",
        "#     format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "#     datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "# )\n",
        "\n",
        "# from gating_model import train_gating_model_torch\n",
        "\n",
        "# def build_tasks(df):\n",
        "#     \"\"\"\n",
        "#     Build a flat list of (h5_path, channel_idx) from your DataFrame,\n",
        "#     skipping any channels that fall into known notch filter frequency ranges.\n",
        "#     \"\"\"\n",
        "#     GBT_NOTCH_FILTERS = {\n",
        "#         \"L\": [(1200, 1340)],\n",
        "#         \"S\": [(2300, 2360)],\n",
        "#     }\n",
        "\n",
        "#     tasks = []\n",
        "\n",
        "#     for _, row in df.iterrows():\n",
        "#         h5 = row[\".h5 path\"]\n",
        "#         band = row[\"Band\"]  # e.g., 'L', 'S', etc.\n",
        "#         fb = bl.Waterfall(h5, load_data=False)\n",
        "#         nfreq = fb.header.get(\"nchans\")\n",
        "#         nfpc = fb.header.get(\"nfpc\", 1024)\n",
        "#         fch1 = fb.header[\"fch1\"]\n",
        "#         foff = fb.header[\"foff\"]\n",
        "#         n_coarse = nfreq // nfpc\n",
        "\n",
        "#         for ch in range(n_coarse):\n",
        "#             f0 = fch1 + ch * nfpc * foff\n",
        "#             f1 = fch1 + (ch + 1) * nfpc * foff\n",
        "#             f_min, f_max = sorted([f0, f1])\n",
        "\n",
        "#             # Check against notch filter exclusion ranges\n",
        "#             skip = False\n",
        "#             if band in GBT_NOTCH_FILTERS:\n",
        "#                 for lo, hi in GBT_NOTCH_FILTERS[band]:\n",
        "#                     if lo <= f_min <= hi or lo <= f_max <= hi:\n",
        "#                         skip = True\n",
        "#                         break\n",
        "#             if not skip:\n",
        "#                 tasks.append((h5, ch))\n",
        "\n",
        "#     return tasks\n",
        "\n",
        "\n",
        "# def split_tasks(tasks, train_frac=0.8, seed=42):\n",
        "#     \"\"\"\n",
        "#     Shuffle & split the flat task list into train vs. val sets.\n",
        "#     Returns two sets of (h5_path, channel_idx).\n",
        "#     \"\"\"\n",
        "#     random.seed(seed)\n",
        "#     shuffled = tasks.copy()\n",
        "#     random.shuffle(shuffled)\n",
        "#     cut = int(train_frac * len(shuffled))\n",
        "#     train = set(shuffled[:cut])\n",
        "#     val   = set(shuffled[cut:])\n",
        "#     return train, val\n",
        "\n",
        "# def process_file(job):\n",
        "#     \"\"\"\n",
        "#     job is a tuple:\n",
        "#       (h5_path, channels, global_indices,\n",
        "#        base_dir, factor, dilation,\n",
        "#        class_id, train_set, val_set,\n",
        "#        pad_width, model, mean, std)\n",
        "#     \"\"\"\n",
        "#     (h5_path, channels, global_indices,\n",
        "#      base_dir, factor, dilation,\n",
        "#      class_id, train_set, val_set,\n",
        "#      pad_width, model, mean, std) = job\n",
        "\n",
        "#     # 1) Prepare device & trained gate\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     model = model.to(device)\n",
        "#     model.eval()\n",
        "\n",
        "#     # 2) Load Structured Forest detector once\n",
        "#     EDGE_MODEL_PATH = \"/home/jliang/gbt-rfi/model.yml.gz\"\n",
        "#     sf_detector = cv2.ximgproc.createStructuredEdgeDetection(EDGE_MODEL_PATH)\n",
        "\n",
        "#     # 3) Load the .h5 file\n",
        "#     fb   = bl.Waterfall(h5_path, load_data=True)\n",
        "#     data = 10 * np.log10(fb.data.squeeze())   # (ntime, nfreq)\n",
        "\n",
        "#     # 4) Process each coarse channel\n",
        "#     for ch_idx, gidx in zip(channels, global_indices):\n",
        "#         # decide train vs val\n",
        "#         subset = \"train\" if (h5_path, ch_idx) in train_set else \"val\"\n",
        "\n",
        "#         # make sure directories exist\n",
        "#         img_dir = Path(base_dir)/subset/\"images\"\n",
        "#         lbl_dir = Path(base_dir)/subset/\"labels\"\n",
        "#         vis_dir = Path(base_dir)/\"visualization\"/subset\n",
        "#         for d in (img_dir, lbl_dir, vis_dir):\n",
        "#             d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#         # extract the 80% middle of the block\n",
        "#         cw = fb.header.get(\"nfpc\", 1024)\n",
        "#         f0, f1 = ch_idx*cw, (ch_idx+1)*cw\n",
        "#         block = data[:, f0:f1]\n",
        "#         low, high = int(0.1*cw), int(0.9*cw)\n",
        "#         block = block[:, low:high]\n",
        "\n",
        "#         # remove single‐column artifact\n",
        "#         col_means = block.mean(axis=0)\n",
        "#         c = int(np.argmax(col_means))\n",
        "#         if 0 < c < block.shape[1]-1:\n",
        "#             block[:,c] = 0.5*(block[:,c-1] + block[:,c+1])\n",
        "\n",
        "#         # normalize & build RGB input for SF\n",
        "#         norm = (block - block.min())/(block.ptp()+1e-6)\n",
        "#         img_rgb = cv2.merge([norm.astype(np.float32)]*3)\n",
        "\n",
        "#         # compute the two expert edge‐maps\n",
        "#         sf_edge_map  = sf_detector.detectEdges(img_rgb).squeeze()\n",
        "#         gray_uint8  = (255*norm).astype(np.uint8)\n",
        "#         gx = cv2.Sobel(gray_uint8, cv2.CV_32F, 1, 0, ksize=3)\n",
        "#         gy = cv2.Sobel(gray_uint8, cv2.CV_32F, 0, 1, ksize=3)\n",
        "#         ued_edge_map = np.hypot(gx, gy)\n",
        "#         ued_edge_map = cv2.normalize(ued_edge_map, None, 0.0, 1.0, cv2.NORM_MINMAX)\n",
        "\n",
        "#         # 5) Fuse using the PyTorch gate\n",
        "#         fused_edges = apply_gating_model_torch(\n",
        "#             img_rgb, sf_edge_map, ued_edge_map,\n",
        "#             model, mean, std,\n",
        "#             device=device\n",
        "#         )\n",
        "\n",
        "#         # 6) Generate & save YOLO boxes\n",
        "#         boxes = generate_yolo_boxes(fused_edges, threshold=0.1)\n",
        "\n",
        "#         # Dimensions for filtering\n",
        "#         h_img, w_img = block.shape\n",
        "#         min_size = 3                   # px\n",
        "#         full_image_threshold = 0.95    # normalized area\n",
        "\n",
        "#         # Filter\n",
        "#         final_boxes = []\n",
        "#         for x_c, y_c, w_n, h_n, area in boxes:\n",
        "#             if area > full_image_threshold:\n",
        "#                 continue\n",
        "#             if (w_n * w_img) < min_size or (h_n * h_img) < min_size:\n",
        "#                 continue\n",
        "#             final_boxes.append((x_c, y_c, w_n, h_n))\n",
        "\n",
        "#         # Calculate frequency range of this coarse channel\n",
        "#         f_start = fb.header['fch1'] + f0 * fb.header['foff']\n",
        "#         f_stop  = fb.header['fch1'] + (f1 - 1) * fb.header['foff']\n",
        "\n",
        "#         # Build filename\n",
        "#         fn = f\"img_{gidx:0{pad_width}d}_f_{f_start:.4f}_{f_stop:.4f}.png\"\n",
        "#         img_path = img_dir / fn\n",
        "#         txt_path = lbl_dir / fn.replace(\".png\", \".txt\")\n",
        "\n",
        "#         # Save image\n",
        "#         arr8 = (255 * (block - block.min()) / (block.ptp() + 1e-6)).astype(np.uint8)\n",
        "#         img = Image.fromarray(np.stack([arr8]*3, axis=-1))\n",
        "#         img.save(img_path)\n",
        "\n",
        "#         # Write YOLO labels\n",
        "#         with open(txt_path, \"w\") as f:\n",
        "#             for x_c, y_c, w_n, h_n in final_boxes:\n",
        "#                 f.write(f\"{class_id} {x_c:.6f} {y_c:.6f} {w_n:.6f} {h_n:.6f}\\n\")\n",
        "\n",
        "#         # If empty, log it\n",
        "#         if not final_boxes:\n",
        "#             (lbl_dir/\"empty_labels.csv\").open(\"a\").write(f\"{fn},{h5_path},{ch_idx}\\n\")\n",
        "\n",
        "#         # Draw & save visualization\n",
        "#         vis_img = img.convert(\"RGB\")\n",
        "#         draw    = ImageDraw.Draw(vis_img)\n",
        "#         for x_c, y_c, w_n, h_n in final_boxes:\n",
        "#             xc, yc = x_c*w_img, y_c*h_img\n",
        "#             bw, bh = w_n*w_img, h_n*h_img\n",
        "#             x0, y0 = int(xc - bw/2), int(yc - bh/2)\n",
        "#             x1, y1 = int(xc + bw/2), int(yc + bh/2)\n",
        "#             draw.rectangle([x0, y0, x1, y1], outline=\"red\", width=2)\n",
        "#         vis_img.save(vis_dir/fn)\n",
        "\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # === your settings ===\n",
        "#     BASE_DIR             = \"/datax/scratch/jliang/dataset_moe\"\n",
        "#     FACTOR              = 6\n",
        "#     DILATION = (30, 30)\n",
        "#     CLASS_ID             = 0\n",
        "#     TRAIN_FRAC           = 0.8\n",
        "#     SEED                 = 42\n",
        "#     NUM_WORKERS          = 2\n",
        "\n",
        "#     # --- assume you already have a DataFrame `df` with at least 'h5_path' and optionally 'nchans' ---\n",
        "#     # df = pd.read_csv(...)  # or however you built it\n",
        "\n",
        "#     # 1) Build & split tasks\n",
        "#     tasks = build_tasks(df)\n",
        "#     train_set, val_set = split_tasks(tasks, TRAIN_FRAC, SEED)\n",
        "\n",
        "#     # 2) compute zero-pad width from total images\n",
        "#     pad_width = len(str(len(tasks) - 1))\n",
        "\n",
        "#     # 3) regroup tasks by file to load each .h5 only once\n",
        "#     jobs = {}\n",
        "#     for gidx, (h5, ch) in enumerate(tasks):\n",
        "#         jobs.setdefault(h5, {\"chs\": [], \"gidxs\": []})\n",
        "#         jobs[h5][\"chs\"].append(ch)\n",
        "#         jobs[h5][\"gidxs\"].append(gidx)\n",
        "\n",
        "#     # 4) prepare job‐tuples\n",
        "#     job_list = []\n",
        "#     for h5, info in jobs.items():\n",
        "#         job_list.append((\n",
        "#             h5,\n",
        "#             info[\"chs\"],\n",
        "#             info[\"gidxs\"],\n",
        "#             BASE_DIR,\n",
        "#             FACTOR,\n",
        "#             DILATION,\n",
        "#             CLASS_ID,\n",
        "#             train_set,\n",
        "#             val_set,\n",
        "#             pad_width\n",
        "#         ))\n",
        "\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#     # train_images, train_sf_edges, train_ued_edges = extract_samples_batched(\n",
        "#     #     df, total_samples=40000, num_workers=2\n",
        "#     # )\n",
        "\n",
        "#     # model, mean, std = train_gating_model_torch(\n",
        "#     #     train_images,\n",
        "#     #     train_sf_edges,\n",
        "#     #     train_ued_edges,\n",
        "#     #     batch_size=64,\n",
        "#     #     epochs=10,\n",
        "#     #     lr=1e-3,\n",
        "#     #     device=device\n",
        "#     # )\n",
        "\n",
        "#     # if model is None:\n",
        "#     #     logging.warning(\"Skipping inference because gate net couldn't train.\")\n",
        "#     #     exit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae743955",
      "metadata": {
        "id": "ae743955"
      },
      "outputs": [],
      "source": [
        "# import h5py, numpy as np, torch, logging\n",
        "# from torch.utils.data import IterableDataset, DataLoader\n",
        "# import cv2\n",
        "# from skimage.segmentation import slic\n",
        "# from yolo_moe_pytorch_channel import (\n",
        "#     apply_gating_model_torch,\n",
        "#     generate_yolo_boxes,\n",
        "#     compute_heuristics,\n",
        "#     pseudo_labels\n",
        "# )\n",
        "# from PIL import Image, ImageDraw\n",
        "\n",
        "# # 1) Streaming dataset\n",
        "# class H5ChannelDataset(IterableDataset):\n",
        "#     def __init__(self, df, nfpc=1024):\n",
        "#         self.paths = df[\".h5 path\"].tolist()\n",
        "#         self.nfpc  = nfpc\n",
        "#     def __iter__(self):\n",
        "#         for p in self.paths:\n",
        "#             with h5py.File(p, \"r\") as f:\n",
        "#                 d = f[\"data\"]\n",
        "#                 n_coarse = d.shape[1] // self.nfpc\n",
        "#                 for ch in range(n_coarse):\n",
        "#                     yield d[:, ch*self.nfpc:(ch+1)*self.nfpc].astype(np.float32)\n",
        "\n",
        "# # 2) Pre- and post-processing helpers\n",
        "# EDGE_MODEL_PATH = \"/home/jliang/gbt-rfi/model.yml.gz\"\n",
        "# sf_detector    = cv2.ximgproc.createStructuredEdgeDetection(EDGE_MODEL_PATH)\n",
        "\n",
        "# def preprocess(block):\n",
        "#     norm = (block - block.min())/(block.ptp()+1e-6)\n",
        "#     img_rgb = np.stack([norm]*3, axis=-1).astype(np.float32)\n",
        "#     sf_map   = sf_detector.detectEdges(img_rgb).squeeze()\n",
        "#     gray8    = (255*norm).astype(np.uint8)\n",
        "#     gx = cv2.Sobel(gray8, cv2.CV_32F, 1,0,3)\n",
        "#     gy = cv2.Sobel(gray8, cv2.CV_32F, 0,1,3)\n",
        "#     ued_map = np.hypot(gx, gy)\n",
        "#     ued_map = cv2.normalize(ued_map, None, 0.0, 1.0, cv2.NORM_MINMAX)\n",
        "#     return img_rgb, sf_map, ued_map\n",
        "\n",
        "# def segment_patches(img, sf, ued):\n",
        "#     segs = slic(img, n_segments=100, compactness=10)\n",
        "#     for v in np.unique(segs):\n",
        "#         m = segs==v\n",
        "#         if m.sum()<50: continue\n",
        "#         sf_p, ued_p = sf*m, ued*m\n",
        "#         if sf_p.mean()+ued_p.mean()<0.01: continue\n",
        "#         yield sf_p, ued_p\n",
        "\n",
        "# # 3) Collate: build feature / label tensors\n",
        "# def collate_fn(batch):\n",
        "#     feats, labs = [], []\n",
        "#     for block in batch:\n",
        "#         img, sf, ued = preprocess(block)\n",
        "#         for sf_p, ued_p in segment_patches(img, sf, ued):\n",
        "#             h_sf = compute_heuristics(sf_p, img)\n",
        "#             h_ued= compute_heuristics(ued_p, img)\n",
        "#             feats.append(h_sf + h_ued)\n",
        "#             labs.append(pseudo_labels(img, sf_p, ued_p))\n",
        "#     if not feats: return None\n",
        "#     return torch.tensor(feats), torch.tensor(labs)\n",
        "\n",
        "# # 4) Training\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# dataset = H5ChannelDataset(df)\n",
        "# loader  = DataLoader(dataset,\n",
        "#                      batch_size=4,\n",
        "#                      num_workers=4,\n",
        "#                      persistent_workers=True,\n",
        "#                      collate_fn=collate_fn)\n",
        "\n",
        "# from your_module import GatingNet  # the BatchNorm version\n",
        "# net   = GatingNet().to(device)\n",
        "# opt   = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
        "# crit  = nn.CrossEntropyLoss()\n",
        "\n",
        "# for epoch in range(10):\n",
        "#     for batch in loader:\n",
        "#         if batch is None: continue\n",
        "#         X, y = batch\n",
        "#         X, y = X.to(device), y.to(device)\n",
        "#         logits = net(X)\n",
        "#         loss   = crit(logits, y)\n",
        "#         opt.zero_grad()\n",
        "#         loss.backward()\n",
        "#         opt.step()\n",
        "#     print(f\"Epoch {epoch+1}/10 done\")\n",
        "\n",
        "# # 5) Build job_list as before, but append (net) instead of (mean,std)\n",
        "# # and in process_file call apply_gating_model_torch(img, sf, ued, net, device=...)\n",
        "\n",
        "# # 6) Inference uses your existing process_file, no further change needed.\n",
        "\n",
        "\n",
        "# # ─── 3) Build job list for inference ────────────────────────────────────\n",
        "\n",
        "# # your existing build_tasks(), split_tasks(), jobs grouping...\n",
        "# tasks    = build_tasks(df)\n",
        "# train_set, val_set = split_tasks(tasks, TRAIN_FRAC, SEED)\n",
        "# pad_width = len(str(len(tasks)))\n",
        "\n",
        "# jobs = {}\n",
        "# for gidx,(h5,ch) in enumerate(tasks):\n",
        "#     jobs.setdefault(h5, {\"chs\":[], \"gidxs\":[]})\n",
        "#     jobs[h5][\"chs\"].append(ch)\n",
        "#     jobs[h5][\"gidxs\"].append(gidx)\n",
        "\n",
        "# job_list = []\n",
        "# for h5,info in jobs.items():\n",
        "#     job_list.append((h5,\n",
        "#                      info[\"chs\"],\n",
        "#                      info[\"gidxs\"],\n",
        "#                      BASE_DIR,\n",
        "#                      FACTOR,\n",
        "#                      DILATION,\n",
        "#                      CLASS_ID,\n",
        "#                      train_set,\n",
        "#                      val_set,\n",
        "#                      pad_width,\n",
        "#                      gate_net, mean, std))\n",
        "\n",
        "# # ─── 4) Run inference in parallel ───────────────────────────────────────\n",
        "# from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "# with ProcessPoolExecutor(max_workers=NUM_WORKERS) as exe:\n",
        "#     for _ in exe.map(process_file, job_list):\n",
        "#         pass\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
