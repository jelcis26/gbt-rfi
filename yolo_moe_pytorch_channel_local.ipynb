{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81b1b1d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81b1b1d6",
        "outputId": "e7c14a90-db1a-45e8-a7ff-a4963961b577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bitshuffle\n",
            "  Downloading bitshuffle-0.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: setuptools>=0.7 in /usr/local/lib/python3.11/dist-packages (from bitshuffle) (75.2.0)\n",
            "Requirement already satisfied: Cython>=0.19 in /usr/local/lib/python3.11/dist-packages (from bitshuffle) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from bitshuffle) (2.0.2)\n",
            "Requirement already satisfied: h5py>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from bitshuffle) (3.14.0)\n",
            "Downloading bitshuffle-0.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitshuffle\n",
            "Successfully installed bitshuffle-0.5.2\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install bitshuffle\n",
        "%pip install Pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2fe6857",
      "metadata": {
        "id": "a2fe6857"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import blimpy as bl\n",
        "#from ultralytics import YOLO\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import psutil\n",
        "import scipy.ndimage\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import cv2\n",
        "from scipy.ndimage import label as connected_components\n",
        "from skimage.filters import sobel\n",
        "from skimage.segmentation import slic\n",
        "from skimage.util import img_as_float\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from random import shuffle\n",
        "from tqdm import tqdm\n",
        "from skimage.util import img_as_float\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from skimage.segmentation import slic\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dba8ef3",
      "metadata": {
        "id": "8dba8ef3",
        "outputId": "9ddfdac7-f728-458e-8bc1-ae71d72cf7d5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Target</th>\n",
              "      <th>Session</th>\n",
              "      <th>Band</th>\n",
              "      <th>Cadence ID</th>\n",
              "      <th>Frequency</th>\n",
              "      <th>.h5 path</th>\n",
              "      <th>.dat path</th>\n",
              "      <th>Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36553</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36554</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36555</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36556</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36557</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>36558 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Target          Session Band  Cadence ID  Frequency  \\\n",
              "0       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "1       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "2       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "3       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "4       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "...        ...              ...  ...         ...        ...   \n",
              "36553  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "36554  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "36555  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "36556  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "36557  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "\n",
              "                                                .h5 path  \\\n",
              "0      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "1      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "2      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "3      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "4      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "...                                                  ...   \n",
              "36553  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "36554  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "36555  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "36556  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "36557  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "\n",
              "                                               .dat path                 Time  \n",
              "0      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "1      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "2      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "3      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "4      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "...                                                  ...                  ...  \n",
              "36553  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "36554  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "36555  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "36556  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "36557  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "\n",
              "[36558 rows x 8 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('/home/cgchoza/galaxies/complete_cadences_catalog.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "730aa85b",
      "metadata": {
        "id": "730aa85b",
        "outputId": "d65fffbf-297a-4b80-b5e7-a4e5a0da00f4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Target</th>\n",
              "      <th>Session</th>\n",
              "      <th>Band</th>\n",
              "      <th>Cadence ID</th>\n",
              "      <th>Frequency</th>\n",
              "      <th>.h5 path</th>\n",
              "      <th>.dat path</th>\n",
              "      <th>Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DDO210</td>\n",
              "      <td>AGBT18A_999_103</td>\n",
              "      <td>L</td>\n",
              "      <td>24777</td>\n",
              "      <td>2251</td>\n",
              "      <td>/datag/pipeline/AGBT18A_999_103/collate/splice...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18A_999_103/collate/sp...</td>\n",
              "      <td>2018-07-07 08:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30309</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30310</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30311</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30312</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30313</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30314 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Target          Session Band  Cadence ID  Frequency  \\\n",
              "0       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "1       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "2       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "3       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "4       DDO210  AGBT18A_999_103    L       24777       2251   \n",
              "...        ...              ...  ...         ...        ...   \n",
              "30309  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "30310  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "30311  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "30312  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "30313  NGC3226   AGBT22B_999_25    L      411390       1126   \n",
              "\n",
              "                                                .h5 path  \\\n",
              "0      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "1      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "2      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "3      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "4      /datag/pipeline/AGBT18A_999_103/collate/splice...   \n",
              "...                                                  ...   \n",
              "30309  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "30310  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "30311  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "30312  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "30313  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "\n",
              "                                               .dat path                 Time  \n",
              "0      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "1      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "2      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "3      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "4      /home/obs/turboseti/AGBT18A_999_103/collate/sp...  2018-07-07 08:49:26  \n",
              "...                                                  ...                  ...  \n",
              "30309  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "30310  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "30311  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "30312  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "30313  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "\n",
              "[30314 rows x 8 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['.h5 path'] = df['.h5 path'].str.replace('0000.h5', '0002.h5', regex=False)\n",
        "df = df.drop_duplicates(subset='.h5 path', keep='first').reset_index(drop=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38b7592b",
      "metadata": {
        "id": "38b7592b",
        "outputId": "a53b361e-b6b1-4939-f5ae-8a8ba54a92e0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Target</th>\n",
              "      <th>Session</th>\n",
              "      <th>Band</th>\n",
              "      <th>Cadence ID</th>\n",
              "      <th>Frequency</th>\n",
              "      <th>.h5 path</th>\n",
              "      <th>.dat path</th>\n",
              "      <th>Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AND_XIV</td>\n",
              "      <td>AGBT18B_999_07</td>\n",
              "      <td>S</td>\n",
              "      <td>30225</td>\n",
              "      <td>3151</td>\n",
              "      <td>/datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18B_999_07/blc40_blp00...</td>\n",
              "      <td>2018-08-18 08:41:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AND_XIV</td>\n",
              "      <td>AGBT18B_999_07</td>\n",
              "      <td>S</td>\n",
              "      <td>30225</td>\n",
              "      <td>3151</td>\n",
              "      <td>/datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18B_999_07/blc40_blp00...</td>\n",
              "      <td>2018-08-18 08:41:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AND_XIV</td>\n",
              "      <td>AGBT18B_999_07</td>\n",
              "      <td>S</td>\n",
              "      <td>30225</td>\n",
              "      <td>3151</td>\n",
              "      <td>/datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18B_999_07/blc40_blp00...</td>\n",
              "      <td>2018-08-18 08:41:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AND_XIV</td>\n",
              "      <td>AGBT18B_999_07</td>\n",
              "      <td>S</td>\n",
              "      <td>30225</td>\n",
              "      <td>3151</td>\n",
              "      <td>/datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18B_999_07/blc40_blp00...</td>\n",
              "      <td>2018-08-18 08:41:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AND_XIV</td>\n",
              "      <td>AGBT18B_999_07</td>\n",
              "      <td>S</td>\n",
              "      <td>30225</td>\n",
              "      <td>3151</td>\n",
              "      <td>/datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT18B_999_07/blc40_blp00...</td>\n",
              "      <td>2018-08-18 08:41:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29341</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29342</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29343</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29344</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29345</th>\n",
              "      <td>NGC3226</td>\n",
              "      <td>AGBT22B_999_25</td>\n",
              "      <td>L</td>\n",
              "      <td>411390</td>\n",
              "      <td>1126</td>\n",
              "      <td>/datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...</td>\n",
              "      <td>/home/obs/turboseti/AGBT22B_999_25/blc16_blp06...</td>\n",
              "      <td>2022-11-19 06:13:36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>29346 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Target         Session Band  Cadence ID  Frequency  \\\n",
              "0      AND_XIV  AGBT18B_999_07    S       30225       3151   \n",
              "1      AND_XIV  AGBT18B_999_07    S       30225       3151   \n",
              "2      AND_XIV  AGBT18B_999_07    S       30225       3151   \n",
              "3      AND_XIV  AGBT18B_999_07    S       30225       3151   \n",
              "4      AND_XIV  AGBT18B_999_07    S       30225       3151   \n",
              "...        ...             ...  ...         ...        ...   \n",
              "29341  NGC3226  AGBT22B_999_25    L      411390       1126   \n",
              "29342  NGC3226  AGBT22B_999_25    L      411390       1126   \n",
              "29343  NGC3226  AGBT22B_999_25    L      411390       1126   \n",
              "29344  NGC3226  AGBT22B_999_25    L      411390       1126   \n",
              "29345  NGC3226  AGBT22B_999_25    L      411390       1126   \n",
              "\n",
              "                                                .h5 path  \\\n",
              "0      /datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...   \n",
              "1      /datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...   \n",
              "2      /datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...   \n",
              "3      /datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...   \n",
              "4      /datag/pipeline/AGBT18B_999_07/blc40_blp00/blc...   \n",
              "...                                                  ...   \n",
              "29341  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "29342  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "29343  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "29344  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "29345  /datag/pipeline/AGBT22B_999_25/blc16_blp06/blc...   \n",
              "\n",
              "                                               .dat path                 Time  \n",
              "0      /home/obs/turboseti/AGBT18B_999_07/blc40_blp00...  2018-08-18 08:41:37  \n",
              "1      /home/obs/turboseti/AGBT18B_999_07/blc40_blp00...  2018-08-18 08:41:37  \n",
              "2      /home/obs/turboseti/AGBT18B_999_07/blc40_blp00...  2018-08-18 08:41:37  \n",
              "3      /home/obs/turboseti/AGBT18B_999_07/blc40_blp00...  2018-08-18 08:41:37  \n",
              "4      /home/obs/turboseti/AGBT18B_999_07/blc40_blp00...  2018-08-18 08:41:37  \n",
              "...                                                  ...                  ...  \n",
              "29341  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "29342  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "29343  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "29344  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "29345  /home/obs/turboseti/AGBT22B_999_25/blc16_blp06...  2022-11-19 06:13:36  \n",
              "\n",
              "[29346 rows x 8 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df[~df['.h5 path'].str.contains('spliced')].reset_index(drop=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f1451bd",
      "metadata": {
        "id": "2f1451bd"
      },
      "outputs": [],
      "source": [
        "df = df.drop(index = 17546)\n",
        "df = df.sample(n=100, random_state=42).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84164fd0",
      "metadata": {
        "id": "84164fd0",
        "outputId": "97ede843-2d35-4e43-fe08-fa395e08ebde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- File Info ---\n",
            "DIMENSION_LABELS : [b'time' b'feed_id' b'frequency']\n",
            "        az_start :                              0.0\n",
            "       data_type :                                1\n",
            "            fch1 :               10501.46484375 MHz\n",
            "            foff :         -0.00286102294921875 MHz\n",
            "           ibeam :                               -1\n",
            "      machine_id :                               20\n",
            "          nbeams :                                1\n",
            "           nbits :                               32\n",
            "          nchans :                            65536\n",
            "            nifs :                                1\n",
            "     rawdatafile : guppi_59411_54386_HIP30264_0094.0000.raw\n",
            "     source_name :                         HIP30264\n",
            "         src_dej :                     -8:26:53.228\n",
            "         src_raj :                      6:21:58.451\n",
            "    telescope_id :                                6\n",
            "           tsamp :                1.073741823999999\n",
            "   tstart (ISOT) :          2021-07-16T15:06:26.000\n",
            "    tstart (MJD) :                59411.62946759259\n",
            "        za_start :                              0.0\n",
            "\n",
            "Num ints in file :                              279\n",
            "      File shape :                  (279, 1, 65536)\n",
            "--- Selection Info ---\n",
            "Data selection shape :                  (279, 1, 65536)\n",
            "Minimum freq (MHz) :                10313.96770477295\n",
            "Maximum freq (MHz) :                   10501.46484375\n"
          ]
        }
      ],
      "source": [
        "fb = bl.Waterfall(df['.h5 path'].iloc[-1], load_data=True)\n",
        "fb.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51b073aa",
      "metadata": {
        "id": "51b073aa"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "import time\n",
        "\n",
        "def ensure_cpu_mem(bytes_needed, safety=0.8):\n",
        "    avail = psutil.virtual_memory().available\n",
        "    if bytes_needed > avail * safety:\n",
        "        time.sleep(120)\n",
        "\n",
        "def ensure_gpu_mem(bytes_needed, safety=0.8):\n",
        "    free, total = cp.cuda.runtime.memGetInfo()\n",
        "    if bytes_needed > free * safety:\n",
        "        time.sleep(120)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "395191b3",
      "metadata": {
        "id": "395191b3"
      },
      "outputs": [],
      "source": [
        "def merge_boxes(boxes, iou_thresh=0.1, proximity_thresh=0.05):\n",
        "    \"\"\"\n",
        "    Merge boxes if their centers are close or they have significant IoU overlap.\n",
        "    boxes: list of (cx, cy, w, h, area)\n",
        "    returns: merged list of boxes\n",
        "    \"\"\"\n",
        "\n",
        "    def iou(b1, b2):\n",
        "        x1_min = b1[0] - b1[2] / 2\n",
        "        x1_max = b1[0] + b1[2] / 2\n",
        "        y1_min = b1[1] - b1[3] / 2\n",
        "        y1_max = b1[1] + b1[3] / 2\n",
        "\n",
        "        x2_min = b2[0] - b2[2] / 2\n",
        "        x2_max = b2[0] + b2[2] / 2\n",
        "        y2_min = b2[1] - b2[3] / 2\n",
        "        y2_max = b2[1] + b2[3] / 2\n",
        "\n",
        "        inter_xmin = max(x1_min, x2_min)\n",
        "        inter_ymin = max(y1_min, y2_min)\n",
        "        inter_xmax = min(x1_max, x2_max)\n",
        "        inter_ymax = min(y1_max, y2_max)\n",
        "\n",
        "        inter_area = max(0, inter_xmax - inter_xmin) * max(0, inter_ymax - inter_ymin)\n",
        "        area1 = b1[2] * b1[3]\n",
        "        area2 = b2[2] * b2[3]\n",
        "        union = area1 + area2 - inter_area\n",
        "\n",
        "        return inter_area / union if union > 0 else 0\n",
        "\n",
        "    def distance(b1, b2):\n",
        "        return np.hypot(b1[0] - b2[0], b1[1] - b2[1])\n",
        "\n",
        "    merged = []\n",
        "    used = [False] * len(boxes)\n",
        "\n",
        "    for i, b1 in enumerate(boxes):\n",
        "        if used[i]:\n",
        "            continue\n",
        "        group = [b1]\n",
        "        used[i] = True\n",
        "        for j, b2 in enumerate(boxes):\n",
        "            if used[j]:\n",
        "                continue\n",
        "            if iou(b1, b2) > iou_thresh or distance(b1, b2) < proximity_thresh:\n",
        "                group.append(b2)\n",
        "                used[j] = True\n",
        "        if len(group) == 1:\n",
        "            merged.append(group[0])\n",
        "        else:\n",
        "            # merge into one big box (average center, min enclosing width/height)\n",
        "            xs = [b[0] for b in group]\n",
        "            ys = [b[1] for b in group]\n",
        "            ws = [b[2] for b in group]\n",
        "            hs = [b[3] for b in group]\n",
        "            xmins = [x - w/2 for x, w in zip(xs, ws)]\n",
        "            xmaxs = [x + w/2 for x, w in zip(xs, ws)]\n",
        "            ymins = [y - h/2 for y, h in zip(ys, hs)]\n",
        "            ymaxs = [y + h/2 for y, h in zip(ys, hs)]\n",
        "            new_x = (min(xmins) + max(xmaxs)) / 2\n",
        "            new_y = (min(ymins) + max(ymaxs)) / 2\n",
        "            new_w = max(xmaxs) - min(xmins)\n",
        "            new_h = max(ymaxs) - min(ymins)\n",
        "            merged.append((new_x, new_y, new_w, new_h, new_w * new_h))\n",
        "    return merged\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54d1011a",
      "metadata": {
        "id": "54d1011a"
      },
      "outputs": [],
      "source": [
        "# def generate_yolo_boxes(edge_map, threshold=0.1):\n",
        "#     binary = (edge_map > threshold).astype(np.uint8)\n",
        "#     labeled, n = connected_components(binary)\n",
        "#     boxes = []\n",
        "#     for i in range(1, n + 1):\n",
        "#         coords = np.argwhere(labeled == i)\n",
        "#         if coords.shape[0] < 10:\n",
        "#             continue\n",
        "#         y0, x0 = coords.min(axis=0)\n",
        "#         y1, x1 = coords.max(axis=0)\n",
        "#         cx = (x0 + x1) / 2 / edge_map.shape[1]\n",
        "#         cy = (y0 + y1) / 2 / edge_map.shape[0]\n",
        "#         w = (x1 - x0) / edge_map.shape[1]\n",
        "#         h = (y1 - y0) / edge_map.shape[0]\n",
        "#         boxes.append((cx, cy, w, h, w * h))\n",
        "#         return boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8aaea8e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_tasks(df):\n",
        "    \"\"\"\n",
        "    Build a flat list of (h5_path, channel_idx) from your DataFrame,\n",
        "    skipping any channels that fall into known notch filter frequency ranges.\n",
        "    \"\"\"\n",
        "    GBT_NOTCH_FILTERS = {\n",
        "        \"L\": [(1200, 1340)],\n",
        "        \"S\": [(2300, 2360)],\n",
        "    }\n",
        "\n",
        "    tasks = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        h5 = row[\".h5 path\"]\n",
        "        band = row[\"Band\"]  # e.g., 'L', 'S', etc.\n",
        "        fb = bl.Waterfall(h5, load_data=False)\n",
        "        nfreq = fb.header.get(\"nchans\")\n",
        "        nfpc = fb.header.get(\"nfpc\", 1024)\n",
        "        fch1 = fb.header[\"fch1\"]\n",
        "        foff = fb.header[\"foff\"]\n",
        "        n_coarse = nfreq // nfpc\n",
        "\n",
        "        for ch in range(n_coarse):\n",
        "            f0 = fch1 + ch * nfpc * foff\n",
        "            f1 = fch1 + (ch + 1) * nfpc * foff\n",
        "            f_min, f_max = sorted([f0, f1])\n",
        "\n",
        "            # Check against notch filter exclusion ranges\n",
        "            skip = False\n",
        "            if band in GBT_NOTCH_FILTERS:\n",
        "                for lo, hi in GBT_NOTCH_FILTERS[band]:\n",
        "                    if lo <= f_min <= hi or lo <= f_max <= hi:\n",
        "                        skip = True\n",
        "                        break\n",
        "            if not skip:\n",
        "                tasks.append((h5, ch))\n",
        "\n",
        "    return tasks\n",
        "\n",
        "\n",
        "def split_tasks(tasks, train_frac=0.8, seed=42):\n",
        "    \"\"\"\n",
        "    Shuffle & split the flat task list into train vs. val sets.\n",
        "    Returns two sets of (h5_path, channel_idx).\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    shuffled = tasks.copy()\n",
        "    random.shuffle(shuffled)\n",
        "    cut = int(train_frac * len(shuffled))\n",
        "    train = set(shuffled[:cut])\n",
        "    val   = set(shuffled[cut:])\n",
        "    return train, val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30d029de",
      "metadata": {
        "id": "30d029de"
      },
      "outputs": [],
      "source": [
        "# import cv2\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch import nn\n",
        "# from pathlib import Path\n",
        "# from PIL import Image, ImageDraw\n",
        "# import blimpy as bl\n",
        "# from skimage.segmentation import slic\n",
        "# #from yolo_moe_pytorch_channel import generate_yolo_boxes\n",
        "\n",
        "# # ─── 1) Define your rule-based gate ──────────────────────────────────────\n",
        "# class RuleGate(nn.Module):\n",
        "#     def __init__(self, w_align=2.0, w_ent=1.0, bias=-0.5):\n",
        "#         super().__init__()\n",
        "#         # simple fixed scalars (or wrap in nn.Parameter to fine‐tune later)\n",
        "#         self.w_align = w_align\n",
        "#         self.w_ent   = w_ent\n",
        "#         self.bias    = bias\n",
        "\n",
        "#     def forward(self, h_sf, h_ued):\n",
        "#         # h_sf, h_ued are torch.tensors of shape [R,3] or [R,4]\n",
        "#         # index 1 = alignment, 2 = entropy\n",
        "#         align = h_ued[:,1] - h_sf[:,1]       # higher → favor Sobel\n",
        "#         ent   = h_sf[:,2] - h_ued[:,2]       # higher → favor Sobel\n",
        "#         score  = self.w_align*align + self.w_ent*ent + self.bias\n",
        "#         return torch.sigmoid(score)          # [R] weights for Sobel\n",
        "\n",
        "# # ─── 2) Heuristic helper (no change) ────────────────────────────────────\n",
        "# def compute_heuristics(edge_map, gray):\n",
        "#     mask = edge_map > 0.2\n",
        "#     density = float(mask.mean())\n",
        "#     # alignment\n",
        "#     if density > 0:\n",
        "#         gx = cv2.Sobel(gray, cv2.CV_32F, 1, 0, 3)\n",
        "#         gy = cv2.Sobel(gray, cv2.CV_32F, 0, 1, 3)\n",
        "#         alignment = float(np.hypot(gx, gy)[mask].mean())\n",
        "#     else:\n",
        "#         alignment = 0.0\n",
        "#     # entropy\n",
        "#     entropy = float(-np.sum(edge_map * np.log2(edge_map + 1e-8)))\n",
        "#     # linearity (aspect‐ratio proxy)\n",
        "#     bw = mask.astype(np.uint8)\n",
        "#     n, labels = cv2.connectedComponents(bw)\n",
        "#     ratios = []\n",
        "#     for L in range(1, n):\n",
        "#         ys, xs = np.where(labels == L)\n",
        "#         if ys.size:\n",
        "#             h, w = ys.ptp()+1, xs.ptp()+1\n",
        "#             ratios.append(float(w)/h if h else 0.0)\n",
        "#     linearity = float(np.mean(ratios)) if ratios else 0.0\n",
        "\n",
        "#     if density < 0.01:\n",
        "#         return [0.0, 0.0, 0.0, 0.0]\n",
        "#     return [density, alignment, entropy, linearity]\n",
        "\n",
        "\n",
        "# # ─── 3) The new process_file ─────────────────────────────────────────────\n",
        "# def process_file(job):\n",
        "#     (h5_path, channels, gidxs,\n",
        "#      base_dir, factor, dilation,\n",
        "#      class_id, train_set, val_set,\n",
        "#      pad_width) = job\n",
        "\n",
        "#     # Load .h5 once\n",
        "#     fb   = bl.Waterfall(h5_path, load_data=True)\n",
        "#     data = 10*np.log10(fb.data.squeeze())  # (ntime, nfreq)\n",
        "\n",
        "#     # Prepare SF detector and gate\n",
        "#     EDGE_MODEL = \"/home/jliang/gbt-rfi/model.yml.gz\"\n",
        "#     sf_det     = cv2.ximgproc.createStructuredEdgeDetection(EDGE_MODEL)\n",
        "#     gate       = RuleGate()  # use default w_align/w_ent/bias\n",
        "\n",
        "#     for ch_idx, gidx in zip(channels, gidxs):\n",
        "#         # [.. create img_dir, lbl_dir, vis_dir ..]\n",
        "#         subset = \"train\" if (h5_path,ch_idx) in train_set else \"val\"\n",
        "#         img_dir = Path(base_dir)/subset/\"images\"\n",
        "#         lbl_dir = Path(base_dir)/subset/\"labels\"\n",
        "#         vis_dir = Path(base_dir)/\"visualization\"/subset\n",
        "#         for d in (img_dir,lbl_dir,vis_dir): d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#         # Extract & clean the block\n",
        "#         cw = fb.header.get(\"nfpc\",1024)\n",
        "#         f0, f1 = ch_idx*cw, (ch_idx+1)*cw\n",
        "#         block = data[:,f0:f1]\n",
        "#         low, high = int(0.1*cw), int(0.9*cw)\n",
        "#         block = block[:,low:high]\n",
        "#         h_img, w_img = block.shape\n",
        "\n",
        "#         # Build img for SF\n",
        "#         norm   = (block - block.min())/(block.ptp()+1e-6)\n",
        "#         img3   = np.stack([norm]*3, axis=-1).astype(np.float32)\n",
        "#         sf_map = sf_det.detectEdges(img3).squeeze()\n",
        "#         gray8  = (255*norm).astype(np.uint8)\n",
        "\n",
        "#         # Sobel map\n",
        "#         gx      = cv2.Sobel(gray8, cv2.CV_32F, 1,0,3)\n",
        "#         gy      = cv2.Sobel(gray8, cv2.CV_32F, 0,1,3)\n",
        "#         ued_map = np.hypot(gx, gy)\n",
        "#         ued_map = cv2.normalize(ued_map, None, 0,1, cv2.NORM_MINMAX)\n",
        "\n",
        "#         # Fuse per-superpixel or per-pixel\n",
        "#         fused = np.zeros_like(sf_map, dtype=np.float32)\n",
        "#         segments = slic(img3, n_segments=100, compactness=10)\n",
        "#         for v in np.unique(segments):\n",
        "#             mask = (segments==v)\n",
        "#             if mask.sum()<50:\n",
        "#                 continue\n",
        "#             # compute heuristics for this region\n",
        "#             gray8 = (255*norm).astype(np.uint8)\n",
        "#             h_sf  = compute_heuristics(sf_map[mask], gray8[mask])\n",
        "#             h_ued = compute_heuristics(ued_map[mask], gray8[mask])\n",
        "#             h_sf_t  = torch.tensor([h_sf], dtype=torch.float32)\n",
        "#             h_ued_t = torch.tensor([h_ued],dtype=torch.float32)\n",
        "#             p = gate(h_sf_t, h_ued_t).item()  # how much to trust Sobel\n",
        "#             fused[mask] = p*ued_map[mask] + (1-p)*sf_map[mask]\n",
        "\n",
        "#         # YOLO boxes\n",
        "#         boxes = generate_yolo_boxes(fused, threshold=0.1)\n",
        "#         # [.. save PNG, write .txt, draw visualization exactly as before ..]\n",
        "#         # Calculate frequency range of this coarse channel\n",
        "#         f_start = fb.header['fch1'] + f0 * fb.header['foff']\n",
        "#         f_stop  = fb.header['fch1'] + (f1 - 1) * fb.header['foff']\n",
        "\n",
        "#         # Build filename\n",
        "#         fn = f\"img_{gidx:0{pad_width}d}_f_{f_start:.4f}_{f_stop:.4f}.png\"\n",
        "#         img_path = img_dir / fn\n",
        "#         txt_path = lbl_dir / fn.replace(\".png\", \".txt\")\n",
        "\n",
        "#         # Save image\n",
        "#         arr8 = (255 * (block - block.min()) / (block.ptp() + 1e-6)).astype(np.uint8)\n",
        "#         img = Image.fromarray(np.stack([arr8]*3, axis=-1))\n",
        "#         img.save(img_path)\n",
        "\n",
        "#         # Write YOLO labels\n",
        "#         with open(txt_path, \"w\") as f:\n",
        "#             for x_c, y_c, w_n, h_n in boxes:\n",
        "#                 f.write(f\"{class_id} {x_c:.6f} {y_c:.6f} {w_n:.6f} {h_n:.6f}\\n\")\n",
        "\n",
        "#         # If empty, log it\n",
        "#         if not boxes:\n",
        "#             (lbl_dir/\"empty_labels.csv\").open(\"a\").write(f\"{fn},{h5_path},{ch_idx}\\n\")\n",
        "\n",
        "#         # Draw & save visualization\n",
        "#         vis_img = img.convert(\"RGB\")\n",
        "#         draw    = ImageDraw.Draw(vis_img)\n",
        "#         for x_c, y_c, w_n, h_n in boxes:\n",
        "#             xc, yc = x_c*w_img, y_c*h_img\n",
        "#             bw, bh = w_n*w_img, h_n*h_img\n",
        "#             x0, y0 = int(xc - bw/2), int(yc - bh/2)\n",
        "#             x1, y1 = int(xc + bw/2), int(yc + bh/2)\n",
        "#             draw.rectangle([x0, y0, x1, y1], outline=\"red\", width=2)\n",
        "#         vis_img.save(vis_dir/fn)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qwWGx7cP07GG",
      "metadata": {
        "id": "qwWGx7cP07GG"
      },
      "outputs": [],
      "source": [
        "# import cv2\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch import nn\n",
        "# from pathlib import Path\n",
        "# from PIL import Image, ImageDraw\n",
        "# import blimpy as bl\n",
        "# from skimage.segmentation import slic\n",
        "# from skimage.filters import threshold_sauvola\n",
        "# import scipy.ndimage\n",
        "# from collections import namedtuple\n",
        "# from scipy.stats import kurtosis\n",
        "\n",
        "# # ─── 0) BoundingBox, IOU & Merge ──────────────────────────────────────────\n",
        "# BoundingBox = namedtuple(\"BoundingBox\",\n",
        "#     [\"x_c\",\"y_c\",\"w\",\"h\",\"area\",\"class_id\"]\n",
        "# )\n",
        "\n",
        "# def calculate_iou(b1, b2):\n",
        "#     x1min, x1max = b1.x_c - b1.w/2, b1.x_c + b1.w/2\n",
        "#     y1min, y1max = b1.y_c - b1.h/2, b1.y_c + b1.h/2\n",
        "#     x2min, x2max = b2.x_c - b2.w/2, b2.x_c + b2.w/2\n",
        "#     y2min, y2max = b2.y_c - b2.h/2, b2.y_c + b2.h/2\n",
        "\n",
        "#     xi0 = max(x1min, x2min); yi0 = max(y1min, y2min)\n",
        "#     xi1 = min(x1max, x2max); yi1 = min(y1max, y2max)\n",
        "#     if xi1<=xi0 or yi1<=yi0:\n",
        "#         return 0.0\n",
        "#     inter = (xi1-xi0)*(yi1-yi0)\n",
        "#     union = b1.w*b1.h + b2.w*b2.h - inter\n",
        "#     return inter/union if union>0 else 0.0\n",
        "\n",
        "# def merge_boxes(boxes, iou_thresh=0.1, prox_thresh=0.05):\n",
        "#     used = [False]*len(boxes)\n",
        "#     merged = []\n",
        "#     for i,b in enumerate(boxes):\n",
        "#         if used[i]: continue\n",
        "#         group = [b]; used[i]=True\n",
        "#         for j in range(i+1,len(boxes)):\n",
        "#             if used[j]: continue\n",
        "#             bj = boxes[j]\n",
        "#             dist = np.hypot(b.x_c-bj.x_c, b.y_c-bj.y_c)\n",
        "#             if calculate_iou(b,bj)>iou_thresh or dist<prox_thresh:\n",
        "#                 group.append(bj); used[j]=True\n",
        "#         if len(group)==1:\n",
        "#             merged.append(b)\n",
        "#         else:\n",
        "#             xmins = [g.x_c-g.w/2 for g in group]\n",
        "#             xmaxs = [g.x_c+g.w/2 for g in group]\n",
        "#             ymins = [g.y_c-g.h/2 for g in group]\n",
        "#             ymaxs = [g.y_c+g.h/2 for g in group]\n",
        "#             new_w = max(xmaxs)-min(xmins)\n",
        "#             new_h = max(ymaxs)-min(ymins)\n",
        "#             new_x = (min(xmins)+max(xmaxs))/2\n",
        "#             new_y = (min(ymins)+max(ymaxs))/2\n",
        "#             new_area = new_w*new_h\n",
        "#             merged.append(BoundingBox(new_x,new_y,new_w,new_h,new_area,group[0].class_id))\n",
        "#     return merged\n",
        "\n",
        "# # ─── 1) Improved YOLO box generator ───────────────────────────────────────\n",
        "# def generate_yolo_boxes(edge_map,\n",
        "#                         threshold=0.1,\n",
        "#                         min_pixels=20,\n",
        "#                         iou_thresh=0.1,\n",
        "#                         prox_thresh=0.05):\n",
        "#     H,W = edge_map.shape\n",
        "\n",
        "#     # (a) Knock out persistent horizontal RFI bands\n",
        "#     bw = (edge_map>threshold).astype(np.uint8)\n",
        "#     horiz_kernel = np.ones((1,50),np.uint8)\n",
        "#     band_mask = scipy.ndimage.binary_opening(bw, structure=horiz_kernel)\n",
        "#     clean = edge_map.copy()\n",
        "#     clean[band_mask] = np.median(edge_map)\n",
        "\n",
        "#     # (b) Local Sauvola threshold\n",
        "#     th = threshold_sauvola(clean, window_size=51, k=0.3)\n",
        "#     mask = clean > th\n",
        "\n",
        "#     # (c) Morphological cleanup\n",
        "#     mask = scipy.ndimage.binary_opening(mask, structure=np.ones((5,5)))\n",
        "#     mask = scipy.ndimage.binary_closing(mask, structure=np.ones((1,30)))\n",
        "#     mask = scipy.ndimage.binary_closing(mask, structure=np.ones((30,1)))\n",
        "\n",
        "#     # (d) CC → raw boxes\n",
        "#     labeled, n = scipy.ndimage.label(mask)\n",
        "#     boxes=[]\n",
        "#     for lab in range(1,n+1):\n",
        "#         ys,xs = np.where(labeled==lab)\n",
        "#         if ys.size<min_pixels: continue\n",
        "#         y0,y1 = ys.min(), ys.max()\n",
        "#         x0,x1 = xs.min(), xs.max()\n",
        "#         w = (x1-x0)/W; h=(y1-y0)/H\n",
        "#         x_c=(x0+(x1-x0)/2)/W; y_c=(y0+(y1-y0)/2)/H\n",
        "#         area=w*h\n",
        "#         boxes.append(BoundingBox(x_c,y_c,w,h,area, class_id=0))\n",
        "\n",
        "#     # (e) Merge clusters\n",
        "#     boxes = merge_boxes(boxes, iou_thresh, prox_thresh)\n",
        "#     return boxes\n",
        "\n",
        "# # ─── 2) Your RuleGate & compute_heuristics (unchanged) ────────────────────\n",
        "# class RuleGate(nn.Module):\n",
        "#     def __init__(self, w_align=2.0, w_ent=1.0, bias=-0.5):\n",
        "#         super().__init__()\n",
        "#         self.w_align, self.w_ent, self.bias = w_align, w_ent, bias\n",
        "#     def forward(self, h_sf, h_ued):\n",
        "#         align = h_ued[:,1]-h_sf[:,1]\n",
        "#         ent   = h_sf[:,2]-h_ued[:,2]\n",
        "#         return torch.sigmoid(self.w_align*align + self.w_ent*ent + self.bias)\n",
        "\n",
        "# def compute_heuristics(edge_map, gray):\n",
        "#     mask = edge_map>0.2\n",
        "#     density = float(mask.mean())\n",
        "#     if density>0:\n",
        "#         gx=cv2.Sobel(gray,cv2.CV_32F,1,0,3)\n",
        "#         gy=cv2.Sobel(gray,cv2.CV_32F,0,1,3)\n",
        "#         alignment=float(np.hypot(gx,gy)[mask].mean())\n",
        "#     else:\n",
        "#         alignment=0.0\n",
        "#     entropy = float(-np.sum(edge_map*np.log2(edge_map+1e-8)))\n",
        "#     bw=mask.astype(np.uint8)\n",
        "#     n,labs=cv2.connectedComponents(bw)\n",
        "#     ratios=[]\n",
        "#     for L in range(1,n):\n",
        "#         ys,xs=np.where(labs==L)\n",
        "#         if ys.size:\n",
        "#             ratios.append((np.ptp(xs)+1)/(np.ptp(ys)+1))\n",
        "#     linearity=float(np.mean(ratios)) if ratios else 0.0\n",
        "#     return [density, alignment, entropy, linearity] if density>=0.01 else [0,0,0,0]\n",
        "\n",
        "# # ─── 3) Updated process_file ──────────────────────────────────────────────\n",
        "# def process_file(job):\n",
        "#     (h5_path, channels, gidxs,\n",
        "#      base_dir,\n",
        "#      class_id, train_set, val_set,\n",
        "#      pad_width, file_idx) = job\n",
        "\n",
        "#     fb = bl.Waterfall(h5_path, load_data=True)\n",
        "#     data = 10*np.log10(fb.data.squeeze())\n",
        "#     nt,nf = data.shape\n",
        "#     nfpc = fb.header.get(\"nfpc\",1024)\n",
        "\n",
        "#     # SF detector & RuleGate\n",
        "#     EDGE_MODEL=\"/home/jliang/gbt-rfi/model.yml.gz\"\n",
        "#     sf_det = cv2.ximgproc.createStructuredEdgeDetection(EDGE_MODEL)\n",
        "#     gate   = RuleGate()\n",
        "\n",
        "#     for ch_idx, gidx in zip(channels, gidxs):\n",
        "#         subset = \"train\" if (h5_path,ch_idx) in train_set else \"val\"\n",
        "#         img_dir=Path(base_dir)/subset/\"images\"\n",
        "#         lbl_dir=Path(base_dir)/subset/\"labels\"\n",
        "#         vis_dir=Path(base_dir)/\"visualization\"/subset\n",
        "#         for d in (img_dir,lbl_dir,vis_dir): d.mkdir(parents=True,exist_ok=True)\n",
        "\n",
        "#         cw = fb.header.get(\"nfpc\",1024)\n",
        "#         f0, f1 = ch_idx*cw, (ch_idx+1)*cw\n",
        "#         block = data[:,f0:f1]\n",
        "#         low, high = int(0.15*cw), int(0.85*cw)\n",
        "#         block = block[:,low:high]\n",
        "#         H, W = block.shape\n",
        "\n",
        "#         # Remove vertical line artifact\n",
        "#         rows, cols = block.shape\n",
        "#         vert_means = block.mean(axis=0)\n",
        "#         center = np.argmax(vert_means)\n",
        "#         left_col = center - 1\n",
        "#         right_col = center + 1\n",
        "#         if left_col >= 0 and right_col < cols:\n",
        "#             block[:, center] = (block[:, left_col] + block[:, right_col]) / 2\n",
        "#         elif left_col >= 0:\n",
        "#             block[:, center] = block[:, left_col]\n",
        "#         elif right_col < cols:\n",
        "#             block[:, center] = block[:, right_col]\n",
        "\n",
        "#         # normalize\n",
        "#         norm=(block-block.min())/(np.ptp(block)+1e-8)\n",
        "#         img3=np.stack([norm]*3,axis=-1).astype(np.float32)\n",
        "\n",
        "#         # SF map\n",
        "#         sf_map=sf_det.detectEdges(img3).squeeze()\n",
        "#         gray8=(norm*255).astype(np.uint8)\n",
        "\n",
        "#         # Sobel map\n",
        "#         gx=cv2.Sobel(gray8,cv2.CV_32F,1,0,3)\n",
        "#         gy=cv2.Sobel(gray8,cv2.CV_32F,0,1,3)\n",
        "#         ued_map=np.hypot(gx,gy)\n",
        "#         ued_map=cv2.normalize(ued_map,None,0,1,cv2.NORM_MINMAX)\n",
        "\n",
        "#         # fuse per-superpixel\n",
        "#         fused=np.zeros_like(sf_map,dtype=np.float32)\n",
        "#         segments=slic(img3,n_segments=100,compactness=10)\n",
        "#         for seg in np.unique(segments):\n",
        "#             m=segments==seg\n",
        "#             if m.sum()<50: continue\n",
        "#             h_sf=compute_heuristics(sf_map[m], gray8[m])\n",
        "#             h_ued=compute_heuristics(ued_map[m],gray8[m])\n",
        "#             p=gate(torch.tensor([h_sf]),torch.tensor([h_ued])).item()\n",
        "#             fused[m]=p*ued_map[m]+(1-p)*sf_map[m]\n",
        "\n",
        "#         # generate & save YOLO boxes\n",
        "#         boxes=generate_yolo_boxes(fused,\n",
        "#                                   threshold=0.2,\n",
        "#                                   min_pixels=30,\n",
        "#                                   iou_thresh=0.1,\n",
        "#                                   prox_thresh=0.05)\n",
        "\n",
        "#         k_val = kurtosis(gray8.flatten(), fisher=True, bias=False)\n",
        "#         f_start = fb.header['fch1'] + f0 * fb.header['foff']\n",
        "#         f_stop  = fb.header['fch1'] + (f1 - 1) * fb.header['foff']\n",
        "#         fn = f\"img_{k_val:.3f}_f_{f_start:.4f}_{f_stop:.4f}.png\"\n",
        "#         arr8=(norm*255).astype(np.uint8)\n",
        "#         img=Image.fromarray(arr8).convert(\"RGB\")\n",
        "#         img.save(img_dir/fn)\n",
        "\n",
        "#         with open(lbl_dir/fn.replace(\".png\",\".txt\"),\"w\") as f:\n",
        "#             for b in boxes:\n",
        "#                 f.write(f\"{class_id} {b.x_c:.6f} {b.y_c:.6f} {b.w:.6f} {b.h:.6f}\\n\")\n",
        "#         if not boxes:\n",
        "#             (lbl_dir/\"empty_labels.csv\").open(\"a\").write(f\"{fn},{h5_path},{ch_idx}\\n\")\n",
        "\n",
        "#         vis=img.copy(); draw=ImageDraw.Draw(vis)\n",
        "#         for b in boxes:\n",
        "#             x0=int((b.x_c-b.w/2)*W); y0=int((b.y_c-b.h/2)*H)\n",
        "#             x1=int((b.x_c+b.w/2)*W); y1=int((b.y_c+b.h/2)*H)\n",
        "#             draw.rectangle([x0,y0,x1,y1],outline=\"red\",width=2)\n",
        "#         vis.save(vis_dir/fn)\n",
        "\n",
        "#     logging.info(f\"Processed {h5_path}\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "#     # 1) your tasks must be list of (h5_path, ch_idx)\n",
        "#     tasks = build_tasks(df)\n",
        "#     train_set, val_set = split_tasks(tasks)\n",
        "\n",
        "#     BASE_DIR    = '/datax/scratch/jliang/dataset_moe_local_thresholding-v2'\n",
        "#     NUM_WORKERS = 4\n",
        "\n",
        "#     # 2) pad_width for zero-padding file_idx\n",
        "#     total_images = len(tasks)\n",
        "#     pad_width    = len(str(total_images))\n",
        "\n",
        "#     # 3) build jobs, now unpacks cleanly in process_file\n",
        "#     job_list = []\n",
        "#     for file_idx, (h5_path, ch_idx) in enumerate(tasks):\n",
        "#         job = (\n",
        "#             h5_path,\n",
        "#             [ch_idx],       # channels\n",
        "#             [file_idx],     # gidxs (or whatever you want)\n",
        "#             BASE_DIR,\n",
        "#             0,              # class_id (default)\n",
        "#             train_set,\n",
        "#             val_set,\n",
        "#             pad_width,\n",
        "#             file_idx        # <-- now process_file will get this too\n",
        "#         )\n",
        "#         job_list.append(job)\n",
        "\n",
        "#     # 4) submit and catch errors\n",
        "#     with ProcessPoolExecutor(max_workers=NUM_WORKERS) as exe:\n",
        "#         futures = [exe.submit(process_file, job) for job in job_list]\n",
        "#         for fut in tqdm(as_completed(futures), total=len(futures)):\n",
        "#             try:\n",
        "#                 fut.result()   # raise any exceptions here\n",
        "#             except Exception as e:\n",
        "#                 logging.error(f\"Error in job: {e}\")\n",
        "\n",
        "#     logging.info(\"All tasks completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BjhpAVLI07lJ",
      "metadata": {
        "id": "BjhpAVLI07lJ"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageDraw\n",
        "import blimpy as bl\n",
        "from skimage.segmentation import slic\n",
        "from skimage.filters import threshold_sauvola\n",
        "import scipy.ndimage\n",
        "from scipy.stats import kurtosis\n",
        "from astropy.stats import sigma_clip\n",
        "import scipy.signal\n",
        "from collections import namedtuple\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# ─── 0) Task builders ─────────────────────────────────────────────────────\n",
        "def build_tasks(df):\n",
        "    tasks = []\n",
        "    for _, row in df.iterrows():\n",
        "        h5 = row[\".h5 path\"]\n",
        "        fb = bl.Waterfall(h5, load_data=False)\n",
        "        nfreq = fb.header.get(\"nchans\")\n",
        "        nfpc  = fb.header.get(\"nfpc\", 1024)\n",
        "        n_coarse = nfreq // nfpc\n",
        "        for ch in range(n_coarse):\n",
        "            tasks.append((h5, ch))\n",
        "    return tasks\n",
        "\n",
        "def split_tasks(tasks, train_frac=0.8, seed=42):\n",
        "    random.seed(seed)\n",
        "    sh = tasks.copy()\n",
        "    random.shuffle(sh)\n",
        "    cut = int(train_frac * len(sh))\n",
        "    return set(sh[:cut]), set(sh[cut:])\n",
        "\n",
        "# ─── 1) BoundingBox, IOU & Merge ──────────────────────────────────────────\n",
        "BoundingBox = namedtuple(\"BoundingBox\",\n",
        "    [\"x_c\",\"y_c\",\"w\",\"h\",\"area\",\"class_id\"]\n",
        ")\n",
        "\n",
        "def calculate_iou(b1, b2):\n",
        "    x1min, x1max = b1.x_c - b1.w/2, b1.x_c + b1.w/2\n",
        "    y1min, y1max = b1.y_c - b1.h/2, b1.y_c + b1.h/2\n",
        "    x2min, x2max = b2.x_c - b2.w/2, b2.x_c + b2.w/2\n",
        "    y2min, y2max = b2.y_c - b2.h/2, b2.y_c + b2.h/2\n",
        "\n",
        "    xi0 = max(x1min, x2min); yi0 = max(y1min, y2min)\n",
        "    xi1 = min(x1max, x2max); yi1 = min(y1max, y2max)\n",
        "    if xi1 <= xi0 or yi1 <= yi0:\n",
        "        return 0.0\n",
        "    inter = (xi1-xi0)*(yi1-yi0)\n",
        "    union = b1.w*b1.h + b2.w*b2.h - inter\n",
        "    return inter/union if union>0 else 0.0\n",
        "\n",
        "def merge_boxes(boxes, iou_thresh=0.1, prox_thresh=0.05):\n",
        "    used = [False]*len(boxes)\n",
        "    merged = []\n",
        "    for i,b in enumerate(boxes):\n",
        "        if used[i]: continue\n",
        "        group = [b]; used[i]=True\n",
        "        for j in range(i+1, len(boxes)):\n",
        "            if used[j]: continue\n",
        "            bj = boxes[j]\n",
        "            dist = np.hypot(b.x_c - bj.x_c, b.y_c - bj.y_c)\n",
        "            if calculate_iou(b,bj)>iou_thresh or dist<prox_thresh:\n",
        "                group.append(bj); used[j]=True\n",
        "        if len(group)==1:\n",
        "            merged.append(b)\n",
        "        else:\n",
        "            xmins = [g.x_c - g.w/2 for g in group]\n",
        "            xmaxs = [g.x_c + g.w/2 for g in group]\n",
        "            ymins = [g.y_c - g.h/2 for g in group]\n",
        "            ymaxs = [g.y_c + g.h/2 for g in group]\n",
        "            new_w = max(xmaxs) - min(xmins)\n",
        "            new_h = max(ymaxs) - min(ymins)\n",
        "            new_x = (min(xmins) + max(xmaxs)) / 2\n",
        "            new_y = (min(ymins) + max(ymaxs)) / 2\n",
        "            new_area = new_w * new_h\n",
        "            merged.append(BoundingBox(new_x, new_y, new_w, new_h, new_area, group[0].class_id))\n",
        "    return merged\n",
        "\n",
        "# ─── 1.1) Spectral SNR‐based verifier (borrowed) ────────────────────────────\n",
        "def snr_bounds(spec, snr=5):\n",
        "    \"\"\"\n",
        "    Returns (l, r, metadata) if a peak exceeds snr×noise_std,\n",
        "    else raises ValueError.\n",
        "    \"\"\"\n",
        "    peak = np.argmax(spec)\n",
        "    y = sigma_clip(spec)\n",
        "    x = np.arange(len(spec))\n",
        "\n",
        "    coeffs = np.polyfit(x[~y.mask], y[~y.mask], 1)\n",
        "    poly = np.poly1d(coeffs)\n",
        "    std = np.std(y[~y.mask] - poly(x[~y.mask]))\n",
        "\n",
        "    widths, heights, left_ips, right_ips = scipy.signal.peak_widths(\n",
        "        spec, [peak], 1 - (snr * std) / np.max(spec)\n",
        "    )\n",
        "    if np.isnan(left_ips[0]) or np.isnan(right_ips[0]):\n",
        "        raise ValueError(\"No peak above SNR threshold\")\n",
        "    l = int(np.floor(left_ips[0])) + 1\n",
        "    r = int(np.ceil(right_ips[0]))\n",
        "\n",
        "    return l, r, {\"std\": std, \"spec_max\": np.max(spec)}\n",
        "\n",
        "def verify_box(intensity_map, box, snr_min=5):\n",
        "    H, W = intensity_map.shape\n",
        "    x0 = int((box.x_c - box.w/2) * W)\n",
        "    x1 = int((box.x_c + box.w/2) * W)\n",
        "    # collapse freq-slice\n",
        "    slice_ = intensity_map[:, max(x0,0):min(x1,W)].mean(axis=1)\n",
        "    try:\n",
        "        snr_bounds(slice_, snr=snr_min)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "# ─── 2) YOLO‐style Box Generator with solidity & spectral verification ──\n",
        "def generate_yolo_boxes(edge_map,\n",
        "                        intensity_map,\n",
        "                        thresh_factor=3.5,\n",
        "                        min_pixels=30,\n",
        "                        max_frac=0.6,\n",
        "                        kurtosis_thresh=0.7,\n",
        "                        solidity_thresh=0.5,\n",
        "                        iou_thresh=0.1,\n",
        "                        prox_thresh=0.05,\n",
        "                        snr_thresh=6):\n",
        "    H, W = edge_map.shape\n",
        "\n",
        "    # (a) remove horiz bands\n",
        "    bw = (edge_map>0).astype(np.uint8)\n",
        "    horiz = np.ones((1, W//4), np.uint8)\n",
        "    band_mask = scipy.ndimage.binary_opening(bw, structure=horiz)\n",
        "    clean = edge_map.copy()\n",
        "    clean[band_mask] = np.median(edge_map)\n",
        "\n",
        "    # (b) global threshold\n",
        "    M, S = np.median(clean), np.std(clean)\n",
        "    mask = clean > (M + thresh_factor * S)\n",
        "\n",
        "    # (c) Sauvola\n",
        "    sauv = threshold_sauvola(clean, window_size=61, k=0.3)\n",
        "    mask |= (clean > sauv)\n",
        "\n",
        "    # (d) morphology\n",
        "    mask = scipy.ndimage.binary_opening(mask, structure=np.ones((3,3)))\n",
        "    mask = scipy.ndimage.binary_closing(mask, structure=np.ones((1,20)))\n",
        "    mask = scipy.ndimage.binary_closing(mask, structure=np.ones((20,1)))\n",
        "\n",
        "    # (e) CC → raw boxes with solidity & kurtosis\n",
        "    labeled, n = scipy.ndimage.label(mask)\n",
        "    boxes = []\n",
        "    for lab in range(1, n+1):\n",
        "        ys, xs = np.where(labeled==lab)\n",
        "        if ys.size < min_pixels:\n",
        "            continue\n",
        "        y0, y1 = ys.min(), ys.max()\n",
        "        x0, x1 = xs.min(), xs.max()\n",
        "        w_frac = (x1 - x0) / W\n",
        "        h_frac = (y1 - y0) / H\n",
        "        if w_frac>max_frac or h_frac>max_frac:\n",
        "            continue\n",
        "        ar = w_frac/h_frac if h_frac>0 else 999\n",
        "        if ar>8 or ar<0.125:\n",
        "            continue\n",
        "\n",
        "        # solidity filter\n",
        "        region_mask = (labeled[y0:y1+1, x0:x1+1]==lab).astype(np.uint8)\n",
        "        contours, _ = cv2.findContours(region_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        if not contours:\n",
        "            continue\n",
        "        cnt = contours[0]\n",
        "        hull = cv2.convexHull(cnt)\n",
        "        hull_area = cv2.contourArea(hull)\n",
        "        region_area = region_mask.sum()\n",
        "        solidity = region_area / (hull_area + 1e-8)\n",
        "        if solidity < solidity_thresh:\n",
        "            continue\n",
        "\n",
        "        # kurtosis filter\n",
        "        patch = intensity_map[y0:y1+1, x0:x1+1].ravel()\n",
        "        if kurtosis(patch, fisher=True, bias=False) < kurtosis_thresh:\n",
        "            continue\n",
        "\n",
        "        x_c = (x0 + (x1-x0)/2) / W\n",
        "        y_c = (y0 + (y1-y0)/2) / H\n",
        "        area = w_frac * h_frac\n",
        "        boxes.append(BoundingBox(x_c, y_c, w_frac, h_frac, area, class_id=0))\n",
        "\n",
        "    # (f) merge\n",
        "    boxes = merge_boxes(boxes, iou_thresh, prox_thresh)\n",
        "    # (g) spectral SNR verification\n",
        "    boxes = [b for b in boxes if verify_box(intensity_map, b, snr_min=snr_thresh)]\n",
        "    return boxes\n",
        "\n",
        "# ─── 3) RuleGate & Heuristics ─────────────────────────────────────────────\n",
        "class RuleGate(nn.Module):\n",
        "    def __init__(self, w_align=2.0, w_ent=1.0, bias=-0.5):\n",
        "        super().__init__()\n",
        "        self.w_align, self.w_ent, self.bias = w_align, w_ent, bias\n",
        "    def forward(self, h_sf, h_ued):\n",
        "        align = h_ued[:,1] - h_sf[:,1]\n",
        "        ent   = h_sf[:,2] - h_ued[:,2]\n",
        "        return torch.sigmoid(self.w_align*align + self.w_ent*ent + self.bias)\n",
        "\n",
        "def compute_heuristics(edge_map, gray):\n",
        "    mask = edge_map > 0.2\n",
        "    density = float(mask.mean())\n",
        "    if density>0:\n",
        "        gx = cv2.Sobel(gray,cv2.CV_32F,1,0,3)\n",
        "        gy = cv2.Sobel(gray,cv2.CV_32F,0,1,3)\n",
        "        alignment = float(np.hypot(gx,gy)[mask].mean())\n",
        "    else:\n",
        "        alignment = 0.0\n",
        "    entropy = float(-np.sum(edge_map * np.log2(edge_map + 1e-8)))\n",
        "    bw = mask.astype(np.uint8)\n",
        "    n, labs = cv2.connectedComponents(bw)\n",
        "    ratios = []\n",
        "    for L in range(1, n):\n",
        "        ys, xs = np.where(labs==L)\n",
        "        if ys.size:\n",
        "            ratios.append((np.ptp(xs)+1)/(np.ptp(ys)+1))\n",
        "    linearity = float(np.mean(ratios)) if ratios else 0.0\n",
        "    return [density, alignment, entropy, linearity] if density>=0.01 else [0,0,0,0]\n",
        "\n",
        "# ─── 4) process_file ──────────────────────────────────────────────────────\n",
        "def process_file(job):\n",
        "    (h5_path, channels, gidxs,\n",
        "     base_dir, class_id,\n",
        "     train_set, val_set,\n",
        "     pad_width, file_idx) = job\n",
        "\n",
        "    fb   = bl.Waterfall(h5_path, load_data=True)\n",
        "    data = 10 * np.log10(fb.data.squeeze())  # (ntime, nfreq)\n",
        "    nt, nf = data.shape\n",
        "    nfpc = fb.header.get(\"nfpc\", 1024)\n",
        "\n",
        "    EDGE_MODEL = \"/home/jliang/gbt-rfi/model.yml.gz\"\n",
        "    sf_det     = cv2.ximgproc.createStructuredEdgeDetection(EDGE_MODEL)\n",
        "    gate       = RuleGate()\n",
        "\n",
        "    for ch_idx, gidx in zip(channels, gidxs):\n",
        "        subset = \"train\" if (h5_path,ch_idx) in train_set else \"val\"\n",
        "        img_dir = Path(base_dir)/subset/\"images\"\n",
        "        lbl_dir = Path(base_dir)/subset/\"labels\"\n",
        "        vis_dir = Path(base_dir)/\"visualization\"/subset\n",
        "        for d in (img_dir,lbl_dir,vis_dir):\n",
        "            d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        f0, f1 = ch_idx*nfpc, (ch_idx+1)*nfpc\n",
        "        block  = data[:, f0:f1]\n",
        "        low, high = int(0.15*nfpc), int(0.85*nfpc)\n",
        "        block = block[:, low:high]\n",
        "        H, W  = block.shape\n",
        "        f_start = fb.header['fch1'] + f0 * fb.header['foff']\n",
        "        f_stop  = fb.header['fch1'] + (f1 - 1) * fb.header['foff']\n",
        "\n",
        "\n",
        "        # Remove vertical line artifact\n",
        "        rows, cols = block.shape\n",
        "        vert_means = block.mean(axis=0)\n",
        "        center = np.argmax(vert_means)\n",
        "        left_col = center - 1\n",
        "        right_col = center + 1\n",
        "\n",
        "        if left_col >= 0 and right_col < cols:\n",
        "            block[:, center] = (block[:, left_col] + block[:, right_col]) / 2\n",
        "        elif left_col >= 0:\n",
        "            block[:, center] = block[:, left_col]\n",
        "        elif right_col < cols:\n",
        "            block[:, center] = block[:, right_col]\n",
        "\n",
        "\n",
        "        arr8_orig = ((block - block.min())/(np.ptp(block)+1e-8)*255).astype(np.uint8)\n",
        "        img_orig  = Image.fromarray(arr8_orig).convert(\"RGB\")\n",
        "\n",
        "        norm = (block - block.min())/(np.ptp(block)+1e-8)\n",
        "        img3 = np.stack([norm]*3, axis=-1).astype(np.float32)\n",
        "\n",
        "        sf_map = sf_det.detectEdges(img3).squeeze()\n",
        "        gray8  = arr8_orig\n",
        "\n",
        "        gx = cv2.Sobel(gray8,cv2.CV_32F,1,0,3)\n",
        "        gy = cv2.Sobel(gray8,cv2.CV_32F,0,1,3)\n",
        "        ued_map = np.hypot(gx,gy)\n",
        "        ued_map = cv2.normalize(ued_map,None,0,1,cv2.NORM_MINMAX)\n",
        "\n",
        "        fused = np.zeros_like(sf_map, dtype=np.float32)\n",
        "        segments = slic(img3, n_segments=200, compactness=5)\n",
        "        for seg in np.unique(segments):\n",
        "            m = segments==seg\n",
        "            if m.sum()<50: continue\n",
        "            h_sf  = compute_heuristics(sf_map[m], gray8[m])\n",
        "            h_ued = compute_heuristics(ued_map[m], gray8[m])\n",
        "            w     = gate(torch.tensor([h_sf]), torch.tensor([h_ued])).item()\n",
        "            fused[m] = w*ued_map[m] + (1-w)*sf_map[m]\n",
        "\n",
        "        boxes = generate_yolo_boxes(\n",
        "            fused,\n",
        "            intensity_map=norm,\n",
        "            thresh_factor=3.5,\n",
        "            min_pixels=30,\n",
        "            max_frac=0.6,\n",
        "            kurtosis_thresh=0.7,\n",
        "            solidity_thresh=0.5,\n",
        "            iou_thresh=0.1,\n",
        "            prox_thresh=0.05,\n",
        "            snr_thresh=6\n",
        "        )\n",
        "\n",
        "        k_val = kurtosis(gray8.flatten(), fisher=True, bias=False)\n",
        "        fn = f\"img_{k_val:.3f}_f_{f_start:.3f}_{f_stop:.3f}.png\"\n",
        "\n",
        "        img_orig.save(img_dir/fn)\n",
        "\n",
        "        with open(lbl_dir/fn.replace(\".png\",\".txt\"), \"w\") as f:\n",
        "            for b in boxes:\n",
        "                f.write(f\"{class_id} {b.x_c:.6f} {b.y_c:.6f} {b.w:.6f} {b.h:.6f}\\n\")\n",
        "        if not boxes:\n",
        "            (lbl_dir/\"empty_labels.csv\").open(\"a\").write(f\"{fn},{h5_path},{ch_idx}\\n\")\n",
        "\n",
        "        vis = img_orig.copy()\n",
        "        draw = ImageDraw.Draw(vis)\n",
        "        for b in boxes:\n",
        "            x0 = int((b.x_c - b.w/2)*W)\n",
        "            y0 = int((b.y_c - b.h/2)*H)\n",
        "            x1 = int((b.x_c + b.w/2)*W)\n",
        "            y1 = int((b.y_c + b.h/2)*H)\n",
        "            draw.rectangle([x0,y0,x1,y1], outline=\"red\", width=2)\n",
        "        vis.save(vis_dir/fn)\n",
        "\n",
        "    logging.info(f\"Processed {h5_path}\")\n",
        "\n",
        "# ─── 5) Main Entrypoint ────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    )\n",
        "\n",
        "    tasks    = build_tasks(df)\n",
        "    train_set, val_set = split_tasks(tasks, train_frac=0.8, seed=42)\n",
        "\n",
        "    BASE_DIR    = \"/datax/scratch/jliang/dataset_updated\"\n",
        "    NUM_WORKERS = 8\n",
        "    pad_width   = len(str(len(tasks)-1))\n",
        "\n",
        "    job_list = []\n",
        "    for file_idx, (h5_path, ch_idx) in enumerate(tasks):\n",
        "        job = (\n",
        "            h5_path,\n",
        "            [ch_idx],\n",
        "            [file_idx],\n",
        "            BASE_DIR,\n",
        "            0,\n",
        "            train_set,\n",
        "            val_set,\n",
        "            pad_width,\n",
        "            file_idx\n",
        "        )\n",
        "        job_list.append(job)\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=NUM_WORKERS) as exe:\n",
        "        futures = [exe.submit(process_file, j) for j in job_list]\n",
        "        for _ in tqdm(as_completed(futures), total=len(futures)):\n",
        "            try:\n",
        "                _.result()  # raise any exceptions here\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error in job: {e}\")\n",
        "\n",
        "    logging.info(\"All tasks completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48e59983",
      "metadata": {
        "id": "48e59983"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import cv2\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from skimage.segmentation import slic\n",
        "# from scipy import ndimage\n",
        "\n",
        "# # ─── 1) HEURISTIC COMPUTATION ─────────────────────────────────────────\n",
        "\n",
        "# def compute_heuristics(edge_map, gray_image):\n",
        "#     # edge_map: 2D float32 [0,1], gray_image: 2D uint8\n",
        "#     # 1) gradient magnitude\n",
        "#     gx = cv2.Sobel(gray_image, cv2.CV_32F, 1, 0, ksize=3)\n",
        "#     gy = cv2.Sobel(gray_image, cv2.CV_32F, 0, 1, ksize=3)\n",
        "#     grad = np.hypot(gx, gy)\n",
        "\n",
        "#     # 2) statistics\n",
        "#     density   = np.mean(edge_map > 0.2)\n",
        "#     align     = np.mean(grad[edge_map > 0.2]) if density>0 else 0.0\n",
        "#     entropy   = -np.sum(edge_map * np.log2(edge_map + 1e-8))\n",
        "#     # 3) linearity via eccentricity of components\n",
        "#     bw   = edge_map > 0.2\n",
        "#     lbl, n = ndimage.label(bw)\n",
        "#     props = ndimage.find_objects(lbl)\n",
        "#     eccs = []\n",
        "#     for i, sl in enumerate(props, start=1):\n",
        "#         region = (lbl[sl] == i).astype(np.uint8)\n",
        "#         # approximate by second moments → skip details here\n",
        "#         eccs.append(region.sum()>0 and region.sum() / (region.shape[0]*region.shape[1]))\n",
        "#     linearity = float(np.mean(eccs)) if eccs else 0.0\n",
        "\n",
        "#     if density < 0.01:\n",
        "#         return [0.0, 0.0, 0.0, 0.0]\n",
        "#     return [density, align, entropy, linearity]\n",
        "\n",
        "\n",
        "# # ─── 2) RULE-BASED GATE MODULE ──────────────────────────────────────────\n",
        "\n",
        "# class RuleGate(nn.Module):\n",
        "#     def __init__(self, w_align=1.0, w_ent=1.0, bias=0.0):\n",
        "#         super().__init__()\n",
        "#         # wrap as parameters if you want to tune via backprop:\n",
        "#         self.w_align = nn.Parameter(torch.tensor(w_align))\n",
        "#         self.w_ent   = nn.Parameter(torch.tensor(w_ent))\n",
        "#         self.bias    = nn.Parameter(torch.tensor(bias))\n",
        "\n",
        "#     def forward(self, h_sf, h_ued):\n",
        "#         # h_*: [B,4] tensors on CUDA\n",
        "#         align = h_ued[:,1] - h_sf[:,1]      # favors UED if >0\n",
        "#         ent   = h_sf[:,2] - h_ued[:,2]      # favors UED if >0\n",
        "#         score  = self.w_align * align + self.w_ent * ent + self.bias\n",
        "#         return torch.sigmoid(score)         # [B] weights for UED\n",
        "\n",
        "\n",
        "# class MoEBlock(nn.Module):\n",
        "#     def __init__(self, gate: RuleGate):\n",
        "#         super().__init__()\n",
        "#         self.gate = gate\n",
        "\n",
        "#     def forward(self, sf_map, ued_map, h_sf, h_ued):\n",
        "#         \"\"\"\n",
        "#         sf_map, ued_map: [B,H,W] floats on CUDA\n",
        "#         h_sf, h_ued:        [B,4] heuristic tensors on CUDA\n",
        "#         \"\"\"\n",
        "#         p = self.gate(h_sf, h_ued).view(-1,1,1)  # [B,1,1]\n",
        "#         return p * ued_map + (1 - p) * sf_map\n",
        "\n",
        "\n",
        "# # ─── 3) FULL PROCESSING LOOP ───────────────────────────────────────────\n",
        "\n",
        "# def process_image_patch(patch, sf_detector, gate_block, device='cuda'):\n",
        "#     \"\"\"\n",
        "#     patch: HxWx3 BGR uint8\n",
        "#     sf_detector: cv2.ximgproc StructuredEdgeDetection\n",
        "#     gate_block:   instance of MoEBlock on device\n",
        "#     \"\"\"\n",
        "#     # 1) compute both edge maps\n",
        "#     gray    = cv2.cvtColor(patch, cv2.COLOR_BGR2GRAY)\n",
        "#     sf_edges= sf_detector.detectEdges(np.float32(patch)/255.0).squeeze()\n",
        "#     gx = cv2.Sobel(gray, cv2.CV_32F, 1,0,3)\n",
        "#     gy = cv2.Sobel(gray, cv2.CV_32F, 0,1,3)\n",
        "#     ued_edges = np.hypot(gx, gy)\n",
        "#     ued_edges = (ued_edges/ued_edges.max()).astype(np.float32)\n",
        "\n",
        "#     # 2) superpixel segmentation\n",
        "#     segments = slic(patch, n_segments=100, compactness=10)\n",
        "\n",
        "#     # 3) gather heuristics and masks\n",
        "#     feats_sf, feats_ued, masks = [], [], []\n",
        "#     for seg in np.unique(segments):\n",
        "#         mask = (segments==seg)\n",
        "#         if mask.sum()<50: continue\n",
        "#         hsf = compute_heuristics(sf_edges*mask, gray)\n",
        "#         hud = compute_heuristics(ued_edges*mask, gray)\n",
        "#         if not any(hsf) and not any(hud): continue\n",
        "#         feats_sf.append(hsf)\n",
        "#         feats_ued.append(hud)\n",
        "#         masks.append(mask)\n",
        "\n",
        "#     if not masks:\n",
        "#         return np.zeros_like(sf_edges, np.float32)\n",
        "\n",
        "#     # 4) move to GPU\n",
        "#     h_sf = torch.tensor(feats_sf, device=device)\n",
        "#     h_ued= torch.tensor(feats_ued, device=device)\n",
        "#     sf_m = torch.tensor(np.stack([sf_edges[m] for m in masks]), device=device)\n",
        "#     ue_m = torch.tensor(np.stack([ued_edges[m] for m in masks]), device=device)\n",
        "\n",
        "#     # 5) fuse with gate\n",
        "#     fused_masks = gate_block(sf_m, ue_m, h_sf, h_ued)  # [N_pixels]\n",
        "#     fused = np.zeros_like(sf_edges, np.float32)\n",
        "#     for p, m in zip(fused_masks.cpu().numpy(), masks):\n",
        "#         fused[m] = p\n",
        "\n",
        "#     # 6) generate YOLO boxes\n",
        "#     return fused\n",
        "\n",
        "\n",
        "# # ─── 4) USAGE ─────────────────────────────────────────────────────────\n",
        "\n",
        "# # Initialize:\n",
        "# EDGE_MODEL_PATH = \"model.yml\"  # your SF model\n",
        "# sf_detector = cv2.ximgproc.createStructuredEdgeDetection(EDGE_MODEL_PATH)\n",
        "# gate        = RuleGate(w_align=2.0, w_ent=1.0, bias=-0.5).cuda()\n",
        "# moe_block   = MoEBlock(gate).cuda()\n",
        "\n",
        "# # For each patch:\n",
        "# # fused_map = process_image_patch(patch, sf_detector, moe_block)\n",
        "# # then threshold fused_map, label, convert to YOLO (cx,cy,w,h) as before.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bde136f",
      "metadata": {
        "id": "8bde136f"
      },
      "outputs": [],
      "source": [
        "# def compute_heuristics(image, edge_map):\n",
        "#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "#     gradient = sobel(gray)\n",
        "#     edge_density = np.mean(edge_map > 0.2)\n",
        "#     edge_gradient_alignment = np.mean(gradient[edge_map > 0.2])\n",
        "#     edge_entropy = -np.sum(edge_map * np.log2(edge_map + 1e-8))\n",
        "#     lin = linearity_score(edge_map)\n",
        "#     if edge_density < 0.01:\n",
        "#         return [0.0, 0.0, 0.0, 0.0]\n",
        "#     return [edge_density, edge_gradient_alignment, edge_entropy, lin]\n",
        "\n",
        "# def pseudo_labels(image, sf_edges, ued_edges):\n",
        "#     h_sf  = compute_heuristics(image, sf_edges)\n",
        "#     h_ued = compute_heuristics(image, ued_edges)\n",
        "#     # if UED better alignment & lower entropy, choose UED\n",
        "#     if h_ued[1] > h_sf[1] and h_ued[2] < h_sf[2]:\n",
        "#         return 0\n",
        "#     else:\n",
        "#         return 1\n",
        "\n",
        "# # PyTorch gating network (input_dim=8 for 4 heuristics each)\n",
        "# class GatingNet(nn.Module):\n",
        "#     def __init__(self, input_dim=8, hidden=16):\n",
        "#         super().__init__()\n",
        "#         self.net = nn.Sequential(\n",
        "#             nn.Linear(input_dim, hidden),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(hidden, 2)\n",
        "#         )\n",
        "#     def forward(self, x):\n",
        "#         return self.net(x)\n",
        "\n",
        "# # Train gating model in PyTorch\n",
        "# def train_gating_model_torch(images, sf_edge_maps, ued_edge_maps,\n",
        "#                              batch_size=64, epochs=10, lr=1e-3,\n",
        "#                              device='cuda'):\n",
        "#     # 1) Collect features & labels\n",
        "#     feats, labels = [], []\n",
        "#     for img, sf, ued in zip(images, sf_edge_maps, ued_edge_maps):\n",
        "#         segments = slic(img_as_float(img), n_segments=100, compactness=10)\n",
        "#         for seg_val in np.unique(segments):\n",
        "#             mask = (segments == seg_val)\n",
        "#             if mask.sum() < 50:\n",
        "#                 continue\n",
        "#             sf_patch, ued_patch = sf * mask, ued * mask\n",
        "#             if sf_patch.mean() + ued_patch.mean() < 0.01:\n",
        "#                 continue\n",
        "#             h_sf  = compute_heuristics(img, sf_patch)\n",
        "#             h_ued = compute_heuristics(img, ued_patch)\n",
        "#             feats.append(np.array(h_sf + h_ued, dtype=np.float32))\n",
        "#             labels.append(pseudo_labels(img, sf_patch, ued_patch))\n",
        "#     if len(set(labels)) < 2:\n",
        "#         return None, None, None\n",
        "\n",
        "#     # 2) Build tensors & standardize\n",
        "#     X = torch.tensor(feats)  # (N, 8)\n",
        "#     y = torch.tensor(labels, dtype=torch.long)  # (N,)\n",
        "#     mean, std = X.mean(dim=0, keepdim=True), X.std(dim=0, keepdim=True) + 1e-6\n",
        "#     X = (X - mean) / std\n",
        "\n",
        "#     # 3) DataLoader\n",
        "#     dataset = TensorDataset(X, y)\n",
        "#     loader  = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "#     # 4) Model, loss, optimizer\n",
        "#     model = GatingNet(input_dim=X.shape[1]).to(device)\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#     # 5) Training loop\n",
        "#     model.train()\n",
        "#     for epoch in range(epochs):\n",
        "#         total_loss = 0.0\n",
        "#         for xb, yb in loader:\n",
        "#             xb, yb = xb.to(device), yb.to(device)\n",
        "#             logits = model(xb)\n",
        "#             loss   = criterion(logits, yb)\n",
        "#             optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "#             total_loss += loss.item()\n",
        "#         print(f\"Epoch {epoch+1}/{epochs} - loss: {total_loss/len(loader):.4f}\")\n",
        "\n",
        "#     return model, mean.to(device), std.to(device)\n",
        "\n",
        "# # Apply gating model to fuse edges\n",
        "# def apply_gating_model_torch(image, sf_edges, ued_edges,\n",
        "#                              model, mean, std, device='cuda'):\n",
        "#     segments = slic(img_as_float(image), n_segments=100, compactness=10)\n",
        "#     feats, masks = [], []\n",
        "#     for seg_val in np.unique(segments):\n",
        "#         mask = (segments == seg_val)\n",
        "#         if mask.sum() < 50:\n",
        "#             continue\n",
        "#         sf_patch, ued_patch = sf_edges * mask, ued_edges * mask\n",
        "#         if sf_patch.mean() + ued_patch.mean() < 0.01:\n",
        "#             continue\n",
        "#         h_sf, h_ued = compute_heuristics(image, sf_patch), compute_heuristics(image, ued_patch)\n",
        "#         f = np.array(h_sf + h_ued, dtype=np.float32)\n",
        "#         if not np.all(np.isfinite(f)) or np.allclose(f, 0, atol=1e-3):\n",
        "#             continue\n",
        "#         feats.append(f)\n",
        "#         masks.append(mask)\n",
        "\n",
        "#     if not feats:\n",
        "#         return np.zeros_like(sf_edges, dtype=float)\n",
        "\n",
        "#     X = torch.tensor(feats).to(device)\n",
        "#     X = (X - mean) / std\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         probs = torch.softmax(model(X), dim=1)[:,1].cpu().numpy()\n",
        "\n",
        "#     gated = np.zeros_like(sf_edges, dtype=float)\n",
        "#     for p, mask in zip(probs, masks):\n",
        "#         gated[mask] = p * sf_edges[mask] + (1 - p) * ued_edges[mask]\n",
        "#     return gated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c822d4e5",
      "metadata": {
        "id": "c822d4e5"
      },
      "outputs": [],
      "source": [
        "# def edge_detection(gray, factor=6, dilation=(3, 3)):\n",
        "#         # STEP 1: apply power threshold to suppress background\n",
        "#         median = np.median(gray)\n",
        "#         mad = np.median(np.abs(gray - median))\n",
        "#         power_mask = gray > (median + factor * mad)\n",
        "\n",
        "#         # STEP 2: edge detection on filtered signal only\n",
        "#         grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
        "#         grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
        "#         magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
        "#         magnitude *= power_mask  # mask out noise\n",
        "#         magnitude = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "\n",
        "#         # STEP 3: binary threshold + dilation\n",
        "#         binary = magnitude > 50  # you can tune this\n",
        "#         binary = scipy.ndimage.binary_dilation(binary, structure=np.ones(dilation))\n",
        "#         labeled, n_objs = scipy.ndimage.label(binary)\n",
        "#         slices = scipy.ndimage.find_objects(labeled)\n",
        "#         return slices\n",
        "\n",
        "# EDGE_MODEL_PATH = '/home/jliang/gbt-rfi/model.yml.gz'\n",
        "# EDGE_DETECTOR = cv2.ximgproc.createStructuredEdgeDetection(EDGE_MODEL_PATH)\n",
        "\n",
        "# def detect_edges(spectrogram):\n",
        "#     img = (spectrogram - spectrogram.min()) / (spectrogram.ptp() + 1e-6)\n",
        "#     img_3ch = cv2.merge([img.astype(np.float32)] * 3)\n",
        "#     return EDGE_DETECTOR.detectEdges(img_3ch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06fea6ff",
      "metadata": {
        "id": "06fea6ff"
      },
      "outputs": [],
      "source": [
        "# def extract_one_sample(h5_path, ch, edge_model_path='/home/jliang/gbt-rfi/model.yml.gz'):\n",
        "#     try:\n",
        "#         EDGE_DETECTOR = cv2.ximgproc.createStructuredEdgeDetection(edge_model_path)\n",
        "#         fb = bl.Waterfall(h5_path, load_data=True)\n",
        "#         data = 10 * np.log10(fb.data.squeeze())\n",
        "\n",
        "#         nfpc = fb.header.get(\"nfpc\", 1024)\n",
        "#         f0, f1 = ch * nfpc, (ch + 1) * nfpc\n",
        "#         block = data[:, f0:f1]\n",
        "\n",
        "#         n_cols = block.shape[1]\n",
        "#         low, high = int(0.1 * n_cols), int(0.9 * n_cols)\n",
        "#         block_middle80 = block[:, low:high]\n",
        "\n",
        "#         vert_means = block_middle80.mean(axis=0)\n",
        "#         center = np.argmax(vert_means)\n",
        "#         left_col, right_col = center - 1, center + 1\n",
        "#         if left_col >= 0 and right_col < block_middle80.shape[1]:\n",
        "#             block_middle80[:, center] = (block_middle80[:, left_col] + block_middle80[:, right_col]) / 2\n",
        "#         elif left_col >= 0:\n",
        "#             block_middle80[:, center] = block_middle80[:, left_col]\n",
        "#         elif right_col < block_middle80.shape[1]:\n",
        "#             block_middle80[:, center] = block_middle80[:, right_col]\n",
        "\n",
        "#         img_norm = (block_middle80 - block_middle80.min()) / (block_middle80.ptp() + 1e-6)\n",
        "#         img_rgb = cv2.merge([img_norm.astype(np.float32)] * 3)\n",
        "\n",
        "#         sf_edges = EDGE_DETECTOR.detectEdges(img_rgb)\n",
        "\n",
        "#         gray = (255 * img_norm).astype(np.uint8)\n",
        "#         grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
        "#         grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
        "#         magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
        "#         magnitude = cv2.normalize(magnitude, None, 0, 1, cv2.NORM_MINMAX)\n",
        "\n",
        "#         sf_mean = np.mean(sf_edges)\n",
        "#         ued_mean = np.mean(magnitude)\n",
        "\n",
        "#         if sf_mean + ued_mean < 0.005:  # threshold to skip empty or background-only blocks\n",
        "#             return None  # skip empty or background-only blocks\n",
        "\n",
        "\n",
        "#         return (img_rgb, sf_edges, magnitude)\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error processing {h5_path} ch {ch}: {e}\")\n",
        "#         return None\n",
        "\n",
        "# def extract_samples_batched(df, total_samples=40000, batch_size=32, num_workers=4):\n",
        "#     all_tasks = []\n",
        "#     for _, row in df.iterrows():\n",
        "#         h5 = row[\".h5 path\"]\n",
        "#         fb = bl.Waterfall(h5, load_data=False)\n",
        "#         nfreq = fb.header.get(\"nchans\")\n",
        "#         nfpc = fb.header.get(\"nfpc\", 1024)\n",
        "#         for ch in range(nfreq // nfpc):\n",
        "#             all_tasks.append((h5, ch))\n",
        "\n",
        "#     shuffle(all_tasks)\n",
        "#     selected_tasks = all_tasks[:total_samples]\n",
        "\n",
        "#     train_images, train_sf_edges, train_ued_edges = [], [], []\n",
        "#     for i in tqdm(range(0, total_samples, batch_size)):\n",
        "#         batch = selected_tasks[i:i+batch_size]\n",
        "#         with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
        "#             futures = [executor.submit(extract_one_sample, h5, ch) for h5, ch in batch]\n",
        "#             for future in as_completed(futures):\n",
        "#                 result = future.result()\n",
        "#                 if result:\n",
        "#                     img_rgb, sf_edge, ued_edge = result\n",
        "#                     train_images.append(img_rgb)\n",
        "#                     train_sf_edges.append(sf_edge)\n",
        "#                     train_ued_edges.append(ued_edge)\n",
        "\n",
        "#     return train_images, train_sf_edges, train_ued_edges\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50421249",
      "metadata": {
        "id": "50421249"
      },
      "outputs": [],
      "source": [
        "# # 4) SET UP LOGGING\n",
        "# logging.basicConfig(\n",
        "#     level=logging.INFO,\n",
        "#     format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "#     datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "# )\n",
        "\n",
        "# from gating_model import train_gating_model_torch\n",
        "\n",
        "# def build_tasks(df):\n",
        "#     \"\"\"\n",
        "#     Build a flat list of (h5_path, channel_idx) from your DataFrame,\n",
        "#     skipping any channels that fall into known notch filter frequency ranges.\n",
        "#     \"\"\"\n",
        "#     GBT_NOTCH_FILTERS = {\n",
        "#         \"L\": [(1200, 1340)],\n",
        "#         \"S\": [(2300, 2360)],\n",
        "#     }\n",
        "\n",
        "#     tasks = []\n",
        "\n",
        "#     for _, row in df.iterrows():\n",
        "#         h5 = row[\".h5 path\"]\n",
        "#         band = row[\"Band\"]  # e.g., 'L', 'S', etc.\n",
        "#         fb = bl.Waterfall(h5, load_data=False)\n",
        "#         nfreq = fb.header.get(\"nchans\")\n",
        "#         nfpc = fb.header.get(\"nfpc\", 1024)\n",
        "#         fch1 = fb.header[\"fch1\"]\n",
        "#         foff = fb.header[\"foff\"]\n",
        "#         n_coarse = nfreq // nfpc\n",
        "\n",
        "#         for ch in range(n_coarse):\n",
        "#             f0 = fch1 + ch * nfpc * foff\n",
        "#             f1 = fch1 + (ch + 1) * nfpc * foff\n",
        "#             f_min, f_max = sorted([f0, f1])\n",
        "\n",
        "#             # Check against notch filter exclusion ranges\n",
        "#             skip = False\n",
        "#             if band in GBT_NOTCH_FILTERS:\n",
        "#                 for lo, hi in GBT_NOTCH_FILTERS[band]:\n",
        "#                     if lo <= f_min <= hi or lo <= f_max <= hi:\n",
        "#                         skip = True\n",
        "#                         break\n",
        "#             if not skip:\n",
        "#                 tasks.append((h5, ch))\n",
        "\n",
        "#     return tasks\n",
        "\n",
        "\n",
        "# def split_tasks(tasks, train_frac=0.8, seed=42):\n",
        "#     \"\"\"\n",
        "#     Shuffle & split the flat task list into train vs. val sets.\n",
        "#     Returns two sets of (h5_path, channel_idx).\n",
        "#     \"\"\"\n",
        "#     random.seed(seed)\n",
        "#     shuffled = tasks.copy()\n",
        "#     random.shuffle(shuffled)\n",
        "#     cut = int(train_frac * len(shuffled))\n",
        "#     train = set(shuffled[:cut])\n",
        "#     val   = set(shuffled[cut:])\n",
        "#     return train, val\n",
        "\n",
        "# def process_file(job):\n",
        "#     \"\"\"\n",
        "#     job is a tuple:\n",
        "#       (h5_path, channels, global_indices,\n",
        "#        base_dir, factor, dilation,\n",
        "#        class_id, train_set, val_set,\n",
        "#        pad_width, model, mean, std)\n",
        "#     \"\"\"\n",
        "#     (h5_path, channels, global_indices,\n",
        "#      base_dir, factor, dilation,\n",
        "#      class_id, train_set, val_set,\n",
        "#      pad_width, model, mean, std) = job\n",
        "\n",
        "#     # 1) Prepare device & trained gate\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     model = model.to(device)\n",
        "#     model.eval()\n",
        "\n",
        "#     # 2) Load Structured Forest detector once\n",
        "#     EDGE_MODEL_PATH = \"/home/jliang/gbt-rfi/model.yml.gz\"\n",
        "#     sf_detector = cv2.ximgproc.createStructuredEdgeDetection(EDGE_MODEL_PATH)\n",
        "\n",
        "#     # 3) Load the .h5 file\n",
        "#     fb   = bl.Waterfall(h5_path, load_data=True)\n",
        "#     data = 10 * np.log10(fb.data.squeeze())   # (ntime, nfreq)\n",
        "\n",
        "#     # 4) Process each coarse channel\n",
        "#     for ch_idx, gidx in zip(channels, global_indices):\n",
        "#         # decide train vs val\n",
        "#         subset = \"train\" if (h5_path, ch_idx) in train_set else \"val\"\n",
        "\n",
        "#         # make sure directories exist\n",
        "#         img_dir = Path(base_dir)/subset/\"images\"\n",
        "#         lbl_dir = Path(base_dir)/subset/\"labels\"\n",
        "#         vis_dir = Path(base_dir)/\"visualization\"/subset\n",
        "#         for d in (img_dir, lbl_dir, vis_dir):\n",
        "#             d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#         # extract the 80% middle of the block\n",
        "#         cw = fb.header.get(\"nfpc\", 1024)\n",
        "#         f0, f1 = ch_idx*cw, (ch_idx+1)*cw\n",
        "#         block = data[:, f0:f1]\n",
        "#         low, high = int(0.1*cw), int(0.9*cw)\n",
        "#         block = block[:, low:high]\n",
        "\n",
        "#         # remove single‐column artifact\n",
        "#         col_means = block.mean(axis=0)\n",
        "#         c = int(np.argmax(col_means))\n",
        "#         if 0 < c < block.shape[1]-1:\n",
        "#             block[:,c] = 0.5*(block[:,c-1] + block[:,c+1])\n",
        "\n",
        "#         # normalize & build RGB input for SF\n",
        "#         norm = (block - block.min())/(block.ptp()+1e-6)\n",
        "#         img_rgb = cv2.merge([norm.astype(np.float32)]*3)\n",
        "\n",
        "#         # compute the two expert edge‐maps\n",
        "#         sf_edge_map  = sf_detector.detectEdges(img_rgb).squeeze()\n",
        "#         gray_uint8  = (255*norm).astype(np.uint8)\n",
        "#         gx = cv2.Sobel(gray_uint8, cv2.CV_32F, 1, 0, ksize=3)\n",
        "#         gy = cv2.Sobel(gray_uint8, cv2.CV_32F, 0, 1, ksize=3)\n",
        "#         ued_edge_map = np.hypot(gx, gy)\n",
        "#         ued_edge_map = cv2.normalize(ued_edge_map, None, 0.0, 1.0, cv2.NORM_MINMAX)\n",
        "\n",
        "#         # 5) Fuse using the PyTorch gate\n",
        "#         fused_edges = apply_gating_model_torch(\n",
        "#             img_rgb, sf_edge_map, ued_edge_map,\n",
        "#             model, mean, std,\n",
        "#             device=device\n",
        "#         )\n",
        "\n",
        "#         # 6) Generate & save YOLO boxes\n",
        "#         boxes = generate_yolo_boxes(fused_edges, threshold=0.1)\n",
        "\n",
        "#         # Dimensions for filtering\n",
        "#         h_img, w_img = block.shape\n",
        "#         min_size = 3                   # px\n",
        "#         full_image_threshold = 0.95    # normalized area\n",
        "\n",
        "#         # Filter\n",
        "#         final_boxes = []\n",
        "#         for x_c, y_c, w_n, h_n, area in boxes:\n",
        "#             if area > full_image_threshold:\n",
        "#                 continue\n",
        "#             if (w_n * w_img) < min_size or (h_n * h_img) < min_size:\n",
        "#                 continue\n",
        "#             final_boxes.append((x_c, y_c, w_n, h_n))\n",
        "\n",
        "#         # Calculate frequency range of this coarse channel\n",
        "#         f_start = fb.header['fch1'] + f0 * fb.header['foff']\n",
        "#         f_stop  = fb.header['fch1'] + (f1 - 1) * fb.header['foff']\n",
        "\n",
        "#         # Build filename\n",
        "#         fn = f\"img_{gidx:0{pad_width}d}_f_{f_start:.4f}_{f_stop:.4f}.png\"\n",
        "#         img_path = img_dir / fn\n",
        "#         txt_path = lbl_dir / fn.replace(\".png\", \".txt\")\n",
        "\n",
        "#         # Save image\n",
        "#         arr8 = (255 * (block - block.min()) / (block.ptp() + 1e-6)).astype(np.uint8)\n",
        "#         img = Image.fromarray(np.stack([arr8]*3, axis=-1))\n",
        "#         img.save(img_path)\n",
        "\n",
        "#         # Write YOLO labels\n",
        "#         with open(txt_path, \"w\") as f:\n",
        "#             for x_c, y_c, w_n, h_n in final_boxes:\n",
        "#                 f.write(f\"{class_id} {x_c:.6f} {y_c:.6f} {w_n:.6f} {h_n:.6f}\\n\")\n",
        "\n",
        "#         # If empty, log it\n",
        "#         if not final_boxes:\n",
        "#             (lbl_dir/\"empty_labels.csv\").open(\"a\").write(f\"{fn},{h5_path},{ch_idx}\\n\")\n",
        "\n",
        "#         # Draw & save visualization\n",
        "#         vis_img = img.convert(\"RGB\")\n",
        "#         draw    = ImageDraw.Draw(vis_img)\n",
        "#         for x_c, y_c, w_n, h_n in final_boxes:\n",
        "#             xc, yc = x_c*w_img, y_c*h_img\n",
        "#             bw, bh = w_n*w_img, h_n*h_img\n",
        "#             x0, y0 = int(xc - bw/2), int(yc - bh/2)\n",
        "#             x1, y1 = int(xc + bw/2), int(yc + bh/2)\n",
        "#             draw.rectangle([x0, y0, x1, y1], outline=\"red\", width=2)\n",
        "#         vis_img.save(vis_dir/fn)\n",
        "\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # === your settings ===\n",
        "#     BASE_DIR             = \"/datax/scratch/jliang/dataset_moe\"\n",
        "#     FACTOR              = 6\n",
        "#     DILATION = (30, 30)\n",
        "#     CLASS_ID             = 0\n",
        "#     TRAIN_FRAC           = 0.8\n",
        "#     SEED                 = 42\n",
        "#     NUM_WORKERS          = 2\n",
        "\n",
        "#     # --- assume you already have a DataFrame `df` with at least 'h5_path' and optionally 'nchans' ---\n",
        "#     # df = pd.read_csv(...)  # or however you built it\n",
        "\n",
        "#     # 1) Build & split tasks\n",
        "#     tasks = build_tasks(df)\n",
        "#     train_set, val_set = split_tasks(tasks, TRAIN_FRAC, SEED)\n",
        "\n",
        "#     # 2) compute zero-pad width from total images\n",
        "#     pad_width = len(str(len(tasks) - 1))\n",
        "\n",
        "#     # 3) regroup tasks by file to load each .h5 only once\n",
        "#     jobs = {}\n",
        "#     for gidx, (h5, ch) in enumerate(tasks):\n",
        "#         jobs.setdefault(h5, {\"chs\": [], \"gidxs\": []})\n",
        "#         jobs[h5][\"chs\"].append(ch)\n",
        "#         jobs[h5][\"gidxs\"].append(gidx)\n",
        "\n",
        "#     # 4) prepare job‐tuples\n",
        "#     job_list = []\n",
        "#     for h5, info in jobs.items():\n",
        "#         job_list.append((\n",
        "#             h5,\n",
        "#             info[\"chs\"],\n",
        "#             info[\"gidxs\"],\n",
        "#             BASE_DIR,\n",
        "#             FACTOR,\n",
        "#             DILATION,\n",
        "#             CLASS_ID,\n",
        "#             train_set,\n",
        "#             val_set,\n",
        "#             pad_width\n",
        "#         ))\n",
        "\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#     # train_images, train_sf_edges, train_ued_edges = extract_samples_batched(\n",
        "#     #     df, total_samples=40000, num_workers=2\n",
        "#     # )\n",
        "\n",
        "#     # model, mean, std = train_gating_model_torch(\n",
        "#     #     train_images,\n",
        "#     #     train_sf_edges,\n",
        "#     #     train_ued_edges,\n",
        "#     #     batch_size=64,\n",
        "#     #     epochs=10,\n",
        "#     #     lr=1e-3,\n",
        "#     #     device=device\n",
        "#     # )\n",
        "\n",
        "#     # if model is None:\n",
        "#     #     logging.warning(\"Skipping inference because gate net couldn't train.\")\n",
        "#     #     exit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae743955",
      "metadata": {
        "id": "ae743955"
      },
      "outputs": [],
      "source": [
        "# import h5py, numpy as np, torch, logging\n",
        "# from torch.utils.data import IterableDataset, DataLoader\n",
        "# import cv2\n",
        "# from skimage.segmentation import slic\n",
        "# from yolo_moe_pytorch_channel import (\n",
        "#     apply_gating_model_torch,\n",
        "#     generate_yolo_boxes,\n",
        "#     compute_heuristics,\n",
        "#     pseudo_labels\n",
        "# )\n",
        "# from PIL import Image, ImageDraw\n",
        "\n",
        "# # 1) Streaming dataset\n",
        "# class H5ChannelDataset(IterableDataset):\n",
        "#     def __init__(self, df, nfpc=1024):\n",
        "#         self.paths = df[\".h5 path\"].tolist()\n",
        "#         self.nfpc  = nfpc\n",
        "#     def __iter__(self):\n",
        "#         for p in self.paths:\n",
        "#             with h5py.File(p, \"r\") as f:\n",
        "#                 d = f[\"data\"]\n",
        "#                 n_coarse = d.shape[1] // self.nfpc\n",
        "#                 for ch in range(n_coarse):\n",
        "#                     yield d[:, ch*self.nfpc:(ch+1)*self.nfpc].astype(np.float32)\n",
        "\n",
        "# # 2) Pre- and post-processing helpers\n",
        "# EDGE_MODEL_PATH = \"/home/jliang/gbt-rfi/model.yml.gz\"\n",
        "# sf_detector    = cv2.ximgproc.createStructuredEdgeDetection(EDGE_MODEL_PATH)\n",
        "\n",
        "# def preprocess(block):\n",
        "#     norm = (block - block.min())/(block.ptp()+1e-6)\n",
        "#     img_rgb = np.stack([norm]*3, axis=-1).astype(np.float32)\n",
        "#     sf_map   = sf_detector.detectEdges(img_rgb).squeeze()\n",
        "#     gray8    = (255*norm).astype(np.uint8)\n",
        "#     gx = cv2.Sobel(gray8, cv2.CV_32F, 1,0,3)\n",
        "#     gy = cv2.Sobel(gray8, cv2.CV_32F, 0,1,3)\n",
        "#     ued_map = np.hypot(gx, gy)\n",
        "#     ued_map = cv2.normalize(ued_map, None, 0.0, 1.0, cv2.NORM_MINMAX)\n",
        "#     return img_rgb, sf_map, ued_map\n",
        "\n",
        "# def segment_patches(img, sf, ued):\n",
        "#     segs = slic(img, n_segments=100, compactness=10)\n",
        "#     for v in np.unique(segs):\n",
        "#         m = segs==v\n",
        "#         if m.sum()<50: continue\n",
        "#         sf_p, ued_p = sf*m, ued*m\n",
        "#         if sf_p.mean()+ued_p.mean()<0.01: continue\n",
        "#         yield sf_p, ued_p\n",
        "\n",
        "# # 3) Collate: build feature / label tensors\n",
        "# def collate_fn(batch):\n",
        "#     feats, labs = [], []\n",
        "#     for block in batch:\n",
        "#         img, sf, ued = preprocess(block)\n",
        "#         for sf_p, ued_p in segment_patches(img, sf, ued):\n",
        "#             h_sf = compute_heuristics(sf_p, img)\n",
        "#             h_ued= compute_heuristics(ued_p, img)\n",
        "#             feats.append(h_sf + h_ued)\n",
        "#             labs.append(pseudo_labels(img, sf_p, ued_p))\n",
        "#     if not feats: return None\n",
        "#     return torch.tensor(feats), torch.tensor(labs)\n",
        "\n",
        "# # 4) Training\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# dataset = H5ChannelDataset(df)\n",
        "# loader  = DataLoader(dataset,\n",
        "#                      batch_size=4,\n",
        "#                      num_workers=4,\n",
        "#                      persistent_workers=True,\n",
        "#                      collate_fn=collate_fn)\n",
        "\n",
        "# from your_module import GatingNet  # the BatchNorm version\n",
        "# net   = GatingNet().to(device)\n",
        "# opt   = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
        "# crit  = nn.CrossEntropyLoss()\n",
        "\n",
        "# for epoch in range(10):\n",
        "#     for batch in loader:\n",
        "#         if batch is None: continue\n",
        "#         X, y = batch\n",
        "#         X, y = X.to(device), y.to(device)\n",
        "#         logits = net(X)\n",
        "#         loss   = crit(logits, y)\n",
        "#         opt.zero_grad()\n",
        "#         loss.backward()\n",
        "#         opt.step()\n",
        "#     print(f\"Epoch {epoch+1}/10 done\")\n",
        "\n",
        "# # 5) Build job_list as before, but append (net) instead of (mean,std)\n",
        "# # and in process_file call apply_gating_model_torch(img, sf, ued, net, device=...)\n",
        "\n",
        "# # 6) Inference uses your existing process_file, no further change needed.\n",
        "\n",
        "\n",
        "# # ─── 3) Build job list for inference ────────────────────────────────────\n",
        "\n",
        "# # your existing build_tasks(), split_tasks(), jobs grouping...\n",
        "# tasks    = build_tasks(df)\n",
        "# train_set, val_set = split_tasks(tasks, TRAIN_FRAC, SEED)\n",
        "# pad_width = len(str(len(tasks)))\n",
        "\n",
        "# jobs = {}\n",
        "# for gidx,(h5,ch) in enumerate(tasks):\n",
        "#     jobs.setdefault(h5, {\"chs\":[], \"gidxs\":[]})\n",
        "#     jobs[h5][\"chs\"].append(ch)\n",
        "#     jobs[h5][\"gidxs\"].append(gidx)\n",
        "\n",
        "# job_list = []\n",
        "# for h5,info in jobs.items():\n",
        "#     job_list.append((h5,\n",
        "#                      info[\"chs\"],\n",
        "#                      info[\"gidxs\"],\n",
        "#                      BASE_DIR,\n",
        "#                      FACTOR,\n",
        "#                      DILATION,\n",
        "#                      CLASS_ID,\n",
        "#                      train_set,\n",
        "#                      val_set,\n",
        "#                      pad_width,\n",
        "#                      gate_net, mean, std))\n",
        "\n",
        "# # ─── 4) Run inference in parallel ───────────────────────────────────────\n",
        "# from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "# with ProcessPoolExecutor(max_workers=NUM_WORKERS) as exe:\n",
        "#     for _ in exe.map(process_file, job_list):\n",
        "#         pass\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
