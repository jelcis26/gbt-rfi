{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9774b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install SAM (and dependencies)\n",
    "%pip install torch torchvision\n",
    "#git clone https://github.com/facebookresearch/segment-anything.git\n",
    "#cd segment-anything\n",
    "#%pip install -e .             # installs `segment_anything`\n",
    "\n",
    "# 2. (Optional) Also install albumentations for data augmentation\n",
    "%pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c67f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image, ImageDraw\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "from segment_anything import sam_model_registry\n",
    "\n",
    "# --- USER CONFIG ---\n",
    "DATA_DIR      = \"/datax/scratch/jliang/augmented\"\n",
    "TRAIN_IMG_DIR = os.path.join(DATA_DIR, \"train\", \"images\")\n",
    "TRAIN_LBL_DIR = os.path.join(DATA_DIR, \"train\", \"labels\")\n",
    "VAL_IMG_DIR   = os.path.join(DATA_DIR, \"val\",   \"images\")\n",
    "VAL_LBL_DIR   = os.path.join(DATA_DIR, \"val\",   \"labels\")\n",
    "\n",
    "CHECKPOINT    = \"/home/jliang/gbt-rfi/segment-anything/sam_vit_b_01ec64.pth\"\n",
    "OUT_WEIGHTS   = \"sam_finetuned.pth\"\n",
    "BATCH_SIZE    = 8\n",
    "LR            = 1e-3\n",
    "EPOCHS        = 30\n",
    "IMG_SIZE      = (1024, 1024)\n",
    "DEVICE        = torch.device(\"cpu\")\n",
    "# -------------------\n",
    "\n",
    "class YoloMaskDataset(Dataset):\n",
    "    def __init__(self, images_dir, labels_dir, img_size, transform=None):\n",
    "        self.images = sorted(glob(os.path.join(images_dir, \"*.*\")))\n",
    "        self.labels = [\n",
    "            os.path.join(labels_dir, os.path.splitext(os.path.basename(p))[0] + \".txt\")\n",
    "            for p in self.images\n",
    "        ]\n",
    "        self.transform = transform or T.Compose([\n",
    "            T.Resize(img_size),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.images[idx]).convert(\"RGB\")\n",
    "        W, H = img.size\n",
    "        mask = Image.new(\"L\", (W, H), 0)\n",
    "        draw = ImageDraw.Draw(mask)\n",
    "        with open(self.labels[idx]) as f:\n",
    "            for line in f:\n",
    "                _, xc, yc, w, h = line.strip().split()\n",
    "                xc, yc, w, h = map(float, (xc, yc, w, h))\n",
    "                x0 = (xc - w/2) * W\n",
    "                y0 = (yc - h/2) * H\n",
    "                x1 = (xc + w/2) * W\n",
    "                y1 = (yc + h/2) * H\n",
    "                draw.rectangle([x0, y0, x1, y1], fill=1)\n",
    "        img_t = self.transform(img)\n",
    "        mask_t = self.transform(mask).float()\n",
    "        return img_t, mask_t\n",
    "\n",
    "\n",
    "def make_dataloaders():\n",
    "    train_ds = YoloMaskDataset(TRAIN_IMG_DIR, TRAIN_LBL_DIR, IMG_SIZE)\n",
    "    val_ds = YoloMaskDataset(VAL_IMG_DIR, VAL_LBL_DIR, IMG_SIZE)\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    return train_dl, val_dl\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    sam = sam_model_registry[\"vit_b\"](checkpoint=CHECKPOINT)\n",
    "    for p in sam.image_encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "    sam.to(DEVICE)\n",
    "    return sam\n",
    "\n",
    "\n",
    "def train():\n",
    "    train_dl, val_dl = make_dataloaders()\n",
    "    model = build_model()\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print(f\"\\n=== Epoch {epoch}/{EPOCHS} ===\", flush=True)\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for imgs, masks in tqdm.tqdm(train_dl, desc=\"Train batches\"):\n",
    "            imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)  # masks shape [B,1,H,W]\n",
    "            # Encode image to get embeddings\n",
    "            image_embeddings = model.image_encoder(imgs)\n",
    "            # Generate prompt embeddings (no prompt)\n",
    "            sparse_embeddings, dense_embeddings = model.prompt_encoder(points=None, boxes=None, masks=None)\n",
    "            image_pe = model.prompt_encoder.get_dense_pe().to(DEVICE)\n",
    "\n",
    "            # Decode mask logits [B,1,Hp,Wp]\n",
    "            logits, _ = model.mask_decoder(\n",
    "                image_embeddings=image_embeddings,\n",
    "                image_pe=image_pe,\n",
    "                sparse_prompt_embeddings=sparse_embeddings,\n",
    "                dense_prompt_embeddings=dense_embeddings,\n",
    "                multimask_output=False,\n",
    "            )\n",
    "\n",
    "            # Upsample logits to GT resolution [H, W]\n",
    "            logits = F.interpolate(logits, size=masks.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "            # Compute loss against masks [B,1,H,W]\n",
    "            loss = criterion(logits, masks)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "        avg_train = running_loss / len(train_dl.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in val_dl:\n",
    "                imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
    "                image_embeddings = model.image_encoder(imgs)\n",
    "                sparse_embeddings, dense_embeddings = model.prompt_encoder(points=None, boxes=None, masks=None)\n",
    "                image_pe = model.prompt_encoder.get_dense_pe().to(DEVICE)\n",
    "                logits, _ = model.mask_decoder(\n",
    "                    image_embeddings=image_embeddings,\n",
    "                    image_pe=image_pe,\n",
    "                    sparse_prompt_embeddings=sparse_embeddings,\n",
    "                    dense_prompt_embeddings=dense_embeddings,\n",
    "                    multimask_output=False,\n",
    "                )\n",
    "                logits = F.interpolate(logits, size=masks.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "                val_loss += criterion(logits, masks).item() * imgs.size(0)\n",
    "        avg_val = val_loss / len(val_dl.dataset)\n",
    "        scheduler.step(avg_val)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} — train loss: {avg_train:.4f}, val loss: {avg_val:.4f}\")\n",
    "\n",
    "        # Save best\n",
    "        if avg_val < best_val_loss:\n",
    "            best_val_loss = avg_val\n",
    "            torch.save(model.state_dict(), OUT_WEIGHTS)\n",
    "            print(\"→ New best model saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
