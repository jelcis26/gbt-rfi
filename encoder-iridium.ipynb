{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision\n",
    "%pip install umap-learn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "import re\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torchvision.transforms as T\n",
    "from PIL import ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf2187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import shutil\n",
    "\n",
    "# # === CONFIGURE THESE PATHS ===\n",
    "# train_src_dirs = [\n",
    "#     Path(\"/datax/scratch/jliang/iridium_train\"),\n",
    "#     Path(\"/datax/scratch/jliang/s154_train\"),\n",
    "# ]\n",
    "# train_dst = Path(\"/datax/scratch/jliang/iridium_s154_train\")\n",
    "\n",
    "# val_src_dirs = [\n",
    "#     Path(\"/datax/scratch/jliang/iridium_val\"),\n",
    "#     Path(\"/datax/scratch/jliang/s154_val\"),\n",
    "# ]\n",
    "# val_dst = Path(\"/datax/scratch/jliang/iridium_s154_val\")\n",
    "# # =============================\n",
    "\n",
    "# # Make sure the destinations exist\n",
    "# train_dst.mkdir(parents=True, exist_ok=True)\n",
    "# val_dst.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # File extensions to consider “images”\n",
    "# IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\", \".tiff\", \".webp\"}\n",
    "\n",
    "# def merge_folders(src_dirs, dst_folder, exts):\n",
    "#     \"\"\"\n",
    "#     Copy every image from each folder in src_dirs into dst_folder,\n",
    "#     but skip any filename you’ve already copied.\n",
    "#     \"\"\"\n",
    "#     seen = set()\n",
    "#     for src in src_dirs:\n",
    "#         for path in src.rglob(\"*\"):\n",
    "#             if not path.is_file(): \n",
    "#                 continue\n",
    "#             if path.suffix.lower() not in exts:\n",
    "#                 continue\n",
    "\n",
    "#             name = path.name\n",
    "#             if name in seen:\n",
    "#                 # we've already copied a file with this name → skip\n",
    "#                 continue\n",
    "\n",
    "#             shutil.copy2(path, dst_folder / name)\n",
    "#             seen.add(name)\n",
    "\n",
    "# # Merge train\n",
    "# merge_folders(train_src_dirs, train_dst, IMAGE_EXTS)\n",
    "# print(f\"Train merge complete: {len(list(train_dst.iterdir()))} files in {train_dst!r}\")\n",
    "\n",
    "# # Merge val\n",
    "# merge_folders(val_src_dirs, val_dst, IMAGE_EXTS)\n",
    "# print(f\"Val   merge complete: {len(list(val_dst.iterdir()))} files in {val_dst!r}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855c79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Hyperparameters ──────────────────────────────────────────────────────\n",
    "IMAGE_SIZE     = 224\n",
    "IMAGE_CHANNELS = 3\n",
    "LATENT_DIM     = 32\n",
    "BATCH_SIZE     = 32\n",
    "LR             = 1e-4\n",
    "EPOCHS         = 20\n",
    "DEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TRAIN_IMG_DIR = '/datax/scratch/jliang/vae_set/train/images'\n",
    "VAL_IMG_DIR   = '/datax/scratch/jliang/vae_set/val/images'\n",
    "TRAIN_LBL_DIR = '/datax/scratch/jliang/vae_set/train/labels'\n",
    "VAL_LBL_DIR   = '/datax/scratch/jliang/vae_set/val/labels'\n",
    "IMG_SIZE = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9fb9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloMaskDataset(Dataset):\n",
    "    def __init__(self, images_dir, labels_dir, img_size, transform=None):\n",
    "        # … your existing __init__ up through building self.images/self.labels …\n",
    "        self.images = sorted(glob(os.path.join(images_dir, \"*.*\")))\n",
    "        self.labels = [\n",
    "            os.path.join(labels_dir, os.path.splitext(os.path.basename(p))[0] + \".txt\")\n",
    "            for p in self.images\n",
    "        ]\n",
    "        self.transform = transform or T.Compose([\n",
    "            T.Resize(img_size),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "        # optionally: compute global min/max f to normalize later\n",
    "        freqs = []\n",
    "        pat = re.compile(r\"_f_(?P<fs>[\\d\\.]+)_(?P<fe>[\\d\\.]+)\")\n",
    "        for p in self.images:\n",
    "            m = pat.search(os.path.basename(p))\n",
    "            if m:\n",
    "                freqs += [float(m.group(\"fs\").rstrip(\".\")), float(m.group(\"fe\").rstrip(\".\"))]\n",
    "        self.fmin, self.fmax = min(freqs), max(freqs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        lbl_path = self.labels[idx]\n",
    "\n",
    "        # — load image & mask (unchanged) —\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        W, H = img.size\n",
    "        mask = Image.new(\"L\", (W, H), 0)\n",
    "        draw = ImageDraw.Draw(mask)\n",
    "        with open(lbl_path) as f:\n",
    "            for line in f:\n",
    "                _, xc, yc, w, h = line.strip().split()\n",
    "                xc, yc, w, h = map(float, (xc, yc, w, h))\n",
    "                x0 = (xc - w/2) * W\n",
    "                y0 = (yc - h/2) * H\n",
    "                x1 = (xc + w/2) * W\n",
    "                y1 = (yc + h/2) * H\n",
    "                draw.rectangle([x0, y0, x1, y1], fill=1)\n",
    "\n",
    "        img_t  = self.transform(img)\n",
    "        mask_t = self.transform(mask).float()\n",
    "\n",
    "        # — robustly parse f_start, f_end from filename —\n",
    "        fname = os.path.basename(img_path)\n",
    "        name_no_ext, _ = os.path.splitext(fname)\n",
    "        try:\n",
    "            # everything after \"_f_\":\n",
    "            freqs_str = name_no_ext.split(\"_f_\")[1]\n",
    "            # split into two parts only\n",
    "            fstart_str, fend_str = freqs_str.split(\"_\", 1)\n",
    "            # strip any trailing dots (e.g. \"1539.5536.\" → \"1539.5536\")\n",
    "            fstart_str = fstart_str.rstrip(\".\")\n",
    "            fend_str   = fend_str.rstrip(\".\")\n",
    "            f_start = float(fstart_str)\n",
    "            f_end   = float(fend_str)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Could not parse f_start/f_end from '{fname}'\") from e\n",
    "\n",
    "        # — normalize into [0,1], if desired —\n",
    "        norm_fs = (f_start - self.fmin) / (self.fmax - self.fmin)\n",
    "        norm_fe = (f_end   - self.fmin) / (self.fmax - self.fmin)\n",
    "        cond = torch.tensor([norm_fs, norm_fe], dtype=torch.float32)\n",
    "\n",
    "        return img_t, mask_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3443ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ─── Custom Dataset for YOLO Cropped Boxes ─────────────────────────────────\n",
    "# class YOLOCropDataset(Dataset):\n",
    "#     def __init__(self, image_dir, label_dir, transform=None):\n",
    "#         self.samples = []\n",
    "#         self.transform = transform\n",
    "#         for img_path in sorted(glob.glob(os.path.join(image_dir, \"*.png\"))):\n",
    "#             base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "#             txt_path = os.path.join(label_dir, base + \".txt\")\n",
    "#             if not os.path.exists(txt_path):\n",
    "#                 continue\n",
    "#             img = Image.open(img_path)\n",
    "#             W, H = img.size\n",
    "#             with open(txt_path) as f:\n",
    "#                 for line in f:\n",
    "#                     parts = line.strip().split()\n",
    "#                     if len(parts) < 5:\n",
    "#                         continue\n",
    "#                     _, x_c, y_c, w, h = parts\n",
    "#                     x_c, y_c, w, h = map(float, (x_c, y_c, w, h))\n",
    "#                     x0 = int((x_c - w/2) * W)\n",
    "#                     y0 = int((y_c - h/2) * H)\n",
    "#                     x1 = int((x_c + w/2) * W)\n",
    "#                     y1 = int((y_c + h/2) * H)\n",
    "#                     self.samples.append((img_path, (x0, y0, x1, y1)))\n",
    "#         if len(self.samples) == 0:\n",
    "#             raise RuntimeError(\"No YOLO crops found. Check image_dir/label_dir.\")\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.samples)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_path, box = self.samples[idx]\n",
    "#         img = Image.open(img_path).convert(\"RGB\")\n",
    "#         crop = img.crop(box)\n",
    "#         if self.transform:\n",
    "#             crop = self.transform(crop)\n",
    "#         return crop, img_path\n",
    "\n",
    "# class YOLOCropDatasetWithPath(YOLOCropDataset):\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_path, box = self.samples[idx]\n",
    "#         img = Image.open(img_path).convert(\"RGB\")\n",
    "#         crop = img.crop(box)\n",
    "#         if self.transform:\n",
    "#             crop = self.transform(crop)\n",
    "#         return crop, img_path\n",
    "    \n",
    "# # ─── Transforms ────────────────────────────────────────────────────────────\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# # ─── Instantiate Datasets & Loaders ───────────────────────────────────────\n",
    "# train_dataset = YOLOCropDataset(\"/datax/scratch/jliang/dataset_final_small/train/images\", \"/datax/scratch/jliang/dataset_final_small/train/labels\", transform)\n",
    "# test_dataset  = YOLOCropDataset(\"/datax/scratch/jliang/dataset_final_small/val/images\", \"/datax/scratch/jliang/dataset_final_small/val/labels\", transform)\n",
    "# train_loader  = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4, pin_memory=True)\n",
    "# test_loader   = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "# full_ds = YOLOCropDatasetWithPath(\n",
    "#     \"/datax/scratch/jliang/dataset_final_small/train/images\", \"/datax/scratch/jliang/dataset_final_small/train/labels\", transform\n",
    "# )\n",
    "# full_dl = DataLoader(full_ds, batch_size=64, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2916763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MaCVAE(nn.Module):\n",
    "#     def __init__(self, in_ch=3, latent_dim=LATENT_DIM, beta=1.5, cond_dim=2):\n",
    "#         super().__init__()\n",
    "#         # Encoder CNN\n",
    "#         channels = [16, 32, 16, 32, 64, 128, 64, 128]\n",
    "#         layers = []\n",
    "#         c_in = in_ch\n",
    "#         for c_out in channels:\n",
    "#             layers += [\n",
    "#                 nn.Conv2d(c_in, c_out, kernel_size=3, stride=2, padding=1),\n",
    "#                 nn.ReLU(inplace=True)\n",
    "#             ]\n",
    "#             c_in = c_out\n",
    "#         self.encoder_cnn = nn.Sequential(*layers)\n",
    "\n",
    "#         # Compute flattened dimension\n",
    "#         with torch.no_grad():\n",
    "#             x0 = torch.zeros(1, in_ch, IMAGE_SIZE, IMAGE_SIZE)\n",
    "#             flat_dim = self.encoder_cnn(x0).numel()\n",
    "\n",
    "#         # Bottleneck layers (conditional)\n",
    "#         self.fc_mu     = nn.Linear(flat_dim + cond_dim, latent_dim)\n",
    "#         self.fc_logvar = nn.Linear(flat_dim + cond_dim, latent_dim)\n",
    "\n",
    "#         # Decoder fc (conditional)\n",
    "#         self.fc_dec = nn.Linear(latent_dim + cond_dim, flat_dim)\n",
    "\n",
    "#         # Decoder CNN transpose\n",
    "#         deconv = []\n",
    "#         c_in = channels[-1]\n",
    "#         for c_out in reversed(channels[:-1]):\n",
    "#             deconv += [\n",
    "#                 nn.ConvTranspose2d(c_in, c_out, 3, 2, 1, output_padding=1),\n",
    "#                 nn.ReLU(inplace=True)\n",
    "#             ]\n",
    "#             c_in = c_out\n",
    "#         deconv.append(nn.ConvTranspose2d(c_in, in_ch, 3, 2, 1, output_padding=1))\n",
    "#         self.decoder_cnn = nn.Sequential(*deconv)\n",
    "\n",
    "#         self.beta = beta\n",
    "\n",
    "#     def encode(self, x, cond):\n",
    "#         h = self.encoder_cnn(x).flatten(1)\n",
    "#         h = torch.cat([h, cond], dim=1)\n",
    "#         return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "#     def reparam(self, mu, logvar):\n",
    "#         sigma = (0.5 * logvar).exp()\n",
    "#         return mu + sigma * torch.randn_like(sigma)\n",
    "\n",
    "#     def decode(self, z, cond):\n",
    "#         zc = torch.cat([z, cond], dim=1)\n",
    "#         h = self.fc_dec(zc).view(z.size(0), -1, 1, 1)\n",
    "#         out = self.decoder_cnn(h)\n",
    "#         out = F.interpolate(\n",
    "#             out,\n",
    "#             size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "#             mode=\"bilinear\",\n",
    "#             align_corners=False\n",
    "#         )\n",
    "#         return out\n",
    "\n",
    "#     def forward(self, x, cond):\n",
    "#         mu, logvar = self.encode(x, cond)\n",
    "#         z = self.reparam(mu, logvar)\n",
    "#         recon = self.decode(z, cond)\n",
    "#         return recon, mu, logvar\n",
    "\n",
    "#     def loss(self, recon, x, mu, logvar):\n",
    "#         rec = F.mse_loss(recon, x, reduction='mean')\n",
    "#         kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
    "#         return rec + self.beta * kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d93e065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MaVAE(nn.Module):\n",
    "#     def __init__(self, in_ch=3, latent_dim=8, beta=1.5):\n",
    "#         super().__init__()\n",
    "#         layers = []\n",
    "#         # 8 conv layers: two reps of [16→32→64→128]\n",
    "#         channels = [16,32,16,32,64,128,64,128]\n",
    "#         c_in = in_ch\n",
    "#         for c_out in channels:\n",
    "#             layers += [nn.Conv2d(c_in, c_out, kernel_size=3, stride=2, padding=1),\n",
    "#                        nn.ReLU(inplace=True)]\n",
    "#             c_in = c_out\n",
    "#         self.encoder_cnn = nn.Sequential(*layers)\n",
    "#         # compute flattened dim\n",
    "#         with torch.no_grad():\n",
    "#             x = torch.zeros(1, in_ch, IMAGE_SIZE, IMAGE_SIZE)\n",
    "#             flat_dim = self.encoder_cnn(x).numel()\n",
    "#         # bottleneck\n",
    "#         self.fc_mu     = nn.Linear(flat_dim, latent_dim)\n",
    "#         self.fc_logvar = nn.Linear(flat_dim, latent_dim)\n",
    "#         # decoder\n",
    "#         self.fc_dec    = nn.Linear(latent_dim, flat_dim)\n",
    "#         # mirror decoder conv-transpose stack\n",
    "#         deconv_layers = []\n",
    "#         for c_out in reversed(channels[:-1]):\n",
    "#             deconv_layers += [\n",
    "#               nn.ConvTranspose2d(c_in, c_out, 3, 2, 1, output_padding=1),\n",
    "#               nn.ReLU(inplace=True)\n",
    "#             ]\n",
    "#             c_in = c_out\n",
    "#         deconv_layers.append(nn.ConvTranspose2d(c_in, in_ch, 3, 2, 1, output_padding=1))\n",
    "#         self.decoder_cnn = nn.Sequential(*deconv_layers)\n",
    "#         self.beta = beta\n",
    "\n",
    "#     def encode(self, x):\n",
    "#         h = self.encoder_cnn(x).flatten(1)\n",
    "#         return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "#     def reparam(self, mu, log_sigma):\n",
    "#         sigma = (0.5*log_sigma).exp()\n",
    "#         return mu + sigma*torch.randn_like(sigma)\n",
    "\n",
    "#     def decode(self, z):\n",
    "#         h   = self.fc_dec(z).view(z.size(0), -1, 1, 1)\n",
    "#         out = self.decoder_cnn(h)                # currently [B, 3, 256,256]\n",
    "#         out = F.interpolate(\n",
    "#             out,\n",
    "#             size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "#             mode=\"bilinear\",\n",
    "#             align_corners=False\n",
    "#         )                                        # now [B, 3, 224,224]\n",
    "#         return out\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mu, log_sigma = self.encode(x)\n",
    "#         z       = self.reparam(mu, log_sigma)\n",
    "#         recon   = self.decode(z)\n",
    "#         return recon, mu, log_sigma\n",
    "\n",
    "#     def loss(self, recon, x, mu, log_sigma):\n",
    "#         rec = F.mse_loss(recon, x, reduction='mean')\n",
    "#         kld = -0.5 * torch.sum(1 + log_sigma - mu.pow(2) - log_sigma.exp()) / x.size(0)\n",
    "#         return rec + self.beta * kld\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77110aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# from torch.optim import Adam\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# # --- assuming these are already defined somewhere above: ---\n",
    "# # DATA_DIR, TRAIN_IMG_DIR, TRAIN_LBL_DIR, VAL_IMG_DIR, VAL_LBL_DIR\n",
    "# # IMG_SIZE, BATCH_SIZE, LR, DEVICE, model (MaCVAE instance)\n",
    "\n",
    "# # 1) Datasets + DataLoaders\n",
    "# train_ds = YoloMaskDataset(\n",
    "#     TRAIN_IMG_DIR,\n",
    "#     TRAIN_LBL_DIR,\n",
    "#     IMG_SIZE,\n",
    "#     transform=T.Compose([\n",
    "#         T.Resize(IMG_SIZE),\n",
    "#         T.ToTensor(),\n",
    "#     ])\n",
    "# )\n",
    "# val_ds = YoloMaskDataset(\n",
    "#     VAL_IMG_DIR,\n",
    "#     VAL_LBL_DIR,\n",
    "#     IMG_SIZE,\n",
    "#     transform=T.Compose([\n",
    "#         T.Resize(IMG_SIZE),\n",
    "#         T.ToTensor(),\n",
    "#     ])\n",
    "# )\n",
    "\n",
    "# train_dl = DataLoader(\n",
    "#     train_ds,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=True,\n",
    "#     num_workers=2,\n",
    "#     pin_memory=True\n",
    "# )\n",
    "# val_dl = DataLoader(\n",
    "#     val_ds,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=False,\n",
    "#     num_workers=2,\n",
    "#     pin_memory=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338f734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Assumes `model` is your MaCVAE, and `train_dl`, `val_dl`, `optimizer`, `scheduler`,\n",
    "# # `DEVICE`, and `EPOCHS` are already defined.\n",
    "\n",
    "# # ─── Setup ───────────────────────────────────────────────────────────────────\n",
    "# # Assumes you’ve already created:\n",
    "# #   train_dl, val_dl, DEVICE, LR, EPOCHS\n",
    "# # and that train_dl.dataset is your YoloMaskDataset instance\n",
    "\n",
    "# # 1) Instantiate model, optimizer, scheduler\n",
    "# model     = MaCVAE(in_ch=3, latent_dim=LATENT_DIM, beta=1.5, cond_dim=2).to(DEVICE)\n",
    "# optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "# # 2) Grab fmin/fmax from your dataset so it’s in scope\n",
    "# fmin, fmax = train_dl.dataset.fmin, train_dl.dataset.fmax\n",
    "\n",
    "# # ─── Training Function ─────────────────────────────────────────────────────\n",
    "# def train(model, train_dl, val_dl, optimizer, scheduler, epochs, device, fmin, fmax):\n",
    "#     best_val_loss = float('inf')\n",
    "#     train_losses, val_losses = [], []\n",
    "\n",
    "#     for epoch in range(1, epochs + 1):\n",
    "#         # ---- Train ----\n",
    "#         model.train()\n",
    "#         running_train = 0.0\n",
    "#         for imgs, masks, cond in tqdm(train_dl, desc=f\"Epoch {epoch}/{epochs} [Train]\", leave=False):\n",
    "#             imgs, masks, cond = imgs.to(device), masks.to(device), cond.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             recon, mu, logvar = model(imgs, cond)\n",
    "#             loss = model.loss(recon, imgs, mu, logvar)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_train += loss.item() * imgs.size(0)\n",
    "#         avg_train = running_train / len(train_dl.dataset)\n",
    "#         train_losses.append(avg_train)\n",
    "\n",
    "#         # ---- Validate & collect latents ----\n",
    "#         model.eval()\n",
    "#         running_val = 0.0\n",
    "#         mus, freqs = [], []\n",
    "#         with torch.no_grad():\n",
    "#             for imgs, masks, cond in tqdm(val_dl, desc=f\"Epoch {epoch}/{epochs} [Val]\", leave=False):\n",
    "#                 imgs, masks, cond = imgs.to(device), masks.to(device), cond.to(device)\n",
    "#                 recon, mu, logvar = model(imgs, cond)\n",
    "#                 loss = model.loss(recon, imgs, mu, logvar)\n",
    "#                 running_val += loss.item() * imgs.size(0)\n",
    "\n",
    "#                 # store for t-SNE later\n",
    "#                 mus.append(mu.cpu().numpy())\n",
    "#                 # denormalize the f_start (first cond dim)\n",
    "#                 fs_batch = cond[:,0].cpu().numpy() * (fmax - fmin) + fmin\n",
    "#                 freqs.append(fs_batch)\n",
    "\n",
    "#         avg_val = running_val / len(val_dl.dataset)\n",
    "#         val_losses.append(avg_val)\n",
    "\n",
    "#         # LR scheduler\n",
    "#         scheduler.step(avg_val)\n",
    "\n",
    "#         # logging & checkpoint\n",
    "#         print(f\"Epoch {epoch}/{epochs} — Train: {avg_train:.4f}, Val: {avg_val:.4f}\")\n",
    "#         if avg_val < best_val_loss:\n",
    "#             best_val_loss = avg_val\n",
    "#             torch.save(model.state_dict(), \"sam_finetuned_cvae.pth\")\n",
    "#             print(\"→ New best model saved.\")\n",
    "\n",
    "#     return train_losses, val_losses, np.vstack(mus), np.hstack(freqs)\n",
    "\n",
    "# # ─── Run ─────────────────────────────────────────────────────────────────────\n",
    "# train_losses, val_losses, mus_array, freqs_array = train(\n",
    "#     model, train_dl, val_dl,\n",
    "#     optimizer, scheduler,\n",
    "#     EPOCHS, DEVICE,\n",
    "#     fmin, fmax\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae94cdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "MODEL_PATH = None     # if you have a fine-tuned ViT checkpoint, otherwise None\n",
    "N_LATENT   = 32       # desired latent dim\n",
    "\n",
    "# ─── DATASET & DATALOADER ─────────────────────────────────────────────────────\n",
    "# Assume `test_ds` is held-out dataset with:\n",
    "#   test_ds.images → list of image paths\n",
    "#   test_ds.labels → list of corresponding .txt paths\n",
    "# And that `test_ds` applies exactly the transforms below.\n",
    "\n",
    "# Use the pretrained ViT’s recommended normalization\n",
    "weights   = ViT_B_16_Weights.IMAGENET1K_V1\n",
    "\n",
    "vit_preproc = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.Lambda(lambda img: img.convert(\"RGB\")),  # makes 3-channel PIL\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std =[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])\n",
    "\n",
    "mid_res_ds = YoloMaskDataset(\n",
    "    '/datax/scratch/jliang/vit_inputs_midres/images',\n",
    "    labels_dir='/datax/scratch/jliang/vit_inputs_midres/labels',\n",
    "    img_size=(224,224),\n",
    "    transform=vit_preproc,\n",
    ")\n",
    "\n",
    "mid_res_dl = DataLoader(\n",
    "    mid_res_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# ─── BUILD THE ViT ENCODER ────────────────────────────────────────────────────\n",
    "backbone = vit_b_16(weights=weights).to(DEVICE)\n",
    "# Drop the classification head, leave the CLS embedding\n",
    "backbone.heads = nn.Identity()\n",
    "\n",
    "# (Optional) load your own checkpoint\n",
    "if MODEL_PATH:\n",
    "    backbone.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "\n",
    "backbone.eval()\n",
    "\n",
    "# ─── EXTRACT ViT FEATURES ─────────────────────────────────────────────────────\n",
    "feats_list = []\n",
    "with torch.no_grad():\n",
    "    for imgs, _ in mid_res_dl:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        # imgs have already been preprocessed by mid_res_ds → should match `preproc`\n",
    "        # If not, apply: imgs = preproc(imgs)\n",
    "        cls_emb = backbone(imgs)            # shape [B, 768]\n",
    "        feats_list.append(cls_emb.cpu().numpy())\n",
    "\n",
    "features_array = np.vstack(feats_list)   # shape (N_test, 768)\n",
    "\n",
    "# ─── REDUCE TO 32-D WITH PCA ───────────────────────────────────────────────────\n",
    "pca = PCA(n_components=LATENT_DIM, random_state=42)\n",
    "latents_32d = pca.fit_transform(features_array)  # shape (N_test, 32)\n",
    "\n",
    "# ─── EXTRACT FREQUENCIES ───────────────────────────────────────────────────────\n",
    "freqs = []\n",
    "for path in mid_res_ds.images:\n",
    "    fname = os.path.basename(path)\n",
    "    # adjust the regex to match “f_<number>” in your filenames\n",
    "    m = re.search(r\"f_([0-9\\.]+)\", fname)\n",
    "    freqs.append(float(m.group(1)) if m else np.nan)\n",
    "freqs_array = np.array(freqs)            # shape (N_test,)\n",
    "\n",
    "assert latents_32d.shape[0] == freqs_array.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43c6939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# # ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "# MODEL_PATH = \"best_mavae.pth\"\n",
    "\n",
    "# # ─── ASSUMPTIONS ───────────────────────────────────────────────────────────────\n",
    "# # 1) test_ds is your held-out YoloMaskDataset (or similar) that has:\n",
    "# #       test_ds.images  → list of image-file paths\n",
    "# #    and test_ds.labels  → list of corresponding .txt paths\n",
    "# # 2) You create your DataLoader without shuffling so the order matches test_ds.images:\n",
    "# test_ds = YoloMaskDataset(\n",
    "#     \"/datax/scratch/jliang/iridium_s154_test\",\n",
    "#     \"/datax/scratch/jliang/iridium_s154_labels\",\n",
    "#     IMG_SIZE,\n",
    "#     transform=T.Compose([\n",
    "#         T.Resize(IMG_SIZE),\n",
    "#         T.ToTensor(),\n",
    "#     ])\n",
    "# )\n",
    "# test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# # ─── LOAD MODEL ─────────────────────────────────────────────────────────────────\n",
    "# model = MaVAE(in_ch=3, latent_dim=LATENT_DIM, beta=1.5).to(DEVICE)\n",
    "# model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "# model.eval()\n",
    "\n",
    "# # ─── RUN TEST-SET THROUGH THE VAE ──────────────────────────────────────────────\n",
    "# mus_list = []\n",
    "# with torch.no_grad():\n",
    "#     for imgs, _ in test_dl:               # unpack exactly what your test_dl returns\n",
    "#         imgs = imgs.to(DEVICE)\n",
    "#         _, mu, _ = model(imgs)            # ignore recon & logvar\n",
    "#         mus_list.append(mu.cpu().numpy())\n",
    "\n",
    "# # stack into (N_test, latent_dim)\n",
    "# mus_array = np.vstack(mus_list)\n",
    "\n",
    "# # ─── EXTRACT FREQUENCIES FROM FILENAMES ────────────────────────────────────────\n",
    "# # Regex will pull “f_1234.5” out of each image name\n",
    "# freqs = []\n",
    "# for path in test_ds.images:\n",
    "#     fname = path.split(\"/\")[-1]\n",
    "#     m = re.search(r\"f_([0-9\\.]+)\", fname)\n",
    "#     freqs.append(float(m.group(1)) if m else np.nan)\n",
    "# freqs_array = np.array(freqs)\n",
    "\n",
    "# # sanity check\n",
    "# assert mus_array.shape[0] == freqs_array.shape[0], \"Ordering mismatch!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0401b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "# Z_tsne = tsne.fit_transform(latents_32d)\n",
    "\n",
    "# plt.figure(figsize=(6,6))\n",
    "# plt.scatter(Z_tsne[:,0], Z_tsne[:,1], c=freqs_array, cmap=\"viridis\", s=8)\n",
    "# plt.colorbar(label=\"Signal frequency\")\n",
    "# plt.title(\"t-SNE of ViT-16 latents\")\n",
    "# # plt.xlabel(\"TSNE 1\"); plt.ylabel(\"TSNE 2\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c4f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(latents_32d), len(freqs_array), latents_32d.shape, freqs_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae39bcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Suppose Z_mid, Z_high are your [N×D] ViT outputs\n",
    "Z_mid_norm  = normalize(latents_32d,  norm=\"l2\", axis=1)\n",
    "\n",
    "tsne_mid_norm = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "Z_tsne_mid_norm = tsne_mid_norm.fit_transform(Z_mid_norm)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(Z_tsne_mid_norm[:,0], Z_tsne_mid_norm[:,1], c=freqs_array, cmap=\"viridis\", s=8)\n",
    "plt.colorbar(label=\"Signal frequency\")\n",
    "plt.title(\"t-SNE of ViT-16 latents (normalized)\")\n",
    "# plt.xlabel(\"TSNE 1\"); plt.ylabel(\"TSNE 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d425c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import DBSCAN, HDBSCAN\n",
    "\n",
    "# # assume `mus` is your numpy array of shape (N, latent_dim)\n",
    "# clusterer = HDBSCAN(min_cluster_size=100,\n",
    "#                             min_samples=50,\n",
    "#                             metric='cosine',\n",
    "#                             cluster_selection_method='eom')\n",
    "# labels = clusterer.fit_predict(latents_32d)   # array of length N: −1 = noise, ≥0 = cluster IDs\n",
    "\n",
    "# # now you can e.g. visualize\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.scatter(Z_tsne[:,0], Z_tsne[:,1], c=labels, cmap=\"viridis\", s=8)\n",
    "# plt.colorbar(label=\"Cluster ID\")\n",
    "# plt.title(\"t-SNE from HDBSCAN\")\n",
    "# # plt.xlabel(\"TSNE 1\"); plt.ylabel(\"TSNE 2\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20fdf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN, HDBSCAN\n",
    "\n",
    "# assume `mus` is your numpy array of shape (N, latent_dim)\n",
    "clusterer_mid = HDBSCAN(min_cluster_size=100,\n",
    "                            min_samples=50,\n",
    "                            metric='cosine',\n",
    "                            cluster_selection_method='eom')\n",
    "labels = clusterer_mid.fit_predict(Z_mid_norm)   # array of length N: −1 = noise, ≥0 = cluster IDs\n",
    "\n",
    "# now you can e.g. visualize\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(Z_tsne_mid_norm[:,0], Z_tsne_mid_norm[:,1], c=labels, cmap=\"viridis\", s=8)\n",
    "plt.colorbar(label=\"Cluster ID\")\n",
    "plt.title(\"t-SNE from HDBSCAN (normalized)\")\n",
    "# plt.xlabel(\"TSNE 1\"); plt.ylabel(\"TSNE 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e981292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# from pathlib import Path\n",
    "# from torchvision import transforms\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # ─── parameters you can tweak ─────────────────────────────────────────────\n",
    "# ROOT_DIR     = Path(\"/datax/scratch/jliang/waterfall_32_pngs\")\n",
    "# NUM_SAMPLES  = 3_000          # absolute count  (set None to use FRACTION)\n",
    "# FRACTION     = 0.1            # or 10 % of all files\n",
    "# RNG_SEED     = 42             # so the sample is reproducible\n",
    "# BATCH_SIZE   = 32\n",
    "# NUM_WORKERS  = 4\n",
    "# # ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# # 1) collect every PNG path once\n",
    "# all_paths = sorted(ROOT_DIR.rglob(\"*.png\"))\n",
    "# if not all_paths:\n",
    "#     raise RuntimeError(f\"No PNGs found in {ROOT_DIR}\")\n",
    "\n",
    "# # 2) draw a random sample\n",
    "# random.seed(RNG_SEED)\n",
    "# if NUM_SAMPLES is None:\n",
    "#     NUM_SAMPLES = int(len(all_paths) * FRACTION)\n",
    "\n",
    "# sampled_32_paths = random.sample(all_paths, k=NUM_SAMPLES)\n",
    "# print(f\"Sampling {NUM_SAMPLES} of {len(all_paths)} images\")\n",
    "\n",
    "# # 3) build the dataset with ONLY those paths\n",
    "# class WaterfallDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, paths, transform=None):\n",
    "#         self.paths = paths\n",
    "#         self.transform = transform or transforms.ToTensor()\n",
    "#     def __len__(self):\n",
    "#         return len(self.paths)\n",
    "#     def __getitem__(self, idx):\n",
    "#         img = Image.open(self.paths[idx]).convert(\"L\")\n",
    "#         return self.transform(img), str(self.paths[idx])\n",
    "\n",
    "# ds = WaterfallDataset(sampled_paths)\n",
    "# dl = DataLoader(ds, batch_size=BATCH_SIZE,\n",
    "#                 shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b77d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms as T\n",
    "# from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "# DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# BATCH_SIZE = 64\n",
    "# MODEL_PATH = None     # if you have a fine-tuned ViT checkpoint, otherwise None\n",
    "# N_LATENT   = 32       # desired latent dim\n",
    "\n",
    "# # ─── DATASET & DATALOADER ─────────────────────────────────────────────────────\n",
    "# # Assume `test_ds` is held-out dataset with:\n",
    "# #   test_ds.images → list of image paths\n",
    "# #   test_ds.labels → list of corresponding .txt paths\n",
    "# # And that `test_ds` applies exactly the transforms below.\n",
    "\n",
    "# # Use the pretrained ViT’s recommended normalization\n",
    "# weights   = ViT_B_16_Weights.IMAGENET1K_V1\n",
    "\n",
    "# vit_preproc = T.Compose([\n",
    "#     T.Resize(256),\n",
    "#     T.CenterCrop(224),\n",
    "#     T.Lambda(lambda img: img.convert(\"RGB\")),  # makes 3-channel PIL\n",
    "#     T.ToTensor(),\n",
    "#     T.Normalize(\n",
    "#         mean=[0.485, 0.456, 0.406],\n",
    "#         std =[0.229, 0.224, 0.225],\n",
    "#     ),\n",
    "# ])\n",
    "\n",
    "# high_res_32_ds = WaterfallDataset(\n",
    "#     sampled_32_paths,\n",
    "#     transform=vit_preproc,\n",
    "# )\n",
    "\n",
    "# high_res_32_dl = DataLoader(\n",
    "#     high_res_32_ds,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=False,\n",
    "#     num_workers=2,\n",
    "#     pin_memory=True\n",
    "# )\n",
    "\n",
    "# # ─── BUILD THE ViT ENCODER ────────────────────────────────────────────────────\n",
    "# backbone = vit_b_16(weights=weights).to(DEVICE)\n",
    "# # Drop the classification head, leave the CLS embedding\n",
    "# backbone.heads = nn.Identity()\n",
    "\n",
    "# # (Optional) load your own checkpoint\n",
    "# if MODEL_PATH:\n",
    "#     backbone.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "\n",
    "# backbone.eval()\n",
    "\n",
    "# # ─── EXTRACT ViT FEATURES ─────────────────────────────────────────────────────\n",
    "# feats_list = []\n",
    "# with torch.no_grad():\n",
    "#     for imgs, _ in high_res_32_dl:\n",
    "#         imgs = imgs.to(DEVICE)\n",
    "#         # imgs have already been preprocessed by high_res_ds → should match `preproc`\n",
    "#         # If not, apply: imgs = preproc(imgs)\n",
    "#         cls_emb = backbone(imgs)            # shape [B, 768]\n",
    "#         feats_list.append(cls_emb.cpu().numpy())\n",
    "\n",
    "# features_array = np.vstack(feats_list)   # shape (N_test, 768)\n",
    "\n",
    "# # ─── REDUCE TO 32-D WITH PCA ───────────────────────────────────────────────────\n",
    "# pca = PCA(n_components=LATENT_DIM, random_state=42)\n",
    "# latents_32d_high2 = pca.fit_transform(features_array)  # shape (N_test, 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd10135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ─── EXTRACT FREQUENCIES ───────────────────────────────────────────────────────\n",
    "# freqs = []\n",
    "# for path in high_res_32_ds.paths:\n",
    "#     fname = os.path.basename(path)\n",
    "#     # adjust the regex to match “f_<number>” in your filenames\n",
    "#     m = re.search(r\"_([0-9\\.]+)\", fname)\n",
    "#     freqs.append(float(m.group(1)) if m else np.nan)\n",
    "# freqs_array_high2 = np.array(freqs)    # shape (N_test,)\n",
    "\n",
    "# assert latents_32d_high2.shape[0] == freqs_array_high2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e0d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import normalize\n",
    "\n",
    "# # Suppose Z_mid, Z_high are your [N×D] ViT outputs\n",
    "# Z_high_norm2  = normalize(latents_32d_high2,  norm=\"l2\", axis=1)\n",
    "\n",
    "# tsne_high_norm2 = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "# Z_tsne_high_norm2 = tsne_high_norm2.fit_transform(Z_high_norm2)\n",
    "\n",
    "# plt.figure(figsize=(6,6))\n",
    "# plt.scatter(Z_tsne_high_norm2[:,0], Z_tsne_high_norm2[:,1], c=freqs_array_high2, cmap=\"viridis\", s=8)\n",
    "# plt.colorbar(label=\"Signal frequency\")\n",
    "# plt.title(\"t-SNE of ViT-16 latents 1/32 Coarse Channel (normalized)\")\n",
    "# # plt.xlabel(\"TSNE 1\"); plt.ylabel(\"TSNE 2\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21414aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import DBSCAN, HDBSCAN\n",
    "\n",
    "# # assume `mus` is your numpy array of shape (N, latent_dim)\n",
    "# clusterer_high2 = HDBSCAN(min_cluster_size=20,\n",
    "#                             min_samples=15,\n",
    "#                             metric='cosine',\n",
    "#                             cluster_selection_method='eom')\n",
    "# labels = clusterer_high2.fit_predict(Z_high_norm2)   # array of length N: −1 = noise, ≥0 = cluster IDs\n",
    "\n",
    "# # now you can e.g. visualize\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.scatter(Z_tsne_high_norm2[:,0], Z_tsne_high_norm2[:,1], c=labels, cmap=\"viridis\", s=8)\n",
    "# plt.colorbar(label=\"Cluster ID\")\n",
    "# plt.title(\"t-SNE from HDBSCAN (normalized)\")\n",
    "# # plt.xlabel(\"TSNE 1\"); plt.ylabel(\"TSNE 2\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c052ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ─── parameters you can tweak ─────────────────────────────────────────────\n",
    "ROOT_DIR     = Path(\"/datax/scratch/jliang/waterfall_pngs\")\n",
    "NUM_SAMPLES  = 3000          # absolute count  (set None to use FRACTION)\n",
    "FRACTION     = 0.1            # or 10 % of all files\n",
    "RNG_SEED     = 42             # so the sample is reproducible\n",
    "BATCH_SIZE   = 32\n",
    "NUM_WORKERS  = 4\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1) collect every PNG path once\n",
    "all_paths = sorted(ROOT_DIR.rglob(\"*.png\"))\n",
    "if not all_paths:\n",
    "    raise RuntimeError(f\"No PNGs found in {ROOT_DIR}\")\n",
    "\n",
    "# 2) draw a random sample\n",
    "random.seed(RNG_SEED)\n",
    "if NUM_SAMPLES is None:\n",
    "    NUM_SAMPLES = int(len(all_paths) * FRACTION)\n",
    "\n",
    "sampled_paths = random.sample(all_paths, k=NUM_SAMPLES)\n",
    "print(f\"Sampling {NUM_SAMPLES} of {len(all_paths)} images\")\n",
    "\n",
    "# 3) build the dataset with ONLY those paths\n",
    "class WaterfallDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paths, transform=None):\n",
    "        self.paths = paths\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert(\"L\")\n",
    "        return self.transform(img), str(self.paths[idx])\n",
    "\n",
    "# ds = WaterfallDataset(sampled_paths)\n",
    "# dl = DataLoader(ds, batch_size=BATCH_SIZE,\n",
    "#                 shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61f0be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "MODEL_PATH = None     # if you have a fine-tuned ViT checkpoint, otherwise None\n",
    "N_LATENT   = 32       # desired latent dim\n",
    "\n",
    "# ─── DATASET & DATALOADER ─────────────────────────────────────────────────────\n",
    "# Assume `test_ds` is held-out dataset with:\n",
    "#   test_ds.images → list of image paths\n",
    "#   test_ds.labels → list of corresponding .txt paths\n",
    "# And that `test_ds` applies exactly the transforms below.\n",
    "\n",
    "# Use the pretrained ViT’s recommended normalization\n",
    "weights   = ViT_B_16_Weights.IMAGENET1K_V1\n",
    "\n",
    "vit_preproc = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.Lambda(lambda img: img.convert(\"RGB\")),  # makes 3-channel PIL\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std =[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])\n",
    "\n",
    "high_res_ds = WaterfallDataset(\n",
    "    sampled_paths,\n",
    "    transform=vit_preproc,\n",
    ")\n",
    "\n",
    "high_res_dl = DataLoader(\n",
    "    high_res_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# ─── BUILD THE ViT ENCODER ────────────────────────────────────────────────────\n",
    "backbone = vit_b_16(weights=weights).to(DEVICE)\n",
    "# Drop the classification head, leave the CLS embedding\n",
    "backbone.heads = nn.Identity()\n",
    "\n",
    "# (Optional) load your own checkpoint\n",
    "if MODEL_PATH:\n",
    "    backbone.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "\n",
    "backbone.eval()\n",
    "\n",
    "# ─── EXTRACT ViT FEATURES ─────────────────────────────────────────────────────\n",
    "feats_list = []\n",
    "with torch.no_grad():\n",
    "    for imgs, _ in high_res_dl:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        # imgs have already been preprocessed by high_res_ds → should match `preproc`\n",
    "        # If not, apply: imgs = preproc(imgs)\n",
    "        cls_emb = backbone(imgs)            # shape [B, 768]\n",
    "        feats_list.append(cls_emb.cpu().numpy())\n",
    "\n",
    "features_array = np.vstack(feats_list)   # shape (N_test, 768)\n",
    "\n",
    "# ─── REDUCE TO 32-D WITH PCA ───────────────────────────────────────────────────\n",
    "pca = PCA(n_components=LATENT_DIM, random_state=42)\n",
    "latents_32d_high = pca.fit_transform(features_array)  # shape (N_test, 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf74f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── EXTRACT FREQUENCIES ───────────────────────────────────────────────────────\n",
    "freqs = []\n",
    "for path in high_res_ds.paths:\n",
    "    fname = os.path.basename(path)\n",
    "    # adjust the regex to match “f_<number>” in your filenames\n",
    "    m = re.search(r\"_([0-9\\.]+)\", fname)\n",
    "    freqs.append(float(m.group(1)) if m else np.nan)\n",
    "freqs_array_high = np.array(freqs)    # shape (N_test,)\n",
    "\n",
    "assert latents_32d_high.shape[0] == freqs_array_high.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fbbc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Suppose Z_mid, Z_high are your [N×D] ViT outputs\n",
    "Z_high_norm  = normalize(latents_32d_high,  norm=\"l2\", axis=1)\n",
    "\n",
    "tsne_high_norm = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "Z_tsne_high_norm = tsne_high_norm.fit_transform(Z_high_norm)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(Z_tsne_high_norm[:,0], Z_tsne_high_norm[:,1], c=freqs_array_high, cmap=\"viridis\", s=8)\n",
    "plt.colorbar(label=\"Signal frequency\")\n",
    "plt.title(\"t-SNE of ViT-16 latents (normalized)\")\n",
    "# plt.xlabel(\"TSNE 1\"); plt.ylabel(\"TSNE 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f331730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN, HDBSCAN\n",
    "\n",
    "# assume `mus` is your numpy array of shape (N, latent_dim)\n",
    "clusterer_high = HDBSCAN(min_cluster_size=20,\n",
    "                            min_samples=15,\n",
    "                            metric='cosine',\n",
    "                            cluster_selection_method='eom')\n",
    "labels = clusterer_high.fit_predict(Z_high_norm)   # array of length N: −1 = noise, ≥0 = cluster IDs\n",
    "\n",
    "# now you can e.g. visualize\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(Z_tsne_high_norm[:,0], Z_tsne_high_norm[:,1], c=labels, cmap=\"viridis\", s=8)\n",
    "plt.colorbar(label=\"Cluster ID\")\n",
    "plt.title(\"t-SNE from HDBSCAN (normalized)\")\n",
    "# plt.xlabel(\"TSNE 1\"); plt.ylabel(\"TSNE 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09d5180",
   "metadata": {},
   "outputs": [],
   "source": [
    "membership_strength = clusterer_high.probabilities_  # float in [0,1] for each sample\n",
    "plt.scatter(Z_tsne_high_norm[:,0], Z_tsne_high_norm[:,1], c=membership_strength, cmap=\"viridis\", s=8)\n",
    "plt.colorbar(label=\"Membership Strength\")\n",
    "plt.title(\"t-SNE from HDBSCAN\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a895739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.optimize import linear_sum_assignment\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # ───────────────────────────────────────────────────────────────────────\n",
    "# # INPUTS (you must have these in your namespace)\n",
    "# # ───────────────────────────────────────────────────────────────────────\n",
    "# # latents_mid : np.ndarray, shape (N_mid,  d)\n",
    "# # freqs_mid   : np.ndarray, shape (N_mid,)    in MHz\n",
    "# # latents_fine: np.ndarray, shape (N_fine, d)\n",
    "# # freqs_fine  : np.ndarray, shape (N_fine,)  in MHz\n",
    "# # ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# # 1) Define your frequency windows\n",
    "# FREQ_WINDOWS = [\n",
    "#     (1545.0, 1546.0),\n",
    "#     (1610.0, 1630.0),\n",
    "#     # add more windows if you like...\n",
    "# ]\n",
    "\n",
    "# def assign_freq_cluster(freq, windows=FREQ_WINDOWS):\n",
    "#     for cid, (lo, hi) in enumerate(windows):\n",
    "#         if lo <= freq < hi:\n",
    "#             return cid\n",
    "#     return -1  # outside any window\n",
    "\n",
    "# # 2) Bucket each hit by its frequency window\n",
    "# mid_labels  = np.array([assign_freq_cluster(f) for f in freqs_array])\n",
    "# fine_labels = np.array([assign_freq_cluster(f) for f in freqs_array_high])\n",
    "\n",
    "# # 3) List the non‐noise cluster IDs\n",
    "# mid_cids  = sorted(set(mid_labels)  - {-1})\n",
    "# fine_cids = sorted(set(fine_labels) - {-1})\n",
    "\n",
    "# # 4) Compute unit‐length centroids in latent space\n",
    "# def compute_centroids(latents, labels, cids):\n",
    "#     C = []\n",
    "#     for cid in cids:\n",
    "#         members = latents[labels == cid]\n",
    "#         ctr = members.mean(axis=0)\n",
    "#         ctr /= np.linalg.norm(ctr) + 1e-12\n",
    "#         C.append(ctr)\n",
    "#     return np.vstack(C)\n",
    "\n",
    "# mid_C  = compute_centroids(latents_32d,  mid_labels,  mid_cids)\n",
    "# fine_C = compute_centroids(latents_32d_high, fine_labels, fine_cids)\n",
    "\n",
    "# # 5) Build the cosine‐similarity matrix\n",
    "# S = cosine_similarity(mid_C, fine_C)   # shape (len(mid_cids), len(fine_cids))\n",
    "\n",
    "# # 6) Run the Hungarian algorithm on –S to *maximise* total cosine\n",
    "# row_ind, col_ind = linear_sum_assignment(-S)\n",
    "\n",
    "# # 7) Assemble a one‐to‐one mapping (plus similarity)\n",
    "# mid2fine = {\n",
    "#     mid_cids[i]: (fine_cids[j], S[i, j])\n",
    "#     for i, j in zip(row_ind, col_ind)\n",
    "# }\n",
    "\n",
    "# # 8) Print out the results\n",
    "# print(\"Freq-window clusters → optimal latent-space match:\")\n",
    "# for mid_cid, (fine_cid, sim) in mid2fine.items():\n",
    "#     lo, hi = FREQ_WINDOWS[mid_cid]\n",
    "#     print(f\"  Mid window {mid_cid} [{lo:.1f}–{hi:.1f}] MHz \"\n",
    "#           f\"↔ Fine window {fine_cid} [{FREQ_WINDOWS[fine_cid][0]:.1f}–\"\n",
    "#           f\"{FREQ_WINDOWS[fine_cid][1]:.1f}] MHz   cos={sim:.3f}\")\n",
    "\n",
    "# # 9) List any windows that ended up unmatched\n",
    "# unmatched_mid  = [c for c in mid_cids  if c not in mid2fine]\n",
    "# unmatched_fine = [c for c in fine_cids if c not in {f for _,(f,_) in mid2fine.items()}]\n",
    "# print(\"\\nUnmatched mid windows :\", unmatched_mid)\n",
    "# print(\"Unmatched fine windows:\", unmatched_fine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75710f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 0) Your separately‐computed PCA reductions (rectangular allowed)\n",
    "# ----------------------------------------------------------------------\n",
    "mid_red  = latents_32d           # shape (N_mid, d)\n",
    "fine_red = latents_32d_high      # shape (N_fine, d)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1) INITIAL “GREEDY” SEED MATCHING\n",
    "#    (we need some equal‐length seed sets to run Procrustes)\n",
    "# ----------------------------------------------------------------------\n",
    "# full unaligned similarity\n",
    "S0 = cosine_similarity(mid_red, fine_red)        # (N_mid × N_fine)\n",
    "\n",
    "# for each mid vector i, find best‐matching fine vector j\n",
    "best_fine_for_mid = np.argmax(S0, axis=1)         # shape (N_mid,)\n",
    "best_sim_for_mid  = np.max (S0, axis=1)           # shape (N_mid,)\n",
    "\n",
    "# threshold to keep only high‐confidence seeds\n",
    "seed_thr = 0.10\n",
    "seed_mid_idx  = np.where(best_sim_for_mid >= seed_thr)[0]\n",
    "seed_fine_idx = best_fine_for_mid[seed_mid_idx]\n",
    "\n",
    "if len(seed_mid_idx) < mid_red.shape[1]:\n",
    "    raise RuntimeError(\n",
    "        f\"Only {len(seed_mid_idx)} seed pairs (need ≥{mid_red.shape[1]}) — \"\n",
    "        \"lower `seed_thr` or check your data.\"\n",
    "    )\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2) PROCRUSTES ALIGNMENT ON THE SEEDS\n",
    "#    solves for R: fine_red[seed] @ R ≈ mid_red[seed]\n",
    "# ----------------------------------------------------------------------\n",
    "R, _ = orthogonal_procrustes(\n",
    "    fine_red[seed_fine_idx],\n",
    "    mid_red[ seed_mid_idx ]\n",
    ")\n",
    "fine_red_aligned = fine_red @ R\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3) FULL COSINE‐SIMILARITY MATRIX\n",
    "# ----------------------------------------------------------------------\n",
    "S = cosine_similarity(mid_red, fine_red_aligned)  # (N_mid × N_fine)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4) HUNGARIAN ASSIGNMENT (maximize total cosine)\n",
    "# ----------------------------------------------------------------------\n",
    "row_ind, col_ind = linear_sum_assignment(-S)      # returns matching of length min(N_mid,N_fine)\n",
    "mid2fine = {i: (j, S[i,j]) for i,j in zip(row_ind, col_ind)}\n",
    "\n",
    "# (optional) filter out any very weak matches\n",
    "THR = 0.0\n",
    "mid2fine = {m:(f,s) for m,(f,s) in mid2fine.items() if s>=THR}\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 5) REPORT\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"Optimal Mid ↔ Fine mapping (Hungarian after Procrustes):\")\n",
    "for m,(f,s) in sorted(mid2fine.items()):\n",
    "    print(f\"  Mid {m:>4} ↔ Fine {f:>4}   cos={s:.3f}\")\n",
    "\n",
    "unmatched_mid  = [i for i in range(mid_red.shape[0]) if i not in mid2fine]\n",
    "unmatched_fine = [j for j in range(fine_red.shape[0])\n",
    "                  if j not in {f for f,_ in mid2fine.values()}]\n",
    "\n",
    "print(\"\\nUnmatched mid vectors :\", unmatched_mid)\n",
    "print(\"Unmatched fine vectors:\", unmatched_fine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b9867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "# 0) Joint PCA so both clouds live in the same axes\n",
    "d=8\n",
    "all_latents = np.vstack([mid_red, fine_red])       # (N_mid+N_fine, d)\n",
    "pca        = PCA(n_components=d, random_state=0)\\\n",
    "               .fit(all_latents)\n",
    "mid_joint  = pca.transform(mid_red)                # (N_mid, d)\n",
    "fine_joint = pca.transform(fine_red)               # (N_fine, d)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# — assume in scope:\n",
    "#    S                 # (N_mid, N_fine) your final cosine‐similarity matrix\n",
    "#    freqs_array       # shape (N_mid,)  mid‐res frequencies in MHz\n",
    "#    freqs_array_high  # shape (N_fine,) fine‐res frequencies in MHz\n",
    "\n",
    "# 1) Run Hungarian to get every one‐to‐one match\n",
    "row_ind, col_ind = linear_sum_assignment(-S)\n",
    "\n",
    "# 2) Build a DataFrame with no filtering—every matched pair\n",
    "df_matches = pd.DataFrame({\n",
    "    'mid_idx':   row_ind,\n",
    "    'mid_freq':  freqs_array[row_ind],\n",
    "    'fine_idx':  col_ind,\n",
    "    'fine_freq': freqs_array_high[col_ind],\n",
    "    'cosine':    S[row_ind, col_ind],\n",
    "    'difference': freqs_array[row_ind] - freqs_array_high[col_ind]\n",
    "})\n",
    "\n",
    "# 3) Save to CSV\n",
    "df_matches.to_csv('mid_fine_all_matches.csv', index=False)\n",
    "\n",
    "# Optional: inspect the first few rows\n",
    "print(df_matches.head())\n",
    "\n",
    "\n",
    "# # 1) Normalize rows (so dot = cosine)\n",
    "# mid_norm   = normalize(mid_joint,  axis=1)         # (N_mid, d)\n",
    "# fine_norm  = normalize(fine_joint, axis=1)         # (N_fine, d)\n",
    "\n",
    "# # 2) Build full similarity matrix\n",
    "# S = mid_norm @ fine_norm.T                         # (N_mid, N_fine)\n",
    "\n",
    "# # 3) Mutual nearest‐neighbor filter\n",
    "# best_fine_for_mid = np.argmax(S, axis=1)           # shape (N_mid,)\n",
    "# best_mid_for_fine = np.argmax(S, axis=0)           # shape (N_fine,)\n",
    "\n",
    "# mutual_pairs = [\n",
    "#     (i, j, S[i,j])\n",
    "#     for i in range(S.shape[0])\n",
    "#     for j in (best_fine_for_mid[i],)\n",
    "#     if best_mid_for_fine[j] == i\n",
    "# ]\n",
    "\n",
    "# print(f\"Found {len(mutual_pairs)} mutual‐NN pairs out of {S.shape[0]} mid vectors\")\n",
    "\n",
    "# # 4) (Optional) If you still want a one‐to‐one global assignment, you can\n",
    "# #    feed only these mutual seeds into Hungarian:\n",
    "# from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# # — assume you already have:\n",
    "# #    mid_norm, fine_aligned  # (unit-length) embeddings\n",
    "# #    freqs_array, freqs_array_high\n",
    "# #    S = mid_norm @ fine_aligned.T\n",
    "\n",
    "# # 1) run Hungarian globally\n",
    "# row_ind, col_ind = linear_sum_assignment(-S)\n",
    "\n",
    "# # 2) collect all pairs with freqs + cosine\n",
    "# rows = []\n",
    "# for m,f in zip(row_ind, col_ind):\n",
    "#     rows.append({\n",
    "#         'mid_idx':  m,\n",
    "#         'mid_freq': freqs_array[m],\n",
    "#         'fine_idx': f,\n",
    "#         'fine_freq': freqs_array_high[f],\n",
    "#         'cosine':   float(S[m,f]),\n",
    "#         'delta_MHz': freqs_array[m] - freqs_array_high[f]\n",
    "#     })\n",
    "\n",
    "# df_matches = pd.DataFrame(rows)\n",
    "\n",
    "# # 3) prune by cosine threshold (e.g. ≥0.5) and/or Δfreq (e.g. ≤5 MHz)\n",
    "# df_pruned = df_matches[\n",
    "#     (df_matches['cosine'] >= 0.5) &\n",
    "#     (df_matches['delta_MHz'].abs() <= 5)\n",
    "# ].reset_index(drop=True)\n",
    "\n",
    "# # 4) save to CSV\n",
    "# df_pruned.to_csv('hungarian_mid_fine_matches.csv', index=False)\n",
    "\n",
    "# print(f\"Kept {len(df_pruned)} / {len(df_matches)} matches after pruning\")\n",
    "# df_pruned.head()\n",
    "\n",
    "\n",
    "# seed_row, seed_col, _ = zip(*mutual_pairs)\n",
    "# S_sub = S[np.ix_(seed_row, seed_col)]\n",
    "# r, c = linear_sum_assignment(-S_sub)\n",
    "\n",
    "# hungarian_pairs = [\n",
    "#     (seed_row[i], seed_col[j], S_sub[i,j])\n",
    "#     for i,j in zip(r,c)\n",
    "# ]\n",
    "\n",
    "# print(f\"Hungarian on mutual‐NN seeds yields {len(hungarian_pairs)} pairs\")\n",
    "\n",
    "# # 5) Turn into a CSV\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame(\n",
    "#     [(i, freqs_array[i], j, freqs_array_high[j], s)\n",
    "#      for i,j,s in hungarian_pairs],\n",
    "#     columns=['mid_idx','mid_freq','fine_idx','fine_freq','cosine']\n",
    "# )\n",
    "# df.to_csv('mid_fine_matches_pca.csv', index=False)\n",
    "# print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6388cf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# build a list of dicts for each matched pair\n",
    "rows = []\n",
    "for m,(f,s) in mid2fine.items():\n",
    "    rows.append({\n",
    "        'mid_idx':   m,\n",
    "        'mid_freq':  freqs_array[m],\n",
    "        'fine_idx':  f,\n",
    "        'fine_freq': freqs_array_high[f],\n",
    "        'cosine':    s,\n",
    "        'difference': freqs_array[m] - freqs_array_high[f]\n",
    "    })\n",
    "\n",
    "# make a DataFrame and save\n",
    "df_matches = pd.DataFrame(rows)\n",
    "df_matches.to_csv('mid_fine_cosine_0.2.csv', index=False)\n",
    "\n",
    "# optional: peek\n",
    "print(df_matches.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be40fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weird = df_matches[df_matches['difference'] > 30]\n",
    "print(f\"{len(weird)} rows with |Δfreq|>30 MHz:\")\n",
    "print(weird.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d119bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) create a 'difference' column\n",
    "df_matches['difference'] = (df_matches['mid_freq'] - df_matches['fine_freq']).abs()\n",
    "\n",
    "# 2) inspect its distribution\n",
    "print(df_matches['difference'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff1eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# — assume in scope:\n",
    "#    S                 # (N_mid, N_fine) full cosine matrix after Procrustes\n",
    "#    row_ind, col_ind  # arrays of Hungarian‐matched indices\n",
    "#    freqs_array       # (N_mid,) mid frequencies in MHz\n",
    "#    freqs_array_high  # (N_fine,) fine frequencies in MHz\n",
    "\n",
    "# 1) Frequency‐order your axes\n",
    "mid_sorted  = np.argsort(freqs_array)\n",
    "fine_sorted = np.argsort(freqs_array_high)\n",
    "\n",
    "df = pd.DataFrame(S, index=mid_sorted, columns=fine_sorted)\n",
    "\n",
    "# build maps from original index → position in the sorted DataFrame\n",
    "pos_mid  = {m:i for i,m in enumerate(mid_sorted)}\n",
    "pos_fine = {f:j for j,f in enumerate(fine_sorted)}\n",
    "\n",
    "# 2) Precompute which sorted‐positions have matches\n",
    "matched_positions = [(pos_mid[m], pos_fine[f]) for m,f in zip(row_ind, col_ind)]\n",
    "\n",
    "# 3) Labels for ticks\n",
    "row_labels = [f\"{m} ({freqs_array[m]:.1f} MHz)\" for m in mid_sorted]\n",
    "col_labels = [f\"{f} ({freqs_array_high[f]:.1f} MHz)\" for f in fine_sorted]\n",
    "\n",
    "# 4) Loop over row‐blocks of size B, but only if there’s at least one match in that block\n",
    "B = 10\n",
    "Nmid = df.shape[0]\n",
    "\n",
    "for start in range(0, Nmid, B):\n",
    "    end = min(start + B, Nmid)\n",
    "    # which matched pairs fall into this row‐range?\n",
    "    block_matches = [(i,j) for (i,j) in matched_positions if start <= i < end]\n",
    "    if not block_matches:\n",
    "        continue  # skip empty blocks\n",
    "\n",
    "    # get the unique fine‐columns for those matches\n",
    "    cols = sorted({j for (_,j) in block_matches})\n",
    "\n",
    "    # slice out a (B × len(cols)) sub‐DataFrame\n",
    "    df_blk = df.iloc[start:end, cols]\n",
    "\n",
    "    # corresponding labels\n",
    "    row_lbl_blk = row_labels[start:end]\n",
    "    col_lbl_blk = [col_labels[j] for j in cols]\n",
    "\n",
    "    # 5) Plot heatmap for this block\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(\n",
    "            max(4, 0.3 * len(cols)),\n",
    "            max(4, 0.3 * (end - start))\n",
    "        )\n",
    "    )\n",
    "    sns.heatmap(\n",
    "        df_blk,\n",
    "        cmap=\"viridis\", vmin=0, vmax=1,\n",
    "        cbar_kws={'label': 'cosine similarity'},\n",
    "        linewidths=0.5, linecolor='lightgray',\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    ax.set_xticks(np.arange(len(col_lbl_blk)))\n",
    "    ax.set_yticks(np.arange(len(row_lbl_blk)))\n",
    "    ax.set_xticklabels(col_lbl_blk, rotation=45, ha='right', fontsize=6)\n",
    "    ax.set_yticklabels(row_lbl_blk, rotation=0, ha='right', fontsize=6)\n",
    "\n",
    "    ax.set_xlabel('Fine vectors (idx | freq)')\n",
    "    ax.set_ylabel('Mid  vectors (idx | freq)')\n",
    "    ax.set_title(f'Matched pairs rows {start}–{end-1}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccae9266",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# — in scope you have:\n",
    "#    S                 # (N_mid, N_fine) cosine matrix after Procrustes\n",
    "#    row_ind, col_ind  # arrays of matched indices\n",
    "#    freqs_array       # (N_mid,) mid frequencies in MHz\n",
    "#    freqs_array_high  # (N_fine,) fine frequencies in MHz\n",
    "\n",
    "# 1) Sort your axes by increasing frequency\n",
    "mid_sorted  = np.argsort(freqs_array)\n",
    "fine_sorted = np.argsort(freqs_array_high)\n",
    "\n",
    "# 2) Build a DataFrame of S with those sorted axes\n",
    "df = pd.DataFrame(\n",
    "    S,\n",
    "    index=mid_sorted,\n",
    "    columns=fine_sorted\n",
    ")\n",
    "\n",
    "# 3) Mask everything *except* the Hungarian matches\n",
    "#    We need the positions of each (m,f) in the sorted DF:\n",
    "pos_map_mid  = {m:i for i,m in enumerate(mid_sorted)}\n",
    "pos_map_fine = {f:j for j,f in enumerate(fine_sorted)}\n",
    "\n",
    "mask = np.ones(df.shape, dtype=bool)\n",
    "for m,f in zip(row_ind, col_ind):\n",
    "    i = pos_map_mid[m]\n",
    "    j = pos_map_fine[f]\n",
    "    mask[i,j] = False   # keep only these\n",
    "\n",
    "# 4) Build nice tick‐labels “idx (freq MHz)”\n",
    "row_labels = [f\"{m} ({freqs_array[m]:.1f} MHz)\" for m in mid_sorted]\n",
    "col_labels = [f\"{f} ({freqs_array_high[f]:.1f} MHz)\" for f in fine_sorted]\n",
    "\n",
    "# 5) Plot\n",
    "fig, ax = plt.subplots(\n",
    "    figsize=(\n",
    "      max(4, 0.3 * df.shape[1]),\n",
    "      max(4, 0.3 * df.shape[0])\n",
    "    )\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    df, mask=mask,\n",
    "    cmap=\"viridis\", vmin=0, vmax=1,\n",
    "    cbar_kws={'label': 'cosine similarity'},\n",
    "    linewidths=0.5, linecolor='lightgray',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_xticklabels(col_labels, rotation=45, ha='right', fontsize=7)\n",
    "ax.set_yticklabels(row_labels, rotation=0, ha='right', fontsize=7)\n",
    "\n",
    "ax.set_xlabel('Fine vectors (idx | freq MHz)')\n",
    "ax.set_ylabel('Mid  vectors (idx | freq MHz)')\n",
    "ax.set_title('Hungarian‐matched pairs only (freq‐ordered)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4be766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # — assume in scope: \n",
    "# #    S                 # shape (N_mid, N_fine), your cosine_similarity(mid_red, fine_red_aligned)\n",
    "# #    freqs_array       # shape (N_mid,), frequency of each mid vector (in MHz)\n",
    "# #    freqs_array_high  # shape (N_fine,), frequency of each fine vector (in MHz)\n",
    "\n",
    "# # 1) Get sort‐order by frequency\n",
    "# mid_idx_sorted  = np.argsort(freqs_array)\n",
    "# fine_idx_sorted = np.argsort(freqs_array_high)\n",
    "\n",
    "# # 2) Reorder similarity matrix\n",
    "# S_ord = S[np.ix_(mid_idx_sorted, fine_idx_sorted)]\n",
    "\n",
    "# # 3) Build tick labels like \"idx (freq MHz)\"\n",
    "# mid_labels  = [f\"{i} ({freqs_array[i]:.1f} MHz)\"   for i in mid_idx_sorted]\n",
    "# fine_labels = [f\"{j} ({freqs_array_high[j]:.1f} MHz)\" for j in fine_idx_sorted]\n",
    "\n",
    "# # 4) Plot with matplotlib only\n",
    "# fig, ax = plt.subplots(\n",
    "#     figsize=(\n",
    "#       max(4, 0.2 * len(fine_labels)),\n",
    "#       max(4, 0.2 * len(mid_labels))\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# im = ax.imshow(\n",
    "#     S_ord,\n",
    "#     aspect='auto',\n",
    "#     vmin=0, vmax=1            # fix color scale to [0,1]\n",
    "# )\n",
    "\n",
    "# # colorbar\n",
    "# cbar = fig.colorbar(im, ax=ax)\n",
    "# cbar.set_label('cosine similarity')\n",
    "\n",
    "# # axis ticks\n",
    "# ax.set_xticks(np.arange(len(fine_labels)))\n",
    "# ax.set_yticks(np.arange(len(mid_labels)))\n",
    "# ax.set_xticklabels(fine_labels, rotation=45, ha='right', fontsize=8)\n",
    "# ax.set_yticklabels(mid_labels, rotation=0, ha='right', fontsize=8)\n",
    "\n",
    "# ax.set_xlabel('Fine vectors (idx | freq MHz)')\n",
    "# ax.set_ylabel('Mid  vectors (idx | freq MHz)')\n",
    "# ax.set_title('Cosine similarity heatmap: mid vs. fine vectors')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('cosine_heatmap.png', dpi=150)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8609bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "# (re-normalize so everything is unit-length)\n",
    "mid_norm = normalize(latents_32d,           axis=1)\n",
    "fine_norm = normalize(latents_32d_high @ R, axis=1)   # if you’re using your Procrustes R\n",
    "\n",
    "# dot product of each pair (i↔i)\n",
    "diag_cos = np.einsum('ij,ij->i', mid_norm, fine_norm)\n",
    "\n",
    "print(\"Matched-pair cosine percentiles (0,5,25,50,75,95,100):\",\n",
    "      np.round(np.percentile(diag_cos, [0,5,25,50,75,95,100]), 3))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(diag_cos, bins=50)\n",
    "plt.xlabel(\"cosine(mid[i], fine[i])\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e62f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# in scope:\n",
    "#   latents_32d, latents_32d_high, R\n",
    "#   freqs_array, freqs_array_high\n",
    "\n",
    "# 1) Align + normalize\n",
    "fine_aligned = latents_32d_high @ R\n",
    "mid_norm  = normalize(latents_32d,           axis=1)\n",
    "fine_norm = normalize(fine_aligned,          axis=1)\n",
    "\n",
    "# 2) Stack & run joint t-SNE\n",
    "all_latents = np.vstack([mid_norm, fine_norm])\n",
    "Z = TSNE(n_components=2, init='pca', random_state=0).fit_transform(all_latents)\n",
    "Z_mid, Z_fine = Z[: len(mid_norm)], Z[len(mid_norm) :]\n",
    "\n",
    "# 3) Plot\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "# mid-res: circles with white edge\n",
    "sc1 = ax.scatter(\n",
    "    Z_mid[:,0], Z_mid[:,1],\n",
    "    c=freqs_array, cmap='viridis',\n",
    "    s=20, marker='o', edgecolor='w', linewidth=0.3,\n",
    "    alpha=0.8, label='mid-res'\n",
    ")\n",
    "# fine-res: crosses\n",
    "sc2 = ax.scatter(\n",
    "    Z_fine[:,0], Z_fine[:,1],\n",
    "    c=freqs_array_high, cmap='viridis',\n",
    "    s=20, marker='x',\n",
    "    alpha=0.8, label='fine-res'\n",
    ")\n",
    "\n",
    "# colorbar (just need one; fine and mid share freq range)\n",
    "cbar = plt.colorbar(sc1, ax=ax)\n",
    "cbar.set_label('Signal frequency (MHz)')\n",
    "\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_title('Joint t-SNE of mid vs. fine (after Procrustes)')\n",
    "ax.set_xticks([]); ax.set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8609bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Matched mid indices:\", row_ind)\n",
    "print(\"Matched fine indices:\", col_ind)\n",
    "\n",
    "# Which of those lie in the first block?\n",
    "print(\"In block 0–9 → mid:\", [i for i in row_ind if 0 <= i < 10])\n",
    "print(\"In block 0–9 → fine:\",[j for j in col_ind if 0 <= j < 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0167a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------------------------------------------------\n",
    "# # 0) PCA-BASED LATENT REDUCTION  (unchanged)\n",
    "# # ----------------------------------------------------------------------\n",
    "# import numpy as np\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # target_pcs = 8\n",
    "# # all_latents = np.vstack([latents_32d, latents_32d_high])\n",
    "# # n_samples, n_feats = all_latents.shape\n",
    "# # n_components = min(target_pcs, n_feats, n_samples)\n",
    "\n",
    "# # pca = PCA(n_components=n_components, random_state=0).fit(all_latents)\n",
    "# mid_res  = latents_32d\n",
    "# fine_res = latents_32d_high\n",
    "\n",
    "# # ----------------------------------------------------------------------\n",
    "# # 1) CENTROIDS IN PCA SPACE\n",
    "# # ----------------------------------------------------------------------\n",
    "# def compute_centroids(lat, labs):\n",
    "#     cids = sorted(c for c in np.unique(labs) if c != -1)\n",
    "#     C = []\n",
    "#     for cid in cids:\n",
    "#         pts = lat[labs == cid]\n",
    "#         ctr = pts.mean(axis=0)\n",
    "#         ctr /= np.linalg.norm(ctr) + 1e-12\n",
    "#         C.append(ctr)\n",
    "#     return cids, np.stack(C)\n",
    "\n",
    "# mid_ids,  mid_C  = compute_centroids(mid_red,  clusterer_mid.labels_)\n",
    "# fine_ids, fine_C = compute_centroids(fine_red, clusterer_high.labels_)\n",
    "\n",
    "# # ----------------------------------------------------------------------\n",
    "# # 2) INITIAL GREEDY MATCH  (for Procrustes seed only)\n",
    "# # ----------------------------------------------------------------------\n",
    "# sim0 = cosine_similarity(mid_C, fine_C)\n",
    "# greedy_mid2fine = {mid_ids[i]: fine_ids[np.argmax(sim0[i])]\n",
    "#                    for i in range(len(mid_ids))}\n",
    "\n",
    "# # choose paired rows\n",
    "# seed_pairs   = [(m, greedy_mid2fine[m]) for m in greedy_mid2fine]\n",
    "# mid_idx_seed = [mid_ids.index(m)   for m, _ in seed_pairs]\n",
    "# fine_idx_seed= [fine_ids.index(f)  for _, f in seed_pairs]\n",
    "\n",
    "# # ----------------------------------------------------------------------\n",
    "# # 3) PROCRUSTES ALIGNMENT\n",
    "# # ----------------------------------------------------------------------\n",
    "# from scipy.linalg import orthogonal_procrustes\n",
    "# A = mid_C [mid_idx_seed]           # (k,d)\n",
    "# B = fine_C[fine_idx_seed]          # (k,d)\n",
    "# R, _ = orthogonal_procrustes(B, A)\n",
    "# fine_C_aligned = fine_C @ R        # rotate ALL fine centroids\n",
    "\n",
    "# # ----------------------------------------------------------------------\n",
    "# # 4) COSINE SIMILARITY AFTER ALIGNMENT\n",
    "# # ----------------------------------------------------------------------\n",
    "# S = cosine_similarity(mid_C, fine_C_aligned)   # (M × F)\n",
    "\n",
    "# # ----------------------------------------------------------------------\n",
    "# # 5) HUNGARIAN (GLOBAL) ASSIGNMENT\n",
    "# # ----------------------------------------------------------------------\n",
    "# from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# row_ind, col_ind = linear_sum_assignment(-S)   # maximise total cosine\n",
    "\n",
    "# mid2fine_opt  = {mid_ids[i]: (fine_ids[j], S[i, j])\n",
    "#                  for i, j in zip(row_ind, col_ind)}\n",
    "# fine2mid_opt  = {fine_ids[j]: (mid_ids[i], S[i, j])\n",
    "#                  for i, j in zip(row_ind, col_ind)}\n",
    "\n",
    "# # (optional) remove weak links\n",
    "# THR = 0.30\n",
    "# mid2fine_opt = {m:(f,s) for m,(f,s) in mid2fine_opt.items() if s >= THR}\n",
    "# fine2mid_opt = {f:(m,s) for f,(m,s) in fine2mid_opt.items() if s >= THR}\n",
    "\n",
    "# # ----------------------------------------------------------------------\n",
    "# # 6) REPORT\n",
    "# # ----------------------------------------------------------------------\n",
    "# print(\"Optimal Mid <-> Fine mapping (Hungarian, Procrustes-aligned)\")\n",
    "# for m,(f,s) in mid2fine_opt.items():\n",
    "#     print(f\"  Mid {m:>3} <-> Fine {f:>3}   cos={s:.3f}\")\n",
    "\n",
    "# unmatched_mid  = [m for m in mid_ids  if m not in mid2fine_opt]\n",
    "# unmatched_fine = [f for f in fine_ids if f not in fine2mid_opt]\n",
    "\n",
    "# print(\"\\nUnmatched mid clusters :\", unmatched_mid)\n",
    "# print(\"Unmatched fine clusters:\", unmatched_fine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa76410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.mixture        import GaussianMixture\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from scipy.linalg           import orthogonal_procrustes\n",
    "# from scipy.optimize         import linear_sum_assignment\n",
    "\n",
    "# # ─── 0) Assume you already have your reduced latents:\n",
    "# #      mid_red  (N_mid × d), fine_red (N_fine × d)\n",
    "# # ─────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# # ─── 1) Choose #components via BIC (optional) ─────────────────────────\n",
    "# def select_gmm_components(X, min_n=2, max_n=10):\n",
    "#     bics = []\n",
    "#     for n in range(min_n, max_n+1):\n",
    "#         gmm = GaussianMixture(n_components=n,\n",
    "#                               covariance_type='full',\n",
    "#                               random_state=0).fit(X)\n",
    "#         bics.append(gmm.bic(X))\n",
    "#     best = np.argmin(bics) + min_n\n",
    "#     return best\n",
    "\n",
    "# n_mid_best  = select_gmm_components(mid_red,  min_n=2, max_n=5)\n",
    "# n_fine_best = select_gmm_components(fine_red, min_n=2, max_n=5)\n",
    "# # n_mid_best  = 2\n",
    "# # n_fine_best = 2\n",
    "\n",
    "# print(f\"→ Selected mid GMM components:  {n_mid_best}\")\n",
    "# print(f\"→ Selected fine GMM components: {n_fine_best}\")\n",
    "\n",
    "# # ─── 2) Fit the two GMMs and get labels ─────────────────────────────────\n",
    "# gmm_mid  = GaussianMixture(n_components=n_mid_best,\n",
    "#                            covariance_type='full',\n",
    "#                            random_state=0).fit(mid_red)\n",
    "# mid_labels_gmm  = gmm_mid.predict(mid_red)\n",
    "\n",
    "# gmm_fine = GaussianMixture(n_components=n_fine_best,\n",
    "#                            covariance_type='full',\n",
    "#                            random_state=0).fit(fine_red)\n",
    "# fine_labels_gmm = gmm_fine.predict(fine_red)\n",
    "\n",
    "# # ─── 3) Compute unit‐length centroids in PCA space ─────────────────────\n",
    "# def compute_centroids(lat, labs):\n",
    "#     cids = sorted(c for c in np.unique(labs) if c != -1)\n",
    "#     C = []\n",
    "#     for cid in cids:\n",
    "#         pts = lat[labs == cid]\n",
    "#         ctr = pts.mean(axis=0)\n",
    "#         ctr /= np.linalg.norm(ctr) + 1e-12\n",
    "#         C.append(ctr)\n",
    "#     return cids, np.stack(C)\n",
    "\n",
    "# mid_ids,  mid_C  = compute_centroids(mid_red,  mid_labels_gmm)\n",
    "# fine_ids, fine_C = compute_centroids(fine_red, fine_labels_gmm)\n",
    "\n",
    "# # ─── 4) Seed Procrustes with greedy cosine matches ──────────────────────\n",
    "# sim0 = cosine_similarity(mid_C, fine_C)\n",
    "# greedy_mid2fine = {\n",
    "#     mid_ids[i]: fine_ids[np.argmax(sim0[i])]\n",
    "#     for i in range(len(mid_ids))\n",
    "# }\n",
    "# seed_pairs   = [(m, greedy_mid2fine[m]) for m in greedy_mid2fine]\n",
    "# mid_idx_s    = [mid_ids.index(m)   for m,_ in seed_pairs]\n",
    "# fine_idx_s   = [fine_ids.index(f)  for _,f in seed_pairs]\n",
    "\n",
    "# A = mid_C[mid_idx_s]\n",
    "# B = fine_C[fine_idx_s]\n",
    "# R, _ = orthogonal_procrustes(B, A)\n",
    "# fine_C_aligned = fine_C @ R\n",
    "\n",
    "# # ─── 5) Build full cosine‐similarity matrix & do Hungarian ────────────\n",
    "# S = cosine_similarity(mid_C, fine_C_aligned)    # shape (M × F)\n",
    "# row_ind, col_ind = linear_sum_assignment(-S)    # maximise total cosine\n",
    "\n",
    "# mid2fine = {mid_ids[i]: (fine_ids[j], S[i,j]) \n",
    "#             for i,j in zip(row_ind, col_ind)}\n",
    "# fine2mid = {fine_ids[j]: (mid_ids[i], S[i,j]) \n",
    "#             for i,j in zip(row_ind, col_ind)}\n",
    "\n",
    "# # ─── 6) Report ─────────────────────────────────────────────────────────\n",
    "# print(\"GMM→Procrustes→Hungarian mapping\")\n",
    "# for m,(f,sim) in mid2fine.items():\n",
    "#     print(f\"  Mid-GMM {m} → Fine-GMM {f}    cos={sim:.3f}\")\n",
    "\n",
    "# # unmatched (if any)\n",
    "# unmatched_mid  = [m for m in mid_ids  if m not in mid2fine]\n",
    "# unmatched_fine = [f for f in fine_ids if f not in fine2mid]\n",
    "# print(\"Unmatched mid clusters :\", unmatched_mid)\n",
    "# print(\"Unmatched fine clusters:\", unmatched_fine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fcebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# # ───────────────────────────────────────────────────────────────────────────\n",
    "# # Inputs from your GMM pipeline:\n",
    "# #\n",
    "# #   mid_labels_gmm   : array of length N_mid\n",
    "# #   fine_labels_gmm  : array of length N_fine\n",
    "# #   mid_ids, fine_ids: lists of cluster IDs returned by compute_centroids\n",
    "# #   S                : numpy array shape (len(mid_ids), len(fine_ids)),\n",
    "# #                      your aligned cosine_similarity(mid_C, fine_C_aligned)\n",
    "# #\n",
    "# #   freqs_array      : numpy array shape (N_mid,), per-hit freq for mid\n",
    "# #   freqs_array_high : numpy array shape (N_fine,), per-hit freq for fine\n",
    "# # ───────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# # (a) compute centre‐frequency per cluster\n",
    "# def cluster_cfreq(labels, freqs, cluster_ids):\n",
    "#     cf = {}\n",
    "#     for cid in cluster_ids:\n",
    "#         mask = (labels == cid)\n",
    "#         if np.any(mask):\n",
    "#             cf[cid] = freqs[mask].mean()\n",
    "#         else:\n",
    "#             cf[cid] = np.nan\n",
    "#     return cf\n",
    "\n",
    "# mid_cfreq  = cluster_cfreq(mid_labels_gmm,   freqs_array,      mid_ids)\n",
    "# fine_cfreq = cluster_cfreq(fine_labels_gmm,  freqs_array_high, fine_ids)\n",
    "\n",
    "# # (b) build & sort the DataFrame\n",
    "# df_sim = pd.DataFrame(\n",
    "#     S,\n",
    "#     index  = pd.Index(mid_ids,  name=\"mid\"),\n",
    "#     columns= pd.Index(fine_ids, name=\"fine\")\n",
    "# )\n",
    "\n",
    "# # ───────────────────────────────────────────────────────────────────────────\n",
    "# # (b’)  SORT CLUSTERS BY CENTRE-FREQUENCY\n",
    "# # ───────────────────────────────────────────────────────────────────────────\n",
    "# # mid_cfreq and fine_cfreq are your dicts from the GMM cell:\n",
    "# #    mid_cfreq[cid]  = mean MHz of all mid_latents with label cid\n",
    "# #    fine_cfreq[cid] = mean MHz of all fine_latents with label cid\n",
    "\n",
    "# # 1) Build sorted lists of cluster IDs\n",
    "# mid_sorted  = sorted(mid_ids,  key=lambda cid: mid_cfreq[cid])\n",
    "# fine_sorted = sorted(fine_ids, key=lambda cid: fine_cfreq[cid])\n",
    "\n",
    "# # 2) Re-index your similarity DataFrame\n",
    "# df_sim = pd.DataFrame(\n",
    "#     S,\n",
    "#     index  = pd.Index(mid_ids,  name=\"mid\"),\n",
    "#     columns= pd.Index(fine_ids, name=\"fine\")\n",
    "# ).loc[mid_sorted, fine_sorted]\n",
    "\n",
    "# # 3) Recompute the mask\n",
    "# SIM_THRESH = 0.05\n",
    "# mask       = df_sim < SIM_THRESH\n",
    "\n",
    "# # 4) Build new tick labels\n",
    "# row_labels = [f\"{cid} ({mid_cfreq[cid]:.1f} MHz)\"   for cid in mid_sorted]\n",
    "# col_labels = [f\"{cid} ({fine_cfreq[cid]:.1f} MHz)\" for cid in fine_sorted]\n",
    "\n",
    "\n",
    "# # (d) plot\n",
    "# n_mid, n_fine = len(mid_sorted), len(fine_sorted)\n",
    "# fig_w = max(4, 0.35 * n_fine)\n",
    "# fig_h = max(4, 0.35 * n_mid)\n",
    "\n",
    "# plt.figure(figsize=(fig_w, fig_h))\n",
    "# ax = sns.heatmap(\n",
    "#     df_sim,\n",
    "#     mask       = mask,\n",
    "#     cmap       = \"viridis\",\n",
    "#     vmin       = 0, vmax = 1,\n",
    "#     cbar_kws   = dict(label=\"Cosine similarity\", shrink=.85),\n",
    "#     annot      = False,\n",
    "#     linecolor  = \"white\",\n",
    "#     linewidths = 0.5\n",
    "# )\n",
    "# ax.set_aspect(\"auto\")\n",
    "# ax.set_xticklabels(col_labels, rotation=45, ha=\"right\", fontsize=8)\n",
    "# ax.set_yticklabels(row_labels, rotation=0, ha=\"right\", fontsize=8)\n",
    "\n",
    "# # (e) annotate only Hungarian pairs\n",
    "# #    row_ind, col_ind from linear_sum_assignment(-S)\n",
    "# for m_idx, f_idx in zip(row_ind, col_ind):\n",
    "#     m_id, f_id = mid_ids[m_idx], fine_ids[f_idx]\n",
    "#     if m_id in mid_sorted and f_id in fine_sorted:\n",
    "#         r = mid_sorted.index(m_id)\n",
    "#         c = fine_sorted.index(f_id)\n",
    "#         sim_val = df_sim.iat[r, c]\n",
    "#         if sim_val >= SIM_THRESH:\n",
    "#             ax.text(\n",
    "#                 c+0.5, r+0.5,\n",
    "#                 f\"{sim_val:.2f}\",\n",
    "#                 ha=\"center\", va=\"center\",\n",
    "#                 color=\"black\", fontsize=7\n",
    "#             )\n",
    "\n",
    "# plt.xlabel(\"Fine-GMM clusters  (ID • centre-freq)\")\n",
    "# plt.ylabel(\"Mid-GMM clusters   (ID • centre-freq)\")\n",
    "# plt.title(\"Aligned cosine similarity   —   Mid-GMM vs Fine-GMM\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5964f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# 7)  HEAT-MAP OF MID × FINE CLUSTERS  (frequency-ordered)\n",
    "# ======================================================================\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# (a)  Build centre-frequency dictionaries if you have them already.\n",
    "#      Here we compute them as the mean of member frequencies.\n",
    "#      Replace this with your own arrays if available.\n",
    "# ------------------------------------------------------------------\n",
    "# freqs_mid  : np.ndarray shape (N_mid,)   (MHz for every latent x_i)\n",
    "# freqs_fine : np.ndarray shape (N_fine,)  (MHz for every latent y_j)\n",
    "\n",
    "def cluster_cfreq(labels, freqs):\n",
    "    return {cid: freqs[labels == cid].mean()\n",
    "            for cid in np.unique(labels) if cid != -1}\n",
    "\n",
    "mid_cfreq  = cluster_cfreq(clusterer_mid.labels_,   freqs_array)\n",
    "fine_cfreq = cluster_cfreq(clusterer_high.labels_,  freqs_array_high)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# (b)  Put similarity matrix in a DataFrame and sort by frequency\n",
    "# ------------------------------------------------------------------\n",
    "df_sim = pd.DataFrame(\n",
    "    S,                                   # S from Step-4 (after Procrustes)\n",
    "    index=pd.Index(mid_ids,  name=\"mid\"),\n",
    "    columns=pd.Index(fine_ids, name=\"fine\")\n",
    ")\n",
    "\n",
    "mid_sorted  = sorted(mid_ids,  key=lambda i: mid_cfreq[i])\n",
    "fine_sorted = sorted(fine_ids, key=lambda j: fine_cfreq[j])\n",
    "df_sim = df_sim.loc[mid_sorted, fine_sorted]\n",
    "\n",
    "# mask tiny values for clarity\n",
    "SIM_THRESH = 0.05\n",
    "mask = df_sim < SIM_THRESH\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# (c)  Prepare tick labels “ID  (freq MHz)”\n",
    "# ------------------------------------------------------------------\n",
    "row_labels = [f\"{cid} ({mid_cfreq[cid]:.1f})\"  for cid in mid_sorted]\n",
    "col_labels = [f\"{cid} ({fine_cfreq[cid]:.1f})\" for cid in fine_sorted]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# PLOT — auto aspect and dynamic figsize\n",
    "# ------------------------------------------------------------------\n",
    "n_mid, n_fine = len(mid_sorted), len(fine_sorted)\n",
    "\n",
    "# choose 0.35 in of width per column, 0.35 in of height per row\n",
    "fig_w = max(4, 0.35 * n_fine)   # at least 4 inches\n",
    "fig_h = max(4, 0.35 * n_mid)\n",
    "\n",
    "plt.figure(figsize=(fig_w, fig_h))\n",
    "ax = sns.heatmap(\n",
    "    df_sim,\n",
    "    mask       = mask,\n",
    "    cmap       = \"viridis\",\n",
    "    vmin       = 0, vmax = 1,\n",
    "    cbar_kws   = dict(label=\"cosine similarity\", shrink=.85),\n",
    "    annot      = False,\n",
    "    linecolor  = \"white\", linewidths=0.5\n",
    ")\n",
    "\n",
    "# allow rectangular cells\n",
    "ax.set_aspect(\"auto\")           \n",
    "\n",
    "# nicer tick labels\n",
    "ax.set_xticklabels(col_labels, rotation=45, ha=\"right\", fontsize=8)\n",
    "ax.set_yticklabels(row_labels, rotation=0,  ha=\"right\", fontsize=8)\n",
    "\n",
    "# annotate only Hungarian pairs\n",
    "for m_idx, f_idx in zip(row_ind, col_ind):\n",
    "    m_id, f_id = mid_ids[m_idx], fine_ids[f_idx]\n",
    "    if m_id in mid_sorted and f_id in fine_sorted:\n",
    "        r = mid_sorted.index(m_id)\n",
    "        c = fine_sorted.index(f_id)\n",
    "        sim_val = df_sim.iat[r, c]\n",
    "        ax.text(c + 0.5, r + 0.5, f\"{sim_val:.2f}\",\n",
    "                    color=\"black\", ha=\"center\", va=\"center\", fontsize=7)\n",
    "\n",
    "plt.xlabel(\"Fine clusters  (ID | centre-freq MHz)\")\n",
    "plt.ylabel(\"Mid clusters   (ID | centre-freq MHz)\")\n",
    "plt.title(\"Aligned cosine similarity   |   Mid vs. Fine cluster map\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebc91ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "mask = labels!=-1\n",
    "sil = silhouette_score(fine_red[mask], labels[mask], metric='cosine')\n",
    "print(\"Fine‐res silhouette:\", sil)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c99318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "mask = labels!=-1\n",
    "sil = silhouette_score(fine_red[mask], labels[mask], metric='cosine')\n",
    "print(\"Fine‐res silhouette:\", sil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDUCING LATENT SPACE TO 8 DIMENSIONS USING PCA\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1) Choose PCA dimension (e.g. 8) but cap at available rank\n",
    "# target_pcs = 8\n",
    "# all_latents = np.vstack([latents_32d, latents_32d_high])\n",
    "# n_samples, n_feats = all_latents.shape\n",
    "# n_components = min(target_pcs, n_feats, n_samples)\n",
    "\n",
    "# # 2) Fit PCA on the *combined* set\n",
    "# pca = PCA(n_components=n_components, random_state=0)\n",
    "# pca.fit(all_latents)\n",
    "\n",
    "# 3) Transform mid & fine latents into the shared PCA space\n",
    "mid_red  = latents_32d   # shape (N_mid,  n_components)\n",
    "fine_red = latents_32d_high  # shape (N_fine, n_components)\n",
    "\n",
    "# 4) Compute cluster centroids in that PCA space\n",
    "def compute_centroids(lat, labs):\n",
    "    cids = sorted(c for c in np.unique(labs) if c != -1)\n",
    "    C = []\n",
    "    for cid in cids:\n",
    "        pts = lat[labs == cid]\n",
    "        ctr = pts.mean(axis=0)\n",
    "        ctr /= np.linalg.norm(ctr) + 1e-12\n",
    "        C.append(ctr)\n",
    "    return cids, np.stack(C, axis=0)\n",
    "\n",
    "mid_ids,  mid_C  = compute_centroids(mid_red,  clusterer_mid.labels_)\n",
    "fine_ids, fine_C = compute_centroids(fine_red, clusterer_high.labels_)\n",
    "\n",
    "# 5) Cosine‐similarity matrix in PCA space\n",
    "sim_mat = cosine_similarity(mid_C, fine_C)\n",
    "\n",
    "# 6) Best matches in both directions\n",
    "best_mid2fine = {\n",
    "    mid_ids[i]: (fine_ids[j], sim_mat[i, j])\n",
    "    for i, j in enumerate(np.argmax(sim_mat, axis=1))\n",
    "}\n",
    "best_fine2mid = {\n",
    "    fine_ids[j]: (mid_ids[i], sim_mat[i, j])\n",
    "    for j, i in enumerate(np.argmax(sim_mat, axis=0))\n",
    "}\n",
    "\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# ─── 1) build parallel arrays of matched centroids ─────────────────────────\n",
    "matched = [(mid, fine) for mid,(fine,_) in best_mid2fine.items()]\n",
    "mid_match_ids, fine_match_ids = zip(*matched)\n",
    "\n",
    "# find their row‐indices in mid_C / fine_C\n",
    "mid_idx  = [mid_ids.index(mid)  for mid in mid_match_ids]\n",
    "fine_idx = [fine_ids.index(fine) for fine in fine_match_ids]\n",
    "\n",
    "A = mid_C[mid_idx]    # shape (k, d)\n",
    "B = fine_C[fine_idx]  # shape (k, d)\n",
    "\n",
    "# ─── 2) solve for the best orthogonal rotation R so that B @ R ≈ A ────\n",
    "R, scale = orthogonal_procrustes(B, A)\n",
    "\n",
    "# ─── 3) apply that rotation to ALL fine centroids ───────────────────────\n",
    "fine_C_aligned = fine_C.dot(R)   # now aligned to mid‐space\n",
    "\n",
    "# ─── 4) re‐compute cosine similarities in the aligned space ────────────\n",
    "sim_aligned = cosine_similarity(mid_C, fine_C_aligned)\n",
    "\n",
    "# ─── 5) get new best matches Mid→Fine & Fine→Mid ────────────────────────\n",
    "best_mid2fine_aligned = {\n",
    "    mid_ids[i]: (fine_ids[j], sim_aligned[i, j])\n",
    "    for i, j in enumerate(np.argmax(sim_aligned, axis=1))\n",
    "}\n",
    "best_fine2mid_aligned = {\n",
    "    fine_ids[j]: (mid_ids[i], sim_aligned[i, j])\n",
    "    for j, i in enumerate(np.argmax(sim_aligned, axis=0))\n",
    "}\n",
    "\n",
    "# ─── 6) report ───────────────────────────────────────────────────────────\n",
    "print(\"After Procrustes alignment:\")\n",
    "print(\"\\nMid -> Fine:\")\n",
    "for m,(f,sim) in best_mid2fine_aligned.items():\n",
    "    print(f\"  Mid {m} -> Fine {f}   cosine={sim:.3f}\")\n",
    "\n",
    "print(\"\\nFine -> Mid:\")\n",
    "for f,(m,sim) in best_fine2mid_aligned.items():\n",
    "    print(f\"  Fine {f} -> Mid {m}   cosine={sim:.3f}\")\n",
    "\n",
    "# # ─── Report ────────────────────────────────────────────────────────────────\n",
    "# print(\"Mid → Fine (in PCA space):\")\n",
    "# for m, (f, s) in best_mid2fine.items():\n",
    "#     print(f\"  Mid #{m} → Fine #{f}   cosine={s:.3f}\")\n",
    "\n",
    "# print(\"\\nFine → Mid (in PCA space):\")\n",
    "# for f, (m, s) in best_fine2mid.items():\n",
    "#     print(f\"  Fine #{f} → Mid #{m}   cosine={s:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36ec658",
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_latents = latents_32d\n",
    "fine_latents = latents_32d_high\n",
    "mid_labels = clusterer_mid.labels_\n",
    "fine_labels = clusterer_high.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37f4a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# ─── your inputs ─────────────────────────────────────────────────────────\n",
    "# mid_latents   : np.ndarray, shape (N_mid,  D)\n",
    "# mid_labels    : np.ndarray, shape (N_mid,)\n",
    "# fine_latents  : np.ndarray, shape (N_fine, D)\n",
    "# fine_labels   : np.ndarray, shape (N_fine,)\n",
    "# best_mid2fine_aligned : dict[mid_cid] -> (fine_cid, similarity)\n",
    "#                                                                             \n",
    "\n",
    "# sanity‐check\n",
    "assert mid_latents.shape[0] == mid_labels.shape[0]\n",
    "assert fine_latents.shape[0] == fine_labels.shape[0]\n",
    "\n",
    "# ─── 1) run t-SNE on the combined set ───────────────────────────────────────\n",
    "all_latents = np.vstack([mid_latents, fine_latents])\n",
    "n = all_latents.shape[0]\n",
    "perp = min(30, max(2, n // 3))   # ensure 2 ≤ perplexity < n\n",
    "\n",
    "tsne = TSNE(\n",
    "    n_components    = 2,\n",
    "    perplexity      = perp,\n",
    "    init            = 'pca',\n",
    "    learning_rate   = 'auto',\n",
    "    random_state    = 0,\n",
    ")\n",
    "all_xy = tsne.fit_transform(all_latents)\n",
    "\n",
    "mid_xy  = all_xy[: mid_latents.shape[0], :]\n",
    "fine_xy = all_xy[mid_latents.shape[0]:, :]\n",
    "\n",
    "# ─── 2) plot the point clouds ───────────────────────────────────────────────\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# mid-res (circles)\n",
    "plt.scatter(\n",
    "    mid_xy[:,0], mid_xy[:,1],\n",
    "    c=mid_labels,\n",
    "    cmap='tab20',\n",
    "    marker='o',\n",
    "    s=30,\n",
    "    alpha=0.6,\n",
    "    label='mid-res'\n",
    ")\n",
    "\n",
    "# fine-res (squares)\n",
    "plt.scatter(\n",
    "    fine_xy[:,0], fine_xy[:,1],\n",
    "    c=fine_labels,\n",
    "    cmap='tab20',\n",
    "    marker='s',\n",
    "    s=30,\n",
    "    alpha=0.6,\n",
    "    label='fine-res'\n",
    ")\n",
    "\n",
    "cb = plt.colorbar(ticks=np.unique(mid_labels))\n",
    "cb.set_label(\"Cluster ID\")\n",
    "\n",
    "# ─── 3) compute 2D centroids & draw match lines ─────────────────────────────\n",
    "# build centroid coords for each cluster ID\n",
    "mid_centroid_2d = { cid: mid_xy[mid_labels==cid].mean(axis=0)\n",
    "                    for cid in np.unique(mid_labels) if cid != -1 }\n",
    "fine_centroid_2d= { cid: fine_xy[fine_labels==cid].mean(axis=0)\n",
    "                    for cid in np.unique(fine_labels) if cid != -1 }\n",
    "\n",
    "for mid_cid, (fine_cid, sim) in best_mid2fine_aligned.items():\n",
    "    if mid_cid in mid_centroid_2d and fine_cid in fine_centroid_2d:\n",
    "        x0,y0 = mid_centroid_2d[mid_cid]\n",
    "        x1,y1 = fine_centroid_2d[fine_cid]\n",
    "        plt.plot([x0,x1],[y0,y1], '-k', alpha=0.4, linewidth=1)\n",
    "        # optional: annotate sim at midpoint\n",
    "        plt.text((x0+x1)/2, (y0+y1)/2,\n",
    "                 f\"{sim:.2f}\",\n",
    "                 color='k', fontsize=7, ha='center', va='center')\n",
    "\n",
    "plt.legend()\n",
    "plt.title(f\"t-SNE of Mid/Fine latents (perplexity={perp})\")\n",
    "plt.xlabel(\"t-SNE dim 1\")\n",
    "plt.ylabel(\"t-SNE dim 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd32b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# … after you have mid_C and fine_C_aligned from Procrustes …\n",
    "\n",
    "# 1) stack centroids\n",
    "all_centroids = np.vstack([mid_C, fine_C_aligned])\n",
    "n_samples = all_centroids.shape[0]\n",
    "\n",
    "# 2) choose a valid perplexity: < n_samples, and at least 2\n",
    "perplexity = min(30, n_samples - 1)\n",
    "if perplexity < 2:\n",
    "    raise ValueError(f\"Not enough centroids ({n_samples}) for TSNE.\")\n",
    "\n",
    "# 3) run TSNE\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=perplexity,\n",
    "    learning_rate='auto',\n",
    "    init='pca',\n",
    "    random_state=0,\n",
    ")\n",
    "coords2d = tsne.fit_transform(all_centroids)\n",
    "\n",
    "# 4) split back out\n",
    "mid_xy  = coords2d[: len(mid_C)]\n",
    "fine_xy = coords2d[len(mid_C):]\n",
    "\n",
    "# 5) plot\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(mid_xy[:,0],  mid_xy[:,1],  c='C0', s=100, label='Mid centroids')\n",
    "plt.scatter(fine_xy[:,0], fine_xy[:,1], c='C1', s=100, marker='s', label='Fine centroids')\n",
    "\n",
    "for mid_cid, (fine_cid, sim) in best_mid2fine_aligned.items():\n",
    "    i = mid_ids.index(mid_cid)\n",
    "    j = fine_ids.index(fine_cid)\n",
    "    x0,y0 = mid_xy[i]\n",
    "    x1,y1 = fine_xy[j]\n",
    "    plt.plot([x0,x1], [y0,y1], '-k', alpha=0.4)\n",
    "    plt.text((x0+x1)/2, (y0+y1)/2, f\"{sim:.2f}\",\n",
    "             color='gray', fontsize=8, ha='center', va='center')\n",
    "\n",
    "# annotate\n",
    "for i, cid in enumerate(mid_ids):\n",
    "    plt.text(mid_xy[i,0], mid_xy[i,1], str(cid),\n",
    "             fontsize=9, weight='bold', ha='right', va='bottom')\n",
    "for j, cid in enumerate(fine_ids):\n",
    "    plt.text(fine_xy[j,0], fine_xy[j,1], str(cid),\n",
    "             fontsize=9, weight='bold', ha='left',  va='top')\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"t-SNE of Mid vs Fine centroids (perplexity={} )\".format(perplexity))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58513cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_centroids(latents: np.ndarray,\n",
    "                      labels: np.ndarray) -> tuple[list, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute L2‐normalised centroids for each cluster (label != -1).\n",
    "    Returns:\n",
    "      cluster_ids : sorted list of cluster labels\n",
    "      C           : array of shape (n_clusters, dim)\n",
    "    \"\"\"\n",
    "    cluster_ids = sorted(l for l in np.unique(labels) if l != -1)\n",
    "    centroids = []\n",
    "    for cid in cluster_ids:\n",
    "        members = latents[labels == cid]\n",
    "        ctr = members.mean(axis=0)\n",
    "        ctr /= np.linalg.norm(ctr)          # normalise the mean\n",
    "        centroids.append(ctr)\n",
    "    return cluster_ids, np.stack(centroids, axis=0)\n",
    "\n",
    "# ─── assume these exist: mid_latents, mid_labels, fine_latents, fine_labels ───\n",
    "\n",
    "mid_ids,  mid_C  = compute_centroids(latents_32d, clusterer_mid.labels_)\n",
    "fine_ids, fine_C = compute_centroids(latents_32d_high, clusterer_high.labels_)\n",
    "\n",
    "# ─── cosine‐similarity matrix (mid × fine) ───────────────────────────────────\n",
    "sim_mat = cosine_similarity(mid_C, fine_C)\n",
    "# sim_mat[i,j] = cos_sim(mid_ids[i] centroid, fine_ids[j] centroid)\n",
    "\n",
    "# 1) Mid→Fine (best fine cluster for each mid cluster)\n",
    "row_max = np.argmax(sim_mat, axis=1)   # for each mid‐row, index of best fine‐col\n",
    "best_mid2fine = {\n",
    "    mid_ids[i]: (fine_ids[j], sim_mat[i, j])\n",
    "    for i, j in enumerate(row_max)\n",
    "}\n",
    "\n",
    "# 2) Fine→Mid (best mid cluster for each fine cluster)\n",
    "col_max = np.argmax(sim_mat, axis=0)   # for each fine‐col, index of best mid‐row\n",
    "best_fine2mid = {\n",
    "    fine_ids[j]: (mid_ids[i], sim_mat[i, j])\n",
    "    for j, i in enumerate(col_max)\n",
    "}\n",
    "\n",
    "# 3) Print mappings\n",
    "print(\"Mid -> Fine:\")\n",
    "for mid_cid, (fine_cid, sim) in best_mid2fine.items():\n",
    "    print(f\"  Mid {mid_cid} -> Fine {fine_cid}   (cosine={sim:.3f})\")\n",
    "\n",
    "print(\"\\nFine -> Mid:\")\n",
    "for fine_cid, (mid_cid, sim) in best_fine2mid.items():\n",
    "    print(f\"  Fine {fine_cid} -> Mid {mid_cid}   (cosine={sim:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5943a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import to_hex\n",
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "\n",
    "# assume:\n",
    "#   labels      — your HDBSCAN cluster labels, shape (N,)\n",
    "#   dataset     — your YoloMaskDataset (with .images and .labels lists)\n",
    "#   Z_tsne      — your t-SNE coords, shape (N,2)\n",
    "\n",
    "# 1) Pick a discrete colormap (tab20 has 20 distinct colors)\n",
    "unique_labels = np.unique(labels)\n",
    "# remove noise if you like, or keep it\n",
    "# unique_labels = unique_labels[unique_labels>=0]\n",
    "\n",
    "# make a colormap with as many entries as distinct clusters\n",
    "cmap = cm.get_cmap('tab20', len(unique_labels))\n",
    "\n",
    "# map each cluster ID → a hex color\n",
    "label2color = {}\n",
    "for idx, lab in enumerate(unique_labels):\n",
    "    # if lab == -1:\n",
    "    #     label2color[lab] = \"#808080\"           # gray for noise\n",
    "    # else:\n",
    "        rgba               = cmap(idx)        # RGBA tuple in [0,1]\n",
    "        label2color[lab]   = to_hex(rgba)     # convert to \"#rrggbb\"\n",
    "\n",
    "# 2) (Optional) re‐plot your TSNE with the same discrete colors so you can check\n",
    "plt.figure(figsize=(6,6))\n",
    "for lab in unique_labels:\n",
    "    mask = labels == lab\n",
    "    plt.scatter(\n",
    "        Z_tsne_high_norm[mask,0],\n",
    "        Z_tsne_high_norm[mask,1],\n",
    "        c=label2color[lab],\n",
    "        label=f\"{lab}\",\n",
    "        s=8,\n",
    "        linewidths=0\n",
    "    )\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.title(\"t-SNE of VAE latents (discrete colors)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e00676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, re, itertools, colorsys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from PIL import Image\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────\n",
    "ROOT_DIR   = Path(\"/datax/scratch/jliang/waterfall_pngs\")\n",
    "COLS, ROWS = 6, 6\n",
    "PER_PAGE   = COLS * ROWS\n",
    "OUT_PDF    = \"waterfalls_by_cluster_viridis.pdf\"\n",
    "TILE_SIZE_IN = 2.5                     # inches per tile edge\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 0) labels must be provided (same order as paths)\n",
    "#    e.g. labels = np.load(\"cluster_labels.npy\")\n",
    "labels = clusterer_high.labels_                            # ← your 1-D array (N,)\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# 1) collect paths and sanity-check length\n",
    "paths = sampled_paths\n",
    "if len(paths) != len(labels):\n",
    "    raise ValueError(f\"{len(paths)=} does not match {len(labels)=}\")\n",
    "\n",
    "paths = np.array(paths)\n",
    "labels = np.asarray(labels)\n",
    "unique_labels = np.unique(labels)\n",
    "\n",
    "# 2) deterministic colour map: distinct hue per cluster id\n",
    "def label2color(lab):\n",
    "    if lab < 0:                     # noise\n",
    "        return \"#808080\"\n",
    "    hue = (lab * 0.6180339887) % 1  # golden-ratio hop around HSV wheel\n",
    "    r, g, b = colorsys.hsv_to_rgb(hue, 0.8, 0.9)\n",
    "    return (r, g, b)\n",
    "\n",
    "# 3) regex to extract freq min/max\n",
    "freq_pat = re.compile(r\"_(\\d+\\.\\d+)-(\\d+\\.\\d+)\")\n",
    "\n",
    "with PdfPages(OUT_PDF) as pdf:\n",
    "    for lab in unique_labels:\n",
    "        idxs = np.where(labels == lab)[0]\n",
    "        if idxs.size == 0:\n",
    "            continue\n",
    "\n",
    "        n_pages = math.ceil(idxs.size / PER_PAGE)\n",
    "        for p in range(n_pages):\n",
    "            page_slice = idxs[p * PER_PAGE : (p + 1) * PER_PAGE]\n",
    "\n",
    "            fig, axes = plt.subplots(ROWS, COLS,\n",
    "                                     figsize=(COLS * TILE_SIZE_IN,\n",
    "                                              ROWS * TILE_SIZE_IN))\n",
    "            fig.suptitle(f\"Cluster {lab} — page {p+1}/{n_pages}\",\n",
    "                         fontsize=16, weight=\"bold\",\n",
    "                         color=label2color(lab))\n",
    "\n",
    "            for ax, idx in zip(axes.flatten(), page_slice):\n",
    "                img_path = paths[idx]\n",
    "                img      = Image.open(img_path).convert(\"L\")\n",
    "                ax.imshow(img, cmap=\"viridis\", origin=\"lower\", interpolation=\"none\")\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "                # coloured frame\n",
    "                for spine in ax.spines.values():\n",
    "                    spine.set_color(label2color(lab))\n",
    "                    spine.set_linewidth(2)\n",
    "\n",
    "                # title = centre freq ± half-span\n",
    "                m = freq_pat.search(img_path.name)\n",
    "                if m:\n",
    "                    f_min, f_max = map(float, m.groups())\n",
    "                    f_cen  = 0.5 * (f_min + f_max)\n",
    "                    span   = f_max - f_min\n",
    "                    text   = f\"{f_cen:.3f} MHz\\n±{span/2:.3f}\"\n",
    "                else:\n",
    "                    text   = img_path.stem\n",
    "\n",
    "                ax.set_title(text, fontsize=8, weight=\"bold\")\n",
    "\n",
    "            # blank unused tiles on final page\n",
    "            for ax in axes.flatten()[len(page_slice):]:\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)\n",
    "\n",
    "print(f\"✓ Saved {OUT_PDF} ({len(paths)} images, {len(unique_labels)} clusters)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512d7891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.backends.backend_pdf import PdfPages\n",
    "# from PIL import Image\n",
    "\n",
    "# # ─── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "# cols = 6\n",
    "# rows = 6\n",
    "# per_page   = cols * rows\n",
    "# out_pdf    = \"all_crops_by_cluster_i_s154_with_deltas_vit_100_highres.pdf\"\n",
    "\n",
    "# # total time span of each spectrogram (in seconds)\n",
    "# time_window_seconds = 5*60\n",
    "\n",
    "# # assume these are defined:\n",
    "# #  • labels, val_ds.images, val_ds.labels, label2color\n",
    "\n",
    "# unique_labels = sorted(set(labels.tolist()))\n",
    "\n",
    "# with PdfPages(out_pdf) as pdf:\n",
    "#     for lab in unique_labels:\n",
    "#         idxs = np.where(labels == lab)[0]\n",
    "#         if len(idxs) == 0:\n",
    "#             continue\n",
    "\n",
    "#         n_pages = math.ceil(len(idxs) / per_page)\n",
    "#         for p in range(n_pages):\n",
    "#             start     = p * per_page\n",
    "#             page_idxs = idxs[start : start + per_page]\n",
    "\n",
    "#             fig, axes = plt.subplots(rows, cols,\n",
    "#                                      figsize=(cols * 2.5, rows * 2.5))\n",
    "#             fig.suptitle(f\"Cluster {lab} — page {p+1}/{n_pages}\", \n",
    "#                          fontsize=16)\n",
    "\n",
    "#             for ax, idx in zip(axes.flatten(), page_idxs):\n",
    "#                 # load crop → PIL\n",
    "#                 img = Image.open(test_ds.images[idx]).convert(\"L\")\n",
    "\n",
    "#                 # read the single YOLO box\n",
    "#                 lbl_path = test_ds.labels[idx]\n",
    "#                 with open(lbl_path) as f:\n",
    "#                     cls, xc, yc, w_norm, h_norm = map(float, f.readline().split())\n",
    "\n",
    "#                 # — pull per-image freq_min / freq_max from the filename —\n",
    "#                 fname = os.path.basename(test_ds.images[idx])\n",
    "#                 m = re.search(r\"f_(\\d+(?:\\.\\d+)?)_(\\d+(?:\\.\\d+)?)\", fname)\n",
    "#                 if not m:\n",
    "#                     raise ValueError(f\"Could not parse freq range from {fname}\")\n",
    "#                 freq_max_img = float(m.group(1))\n",
    "#                 freq_min_img = float(m.group(2))\n",
    "#                 freq_range_img = freq_max_img - freq_min_img\n",
    "\n",
    "#                 # compute physical sizes\n",
    "#                 time_size = h_norm * time_window_seconds    # seconds\n",
    "#                 freq_size = w_norm * freq_range_img  # MHz\n",
    "\n",
    "#                 # draw image and colored frame\n",
    "#                 ax.imshow(img)\n",
    "#                 ax.axis(\"off\")\n",
    "#                 for spine in ax.spines.values():\n",
    "#                     spine.set_edgecolor(label2color.get(lab, \"#000000\"))\n",
    "#                     spine.set_linewidth(2)\n",
    "\n",
    "#                 # set title to include t and f\n",
    "#                 ax.set_title(\n",
    "#                     f\"C{lab} | t={time_size:.1f}s | f={freq_size:.1f}MHz\\n C={freq_min_img + freq_range_img * xc:.1f}MHz\",\n",
    "#                     color='k',\n",
    "#                     weight='bold',\n",
    "#                     fontsize=10,\n",
    "#                 )\n",
    "\n",
    "#             # blank out unused axes\n",
    "#             for ax in axes.flatten()[len(page_idxs):]:\n",
    "#                 ax.axis(\"off\")\n",
    "\n",
    "#             plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "#             pdf.savefig(fig)\n",
    "#             plt.close(fig)\n",
    "\n",
    "# print(f\"Saved PDF with per box t/f annotations to {out_pdf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cb2224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.backends.backend_pdf import PdfPages\n",
    "# from PIL import Image\n",
    "\n",
    "# # 1) Make sure you have these in your environment:\n",
    "# #    • labels:     numpy array of length N with cluster IDs (-1 for noise)\n",
    "# #    • full_ds:    your dataset, so that full_ds[i] returns (crop_tensor, …)\n",
    "# #    • label2color: dict mapping each cluster ID → hex color (e.g. \"#1f77b4\")\n",
    "\n",
    "# # 2) Derive unique cluster IDs\n",
    "# unique_labels = sorted(set(labels.tolist()))\n",
    "\n",
    "# # 3) Grid dimensions\n",
    "# cols = 6\n",
    "# rows = 6\n",
    "# per_page = cols * rows\n",
    "# out_pdf = \"all_crops_by_cluster_i_s154.pdf\"\n",
    "\n",
    "# with PdfPages(out_pdf) as pdf:\n",
    "#     for lab in unique_labels:\n",
    "#         # if lab == -1:\n",
    "#         #     continue  # skip noise if you’d like\n",
    "\n",
    "#         idxs = np.where(labels == lab)[0]\n",
    "#         # if len(idxs) == 0:\n",
    "#         #     continue\n",
    "\n",
    "#         n_pages = math.ceil(len(idxs) / per_page)\n",
    "#         for p in range(n_pages):\n",
    "#             start = p * per_page\n",
    "#             end   = start + per_page\n",
    "#             page_idxs = idxs[start:end]\n",
    "\n",
    "#             fig, axes = plt.subplots(rows, cols, figsize=(cols*2.5, rows*2.5))\n",
    "#             fig.suptitle(f\"Cluster {lab} — page {p+1}/{n_pages}\", fontsize=16)\n",
    "\n",
    "#             for ax, idx in zip(axes.flatten(), page_idxs):\n",
    "#                 # grab the crop tensor and convert to PIL\n",
    "#                 crop_tensor = val_ds[idx][0]\n",
    "#                 arr = crop_tensor.permute(1,2,0).cpu().numpy()\n",
    "#                 arr = (arr * 255).astype(np.uint8)\n",
    "#                 img = Image.fromarray(arr)\n",
    "\n",
    "#                 ax.imshow(img)\n",
    "#                 ax.axis(\"off\")\n",
    "\n",
    "#                 # color the frame to match cluster\n",
    "#                 for spine in ax.spines.values():\n",
    "#                     spine.set_edgecolor(label2color.get(lab, \"#000000\"))\n",
    "#                     spine.set_linewidth(2)\n",
    "#                 ax.set_title(f\"C{lab}\", color=label2color.get(lab, \"#000000\"), fontsize=8)\n",
    "\n",
    "#             # blank out unused subplots\n",
    "#             for ax in axes.flatten()[len(page_idxs):]:\n",
    "#                 ax.axis(\"off\")\n",
    "\n",
    "#             plt.tight_layout(rect=[0,0.03,1,0.95])\n",
    "#             pdf.savefig(fig)\n",
    "#             plt.close(fig)\n",
    "\n",
    "# print(f\"▶️ Saved all crops to {out_pdf}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8410247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install seaborn\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# # ─── assume these are already in your namespace ────────────────────────────────\n",
    "# # labels      : np.ndarray of shape (N,) with HDBSCAN cluster IDs (−1=noise)\n",
    "# # freqs_array : np.ndarray of shape (N,) with the extracted f_… values\n",
    "\n",
    "# # 1) (Optional) ignore noise points\n",
    "# mask        = labels >= 0\n",
    "# labels_filt = labels[mask]\n",
    "# freqs_filt  = freqs_array[mask]\n",
    "\n",
    "# # 2) Build true frequency‐based classes\n",
    "# #    class 1 = in 1610–1620 MHz, else class 0 (i.e., ~1545 MHz)\n",
    "# true_classes = np.where(\n",
    "#     (freqs_filt >= 1610) & (freqs_filt <= 1620),\n",
    "#     1,\n",
    "#     0\n",
    "# )\n",
    "\n",
    "# # 3) Map each cluster ID to the class whose mean freq it sits in\n",
    "# cluster_ids    = np.unique(labels_filt)\n",
    "# cluster_means  = {cid: freqs_filt[labels_filt == cid].mean()\n",
    "#                   for cid in cluster_ids}\n",
    "\n",
    "# cluster_to_cls = {}\n",
    "# for cid, mean_f in cluster_means.items():\n",
    "#     if 1610 <= mean_f <= 1620:\n",
    "#         cluster_to_cls[cid] = 1\n",
    "#     else:\n",
    "#         cluster_to_cls[cid] = 0\n",
    "\n",
    "# # 4) Build predicted classes from cluster assignments\n",
    "# pred_classes = np.array([cluster_to_cls[c] for c in labels_filt])\n",
    "\n",
    "# # 5) Compute & print the confusion matrix\n",
    "# cm = confusion_matrix(true_classes, pred_classes, labels=[0,1])\n",
    "# print(\"Confusion matrix (rows=true, cols=predicted):\")\n",
    "# print(cm)\n",
    "\n",
    "# # 6) (Optional) More detailed metrics\n",
    "# print(\"\\nClassification report:\")\n",
    "# print(classification_report(true_classes, pred_classes, target_names=[\n",
    "#     \"1545 MHz\",\n",
    "#     \"1610-1620 MHz\"\n",
    "# ]))\n",
    "\n",
    "# # 1) Compute the 2×2 matrix\n",
    "# cm = confusion_matrix(true_classes, pred_classes, labels=[0, 1])\n",
    "\n",
    "# # 2) Plot with Seaborn\n",
    "# plt.figure(figsize=(6, 5))\n",
    "# ax = sns.heatmap(\n",
    "#     cm,\n",
    "#     annot=True,\n",
    "#     fmt=\"d\",\n",
    "#     cmap=\"Blues\",\n",
    "#     xticklabels=[\"1545 MHz\", \"1610-1620 MHz\"],\n",
    "#     yticklabels=[\"1545 MHz\", \"1610-1620 MHz\"]\n",
    "# )\n",
    "# ax.set_xlabel(\"Predicted Class\")\n",
    "# ax.set_ylabel(\"True Class\")\n",
    "# ax.set_title(\"Confusion Matrix: Frequency-Based Clustering\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3e9577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns, matplotlib.pyplot as plt\n",
    "\n",
    "CLS_1610 = 0          # 1610–1620 MHz   (positive)\n",
    "CLS_1545 = 1          # 1545 MHz band   (positive)\n",
    "CLS_NA   = -1         # everything else (ignored)\n",
    "\n",
    "# ─── true classes (only two bands) ────────────────────────────────────────\n",
    "true2 = np.full(freqs_array_high.shape, CLS_NA, dtype=int)\n",
    "true2[(1545  <= freqs_array_high) & (freqs_array_high < 1546)] = CLS_1545\n",
    "true2[(1610  <= freqs_array_high) & (freqs_array_high <= 1620)] = CLS_1610\n",
    "\n",
    "# ─── map each cluster to 0 / 1 / -1 (unmapped ⇒ -1) ───────────────────────\n",
    "cluster_means = {cid: freqs_array_high[labels == cid].mean()\n",
    "                 for cid in np.unique(labels)}\n",
    "\n",
    "cluster_to_cls2 = {}\n",
    "tol = 1.0            # MHz tolerance for the narrow 1545 band\n",
    "for cid, f in cluster_means.items():\n",
    "    if abs(f - 1545) <= tol:           # near 1545 MHz\n",
    "        cluster_to_cls2[cid] = CLS_1545\n",
    "    elif 1610 <= f <= 1620:            # in 1610–1620 MHz window\n",
    "        cluster_to_cls2[cid] = CLS_1610\n",
    "    else:                              # don’t-care cluster\n",
    "        cluster_to_cls2[cid] = CLS_NA\n",
    "\n",
    "pred2 = np.array([cluster_to_cls2[c] for c in labels], dtype=int)\n",
    "\n",
    "# ─── remove “don’t-care” rows/cols BEFORE confusion matrix ────────────────\n",
    "mask_keep = (true2 != CLS_NA) & (pred2 != CLS_NA)\n",
    "y_true    = true2[mask_keep]\n",
    "y_pred    = pred2[mask_keep]\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[CLS_1610, CLS_1545])\n",
    "\n",
    "# ─── PLOT ─────────────────────────────────────────────────────────────────\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"1610–1620\",\"1545\"],\n",
    "            yticklabels=[\"1610–1620\",\"1545\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"2-Class Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5805b722",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d82cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# tsne = TSNE(n_components=2, random_state=42)\n",
    "# Z_tsne = tsne.fit_transform(mus_array)\n",
    "\n",
    "# # 1) Assign each point to a signal type based on its frequency\n",
    "# labels = []\n",
    "# for f in freqs_array:\n",
    "#     if abs(f - 1575) < 10:           # within ±10 MHz of 1575\n",
    "#         labels.append(\"GPS (~1575 MHz)\")\n",
    "#     elif abs(f - 1620) < 10:         # within ±10 MHz of 1620\n",
    "#         labels.append(\"Iridium (~1620 MHz)\")\n",
    "#     else:\n",
    "#         labels.append(\"Other\")\n",
    "\n",
    "# labels = np.array(labels)\n",
    "\n",
    "# # 2) Plot each category separately for discrete coloring + legend\n",
    "# plt.figure(figsize=(6,6))\n",
    "# for lab in np.unique(labels):\n",
    "#     mask = (labels == lab)\n",
    "#     plt.scatter(\n",
    "#         Z_tsne[mask, 0],\n",
    "#         Z_tsne[mask, 1],\n",
    "#         label=lab,\n",
    "#         s=8\n",
    "#     )\n",
    "\n",
    "# plt.title(\"t-SNE of VAE latents, colored by signal type\")\n",
    "# plt.xlabel(\"TSNE 1\")\n",
    "# plt.ylabel(\"TSNE 2\")\n",
    "# plt.legend(markerscale=2, fontsize=\"small\", frameon=True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # 4b) UMAP embedding\n",
    "# um = umap.UMAP(n_components=2, random_state=42)\n",
    "# Z_umap = um.fit_transform(mus)\n",
    "\n",
    "# plt.figure(figsize=(6,6))\n",
    "# plt.scatter(Z_umap[:,0], Z_umap[:,1], c=freqs, cmap=\"plasma\", s=8)\n",
    "# plt.colorbar(label=\"Signal frequency\")\n",
    "# plt.title(\"UMAP of VAE latents colored by f_\")\n",
    "# plt.xlabel(\"UMAP 1\"); plt.ylabel(\"UMAP 2\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
